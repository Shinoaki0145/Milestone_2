{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b72cd4a",
   "metadata": {},
   "source": [
    "# Data Cleaning & Data Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e540762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "from rapidfuzz import fuzz\n",
    "import bibtexparser\n",
    "\n",
    "base_dir = Path(\"23127238_output\")\n",
    "start_folder = \"2304-14607\"\n",
    "papers_to_check = 1500\n",
    "output_file = Path(\"labels\") / \"auto_label.json\"\n",
    "\n",
    "stop_words = {\n",
    "    \"a\", \"an\", \"the\", \"and\", \"or\", \"of\", \"for\", \"in\", \"on\", \"with\",\n",
    "    \"to\", \"from\", \"by\", \"at\", \"into\", \"over\", \"under\", \"after\", \"before\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e284aabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARXIV_URL_RE = re.compile(r\"arxiv\\.org/(?:abs|pdf)/([^\\s#/}]+)\", re.I)\n",
    "ARXIV_OLD_RE = re.compile(r\"[a-z\\-]+/\\d{7}\", re.I)\n",
    "DOI_URL_RE = re.compile(r\"doi\\.org/([^\\s}]+)\", re.I)\n",
    "DOI_RE = re.compile(r\"\\b10\\.\\d{4,9}/\\S+\\b\", re.I)\n",
    "HREF_RE = re.compile(r\"\\\\href\\{([^}]+)\\}\")\n",
    "\n",
    "def normalize_arxiv_id(value: Any) -> Optional[str]:\n",
    "    if not value:\n",
    "        return None\n",
    "    raw = str(value).strip()\n",
    "    raw = raw.replace(\"arXiv:\", \"\").replace(\"arxiv:\", \"\")\n",
    "    raw = raw.lower()\n",
    "    match = re.match(r\"^(\\d{4})[.\\-]?(\\d{4,5})$\", raw)\n",
    "    if match:\n",
    "        return f\"{match.group(1)}-{match.group(2)}\"\n",
    "    \n",
    "    return raw\n",
    "\n",
    "def normalize_doi(value: Any) -> Optional[str]:\n",
    "    if not value:\n",
    "        return None\n",
    "    v = str(value).strip().lower()\n",
    "    v = re.sub(r\"^https?://(dx\\.)?doi\\.org/\", \"\", v)\n",
    "    \n",
    "    return v or None\n",
    "\n",
    "def normalize_plain_text(text: str) -> str:\n",
    "    cleaned = strip_accents(text)\n",
    "    cleaned = cleaned.casefold()\n",
    "    cleaned = re.sub(r\"[^a-z0-9]\", \"\", cleaned)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def strip_accents(text: str) -> str:\n",
    "    return unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "\n",
    "def normalize_for_fuzzy(text: str) -> str:\n",
    "    text = strip_accents(text.casefold())\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if not token:\n",
    "            continue\n",
    "        if token in stop_words:\n",
    "            continue\n",
    "        if token.isdigit():\n",
    "            continue\n",
    "        tokens.append(token)\n",
    "        \n",
    "    return \"\".join(tokens)\n",
    "\n",
    "def extract_year(value: Any) -> Optional[str]:\n",
    "    if not value:\n",
    "        return None\n",
    "    match = re.search(r\"(19|20)\\d{2}\", str(value))\n",
    "    \n",
    "    return match.group(0) if match else None\n",
    "\n",
    "def extract_ids_from_text(text: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    if not text:\n",
    "        return None, None\n",
    "    arxiv_id: Optional[str] = None\n",
    "    doi: Optional[str] = None\n",
    "    targets = [text] + HREF_RE.findall(text)\n",
    "    for chunk in targets:\n",
    "        if not arxiv_id:\n",
    "            m = ARXIV_URL_RE.search(chunk)\n",
    "            if m:\n",
    "                arxiv_id = m.group(1)\n",
    "        if not arxiv_id:\n",
    "            m_old = ARXIV_OLD_RE.search(chunk)\n",
    "            if m_old:\n",
    "                arxiv_id = m_old.group(0)\n",
    "        if not doi:\n",
    "            m_doi_url = DOI_URL_RE.search(chunk)\n",
    "            if m_doi_url:\n",
    "                doi = m_doi_url.group(1)\n",
    "        if not doi:\n",
    "            m_doi = DOI_RE.search(chunk)\n",
    "            if m_doi:\n",
    "                doi = m_doi.group(0)\n",
    "        if arxiv_id and doi:\n",
    "            break\n",
    "        \n",
    "    return arxiv_id, doi\n",
    "\n",
    "\n",
    "def load_json_data(path: Path) -> Tuple[Dict[str, Any], Optional[str]]:\n",
    "    if not path.exists():\n",
    "        return {}, \"json_missing\"\n",
    "    if path.stat().st_size == 0:\n",
    "        return {}, \"json_empty_file\"\n",
    "    try:\n",
    "        text = path.read_text(encoding=\"utf-8-sig\")\n",
    "        data = json.loads(text)\n",
    "    except json.JSONDecodeError as exc:\n",
    "        return {}, f\"json_decode_error:{exc.msg}\"\n",
    "    except UnicodeDecodeError as exc:\n",
    "        return {}, f\"json_decode_error:{exc.reason}\"\n",
    "    if not isinstance(data, dict):\n",
    "        return {}, \"json_not_dict\"\n",
    "    \n",
    "    return data, None\n",
    "\n",
    "\n",
    "def parse_bib_entries(bib_files: List[Path]) -> List[Dict[str, Any]]:\n",
    "    entries: List[Dict[str, Any]] = []\n",
    "    for bib_file in bib_files:\n",
    "        content = bib_file.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        parser = bibtexparser.bparser.BibTexParser(common_strings=True)\n",
    "        parser.ignore_nonstandard_types = False\n",
    "        try:\n",
    "            db = bibtexparser.loads(content, parser=parser)\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except Exception:\n",
    "            print(f\"Parse error in {bib_file.name}, skipping file\")\n",
    "            continue\n",
    "        for entry in db.entries:\n",
    "            entry[\"__bib_file\"] = bib_file.name\n",
    "            entries.append(entry)\n",
    "            \n",
    "    return entries\n",
    "\n",
    "\n",
    "def build_json_records(data: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:\n",
    "    records: Dict[str, Dict[str, Any]] = {}\n",
    "    for raw_id, info in data.items():\n",
    "        if not isinstance(info, dict):\n",
    "            continue\n",
    "        arxiv_id = normalize_arxiv_id(raw_id) or str(raw_id)\n",
    "        record = {\"arxiv_id\": arxiv_id, \"raw_id\": raw_id, \"info\": info}\n",
    "        records.setdefault(arxiv_id, record)\n",
    "        doi_alias = normalize_doi(info.get(\"doi\") or info.get(\"DOI\"))\n",
    "        if doi_alias and doi_alias not in records:\n",
    "            records[doi_alias] = record\n",
    "            \n",
    "    return records\n",
    "\n",
    "\n",
    "def resolve_canonical_id(candidate: Optional[str], records: Dict[str, Dict[str, Any]]) -> Optional[str]:\n",
    "    if not candidate:\n",
    "        return None\n",
    "    rec = records.get(candidate)\n",
    "    \n",
    "    return rec[\"arxiv_id\"] if rec else None\n",
    "\n",
    "\n",
    "def match_entry(entry: Dict[str, Any], records: Dict[str, Dict[str, Any]], available: set) -> Tuple[Optional[str], Optional[str]]:\n",
    "    eprint_norm = normalize_arxiv_id(entry.get(\"eprint\"))\n",
    "    bib_title = entry.get(\"title\")\n",
    "    bib_note = entry.get(\"note\")\n",
    "    bib_year = extract_year(entry.get(\"year\"))\n",
    "    \n",
    "    fields_to_check = [\n",
    "        (entry.get(\"note\"), \"note\"),\n",
    "        (entry.get(\"url\"), \"url\"),\n",
    "        (entry.get(\"howpublished\"), \"howpublished\"),\n",
    "        (entry.get(\"repository\"), \"repository\"),\n",
    "        (entry.get(\"doi\"), \"doi\")\n",
    "    ]\n",
    "\n",
    "    if eprint_norm:\n",
    "        canon = resolve_canonical_id(eprint_norm, records)\n",
    "        if canon and canon in available:\n",
    "            return canon, \"eprint\"\n",
    "\n",
    "    for text_field, tag in fields_to_check:\n",
    "        if not text_field: \n",
    "            continue\n",
    "        arxiv_found, doi_found = extract_ids_from_text(text_field)\n",
    "        for candidate, label in ((arxiv_found, \"arxiv\"), (normalize_doi(doi_found), \"doi\")):\n",
    "            candidate_norm = normalize_arxiv_id(candidate) if label == \"arxiv\" else candidate\n",
    "            canon = resolve_canonical_id(candidate_norm, records)\n",
    "            if canon and canon in available:\n",
    "                return canon, f\"{label}_from_{tag}\"\n",
    "\n",
    "    def get_info_title(info_dict):\n",
    "        return info_dict.get(\"paper_title\") or info_dict.get(\"title\") or \"\"\n",
    "\n",
    "    if bib_title:\n",
    "        bib_title_norm = normalize_plain_text(bib_title)\n",
    "        for arxiv_id in list(available):\n",
    "            info = records[arxiv_id][\"info\"]\n",
    "            json_title = get_info_title(info)\n",
    "            \n",
    "            if json_title and bib_title_norm == normalize_plain_text(json_title):\n",
    "                return arxiv_id, \"title_exact\"\n",
    "\n",
    "    candidates = [text for text in (bib_title, bib_note) if text]\n",
    "    best_score = 0\n",
    "    best_id: Optional[str] = None\n",
    "    best_json_year: Optional[str] = None\n",
    "    \n",
    "    for arxiv_id in list(available):\n",
    "        info = records[arxiv_id][\"info\"]\n",
    "        json_title = get_info_title(info)\n",
    "        \n",
    "        json_year = extract_year(info.get(\"submission_date\") or info.get(\"year\"))\n",
    "        json_norm = normalize_for_fuzzy(json_title)\n",
    "        \n",
    "        for text in candidates:\n",
    "            norm = normalize_for_fuzzy(text)\n",
    "            score = max(\n",
    "                fuzz.token_sort_ratio(norm, json_norm),\n",
    "                fuzz.token_set_ratio(norm, json_norm),\n",
    "                fuzz.partial_ratio(norm, json_norm),\n",
    "            )\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_id = arxiv_id\n",
    "                best_json_year = json_year\n",
    "                \n",
    "    if best_id and best_score >= 75:\n",
    "        if bib_year and best_json_year:\n",
    "            try:\n",
    "                year_diff = abs(int(bib_year) - int(best_json_year))\n",
    "            except ValueError:\n",
    "                year_diff = 0\n",
    "            if year_diff > 1:\n",
    "                return None, None\n",
    "            return best_id, f\"fuzzy_{best_score:.0f}_year_diff_{year_diff}\"\n",
    "        return best_id, f\"fuzzy_{best_score:.0f}\"\n",
    "\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95853367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scan from 2304-14607. Target: 1500 labeled papers.\n",
      "Progress: Found 50/1500 papers (Scanned 52 folders so far)\n",
      "Progress: Found 100/1500 papers (Scanned 103 folders so far)\n",
      "Progress: Found 150/1500 papers (Scanned 159 folders so far)\n",
      "Progress: Found 200/1500 papers (Scanned 209 folders so far)\n",
      "Progress: Found 250/1500 papers (Scanned 259 folders so far)\n",
      "Progress: Found 300/1500 papers (Scanned 312 folders so far)\n",
      "Progress: Found 350/1500 papers (Scanned 363 folders so far)\n",
      "Progress: Found 400/1500 papers (Scanned 413 folders so far)\n",
      "Progress: Found 450/1500 papers (Scanned 467 folders so far)\n",
      "Progress: Found 500/1500 papers (Scanned 518 folders so far)\n",
      "Progress: Found 550/1500 papers (Scanned 570 folders so far)\n",
      "Progress: Found 600/1500 papers (Scanned 621 folders so far)\n",
      "Progress: Found 650/1500 papers (Scanned 671 folders so far)\n",
      "Progress: Found 700/1500 papers (Scanned 722 folders so far)\n",
      "Progress: Found 750/1500 papers (Scanned 774 folders so far)\n",
      "Progress: Found 800/1500 papers (Scanned 826 folders so far)\n",
      "Progress: Found 850/1500 papers (Scanned 879 folders so far)\n",
      "Progress: Found 900/1500 papers (Scanned 929 folders so far)\n",
      "Progress: Found 950/1500 papers (Scanned 981 folders so far)\n",
      "Progress: Found 1000/1500 papers (Scanned 1032 folders so far)\n",
      "Progress: Found 1050/1500 papers (Scanned 1086 folders so far)\n",
      "Progress: Found 1100/1500 papers (Scanned 1138 folders so far)\n",
      "Progress: Found 1150/1500 papers (Scanned 1192 folders so far)\n",
      "Progress: Found 1200/1500 papers (Scanned 1246 folders so far)\n",
      "Progress: Found 1250/1500 papers (Scanned 1298 folders so far)\n",
      "Progress: Found 1300/1500 papers (Scanned 1349 folders so far)\n",
      "Progress: Found 1350/1500 papers (Scanned 1400 folders so far)\n",
      "Progress: Found 1400/1500 papers (Scanned 1452 folders so far)\n",
      "Progress: Found 1450/1500 papers (Scanned 1504 folders so far)\n",
      "Progress: Found 1500/1500 papers (Scanned 1554 folders so far)\n",
      "------------------------------\n",
      "Scan finished at folder: 2305-01938\n",
      "Total folders scanned: 2336\n",
      "Papers processed (valid inputs): 1554\n",
      "Papers skipped (invalid inputs): 782\n",
      "Papers with labels written (TARGET): 1500\n",
      "Output: E:\\Milestone2\\hh\\labels\\auto_label.json\n"
     ]
    }
   ],
   "source": [
    "all_folders = sorted([p for p in base_dir.iterdir() if p.is_dir()], key=lambda x: x.name)\n",
    "start_index = next((i for i, p in enumerate(all_folders) if p.name >= start_folder), 0)\n",
    "candidate_folders = all_folders[start_index:] \n",
    "\n",
    "results: Dict[str, Dict[str, Any]] = {}\n",
    "skipped_missing = 0\n",
    "processed = 0\n",
    "skip_details: List[Tuple[str, str]] = []\n",
    "no_label_folders: List[str] = []\n",
    "\n",
    "papers_found_count = 0 \n",
    "\n",
    "print(f\"Starting scan from {start_folder}. Target: {papers_to_check} labeled papers.\")\n",
    "\n",
    "for folder in candidate_folders:\n",
    "    if papers_found_count >= papers_to_check:\n",
    "        break\n",
    "\n",
    "    json_path = folder / \"references.json\"\n",
    "    bib_files = list(folder.glob(\"*.bib\"))\n",
    "\n",
    "    data, json_error = load_json_data(json_path)\n",
    "    if json_error:\n",
    "        skipped_missing += 1\n",
    "        skip_details.append((folder.name, json_error))\n",
    "        continue\n",
    "\n",
    "    if not bib_files:\n",
    "        skipped_missing += 1\n",
    "        skip_details.append((folder.name, \"no_bib_files\"))\n",
    "        continue\n",
    "\n",
    "    entries = parse_bib_entries(bib_files)\n",
    "    if not entries:\n",
    "        skipped_missing += 1\n",
    "        skip_details.append((folder.name, \"bib_entries_empty\"))\n",
    "        continue\n",
    "\n",
    "    records = build_json_records(data)\n",
    "    if not records:\n",
    "        skipped_missing += 1\n",
    "        skip_details.append((folder.name, \"references.json is empty\"))\n",
    "        continue\n",
    "\n",
    "    available = {rec[\"arxiv_id\"] for rec in records.values()}\n",
    "    \n",
    "    labels = {} \n",
    "    \n",
    "    for entry in entries:\n",
    "        arxiv_id, reason = match_entry(entry, records, available)\n",
    "        if arxiv_id:\n",
    "            bib_key = entry.get(\"ID\") or entry.get(\"id\") or entry.get(\"key\") or \"unknown\"\n",
    "            labels[bib_key] = arxiv_id\n",
    "            available.discard(arxiv_id)\n",
    "            \n",
    "    if labels:\n",
    "        results[folder.name] = labels\n",
    "        papers_found_count += 1 \n",
    "    else:\n",
    "        no_label_folders.append(folder.name)\n",
    "    \n",
    "    processed += 1\n",
    "\n",
    "    if papers_found_count % 50 == 0 and labels:\n",
    "        print(f\"Progress: Found {papers_found_count}/{papers_to_check} papers (Scanned {processed} folders so far)\")\n",
    "        \n",
    "output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "output_file.write_text(json.dumps(results, indent=4), encoding=\"utf-8\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Scan finished at folder: {folder.name}\")\n",
    "print(f\"Total folders scanned: {processed + skipped_missing}\")\n",
    "print(f\"Papers processed (valid inputs): {processed}\")\n",
    "print(f\"Papers skipped (invalid inputs): {skipped_missing}\")\n",
    "print(f\"Papers with labels written (TARGET): {len(results)}\")\n",
    "print(f\"Output: {output_file.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
