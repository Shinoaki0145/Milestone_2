{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca1c7abe",
   "metadata": {},
   "source": [
    "# Hierarchical Parsing and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1dab0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Dict, Any, Set, Tuple\n",
    "from enum import Enum\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import concurrent.futures\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87fb41ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeType(Enum):\n",
    "    \"\"\"Types of nodes in the document hierarchy\"\"\"\n",
    "    DOCUMENT = \"document\"\n",
    "    CHAPTER = \"chapter\"\n",
    "    SECTION = \"section\"\n",
    "    SUBSECTION = \"subsection\"\n",
    "    SUBSUBSECTION = \"subsubsection\"\n",
    "    PARAGRAPH = \"paragraph\"\n",
    "    SUBPARAGRAPH = \"subparagraph\"\n",
    "    # Leaf nodes\n",
    "    SENTENCE = \"sentence\"\n",
    "    BLOCK_FORMULA = \"block_formula\"\n",
    "    FIGURE = \"figure\"\n",
    "    TABLE = \"table\"\n",
    "    # Special\n",
    "    ABSTRACT = \"abstract\"\n",
    "    ACKNOWLEDGMENTS = \"acknowledgments\"\n",
    "    APPENDIX = \"appendix\"\n",
    "\n",
    "\n",
    "# Hierarchy order for determining nesting levels\n",
    "HIERARCHY_ORDER = [\n",
    "    NodeType.DOCUMENT,\n",
    "    NodeType.CHAPTER,\n",
    "    NodeType.SECTION,\n",
    "    NodeType.SUBSECTION,\n",
    "    NodeType.SUBSUBSECTION,\n",
    "    NodeType.PARAGRAPH,\n",
    "    NodeType.SUBPARAGRAPH,\n",
    "]\n",
    "\n",
    "# Leaf node types (smallest elements)\n",
    "LEAF_TYPES = {NodeType.SENTENCE, NodeType.BLOCK_FORMULA, NodeType.FIGURE, NodeType.TABLE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a91f730",
   "metadata": {},
   "outputs": [],
   "source": [
    "NODE_ID_LENGTH = 16\n",
    "\n",
    "@dataclass\n",
    "class HierarchyNode:\n",
    "    \"\"\"A node in the document hierarchy tree\"\"\"\n",
    "    node_type: NodeType\n",
    "    title: str = \"\"\n",
    "    content: str = \"\"\n",
    "    children: List['HierarchyNode'] = field(default_factory=list)\n",
    "    label: str = \"\"\n",
    "    source_file: str = \"\"\n",
    "    content_hash: str = \"\"\n",
    "    unique_id: str = \"\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # Generate content hash first (used for deduplication)\n",
    "        if not self.content_hash and self.content:\n",
    "            # Normalize content for consistent hashing\n",
    "            normalized_content = re.sub(r'\\s+', ' ', self.content.strip().lower())\n",
    "            self.content_hash = hashlib.md5(normalized_content.encode()).hexdigest()\n",
    "        \n",
    "        # Generate unique_id\n",
    "        if not self.unique_id:\n",
    "            # For leaf nodes (sentences, formulas, figures, tables), use content-based ID\n",
    "            # This ensures identical content across versions gets the same ID\n",
    "            if self.node_type in LEAF_TYPES and self.content_hash:\n",
    "                self.unique_id = self.content_hash[:NODE_ID_LENGTH]\n",
    "            else:\n",
    "                # For structural nodes, use type + title + content prefix\n",
    "                self.unique_id = hashlib.md5(\n",
    "                    f\"{self.node_type.value}:{self.title}:{self.content[:100]}\".encode()\n",
    "                ).hexdigest()[:NODE_ID_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f08e3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass \n",
    "class BibEntry:\n",
    "    \"\"\"A bibliography entry\"\"\"\n",
    "    key: str\n",
    "    entry_type: str\n",
    "    fields: Dict[str, str] = field(default_factory=dict)\n",
    "    \n",
    "    def _normalize_field(self, value: str) -> str:\n",
    "        \"\"\"Normalize a field value for comparison\"\"\"\n",
    "        if not value:\n",
    "            return \"\"\n",
    "        # Lowercase, strip whitespace, remove extra spaces\n",
    "        normalized = value.lower().strip()\n",
    "        # Remove LaTeX commands and braces\n",
    "        normalized = re.sub(r'\\\\[a-zA-Z]+\\{([^}]*)\\}', r'\\1', normalized)\n",
    "        # Remove punctuation for comparison\n",
    "        normalized = re.sub(r'[{}\"\\'.,;:\\-]+', ' ', normalized)\n",
    "        # Collapse whitespace\n",
    "        normalized = re.sub(r'\\s+', ' ', normalized).strip()\n",
    "        return normalized\n",
    "    \n",
    "    def get_normalized_title(self) -> str:\n",
    "        \"\"\"Get normalized title for fuzzy comparison\"\"\"\n",
    "        return self._normalize_field(self.fields.get('title', ''))\n",
    "    \n",
    "    def get_normalized_author(self) -> str:\n",
    "        \"\"\"Get normalized author for fuzzy comparison\"\"\"\n",
    "        return self._normalize_field(self.fields.get('author', ''))\n",
    "    \n",
    "    def has_sufficient_fields(self) -> bool:\n",
    "        \"\"\"Check if entry has enough fields for content-based deduplication\"\"\"\n",
    "        title = self.fields.get('title', '').strip()\n",
    "        author = self.fields.get('author', '').strip()\n",
    "        year = self.fields.get('year', '').strip()\n",
    "        doi = self.fields.get('doi', '').strip()\n",
    "        \n",
    "        # Need at least title, or (author + year), or doi\n",
    "        has_title = len(title) > 5\n",
    "        has_author_year = len(author) > 3 and len(year) >= 4\n",
    "        has_doi = len(doi) > 5\n",
    "        \n",
    "        return has_title or has_author_year or has_doi\n",
    "    \n",
    "    def content_hash(self) -> str:\n",
    "        \"\"\"\n",
    "        Generate hash based on content for deduplication.\n",
    "        Returns unique key-based hash if fields are insufficient.\n",
    "        \"\"\"\n",
    "        # If fields are empty or insufficient, return key-based hash to prevent false merges\n",
    "        if not self.fields or not self.has_sufficient_fields():\n",
    "            # Use key as unique identifier - no two entries with same key should merge incorrectly\n",
    "            unique_str = f\"__KEY_ONLY__:{self.key}:{self.entry_type}\"\n",
    "            return hashlib.md5(unique_str.encode()).hexdigest()\n",
    "        \n",
    "        # Build hash from normalized key identifying fields\n",
    "        hash_parts = []\n",
    "        \n",
    "        # Title (primary identifier)\n",
    "        title = self.get_normalized_title()\n",
    "        if title:\n",
    "            hash_parts.append(f\"title:{title}\")\n",
    "        \n",
    "        # Author (secondary identifier)  \n",
    "        author = self.get_normalized_author()\n",
    "        if author:\n",
    "            # Extract first author's last name for more robust matching\n",
    "            first_author = author.split(' and ')[0].strip()\n",
    "            hash_parts.append(f\"author:{first_author}\")\n",
    "        \n",
    "        # Year\n",
    "        year = self.fields.get('year', '').strip()\n",
    "        if year:\n",
    "            hash_parts.append(f\"year:{year}\")\n",
    "        \n",
    "        # DOI (very reliable identifier if present)\n",
    "        doi = self._normalize_field(self.fields.get('doi', ''))\n",
    "        if doi:\n",
    "            hash_parts.append(f\"doi:{doi}\")\n",
    "        \n",
    "        if not hash_parts:\n",
    "            # Fallback to key-based hash\n",
    "            unique_str = f\"__KEY_ONLY__:{self.key}:{self.entry_type}\"\n",
    "            return hashlib.md5(unique_str.encode()).hexdigest()\n",
    "        \n",
    "        content = \"|\".join(sorted(hash_parts))\n",
    "        return hashlib.md5(content.encode()).hexdigest()\n",
    "    \n",
    "    def to_bibtex(self, indent: int = 4) -> str:\n",
    "        indent_str = \" \" * indent\n",
    "        lines = [f\"@{self.entry_type}{{{self.key},\"]\n",
    "        for key, value in self.fields.items():\n",
    "            lines.append(f\"{indent_str}{key} = {{{value}}},\")\n",
    "        lines.append(\"}\")\n",
    "        return \"\\n\".join(lines)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbdf1a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaTeXFileGatherer:\n",
    "    # Patterns to match \\input and \\include commands\n",
    "    PATTERN = re.compile(r'(?m)^(?![^%\\n]*%).*\\\\(?:input|include)\\{([^}]+)\\}')\n",
    "    \n",
    "    def __init__(self, base_dir: str):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.included_files: Set[str] = set()\n",
    "        self.file_contents: Dict[str, str] = {}\n",
    "        self.file_order: List[str] = []\n",
    "        \n",
    "    def gather_files(self, main_file: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Recursively gather all files starting from the main file.\n",
    "        Returns a dict mapping file paths to their contents.\n",
    "        \"\"\"\n",
    "        self.included_files.clear()\n",
    "        self.file_contents.clear()\n",
    "        self.file_order.clear()\n",
    "        \n",
    "        main_path = self.base_dir / main_file\n",
    "        self._process_file(main_path)\n",
    "        \n",
    "        return self.file_contents\n",
    "    \n",
    "    def _resolve_path(self, include_path: str, current_file: Path) -> Path:\n",
    "        \"\"\"Resolve the path of an included file\"\"\"\n",
    "        # Add .tex extension if not present\n",
    "        if not include_path.endswith('.tex'):\n",
    "            include_path += '.tex'\n",
    "        \n",
    "        # Try relative to current file first\n",
    "        relative_path = current_file.parent / include_path\n",
    "        if relative_path.exists():\n",
    "            return relative_path\n",
    "        \n",
    "        # Try relative to base directory\n",
    "        base_relative = self.base_dir / include_path\n",
    "        if base_relative.exists():\n",
    "            return base_relative\n",
    "        \n",
    "        return relative_path  # Return even if doesn't exist for error reporting\n",
    "    \n",
    "    def _process_file(self, file_path: Path) -> str:\n",
    "        \"\"\"Process a single file and recursively process includes\"\"\"\n",
    "        # Normalize path for tracking\n",
    "        normalized_path = str(file_path.resolve())\n",
    "        \n",
    "        if normalized_path in self.included_files:\n",
    "            return \"\"  # Already processed\n",
    "        \n",
    "        if not file_path.exists():\n",
    "            print(f\"Warning: File not found: {file_path}\")\n",
    "            return \"\"\n",
    "        \n",
    "        self.included_files.add(normalized_path)\n",
    "        self.file_order.append(normalized_path)\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                content = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            return \"\"\n",
    "        \n",
    "        # Store original content\n",
    "        self.file_contents[normalized_path] = content\n",
    "        \n",
    "        # Find and process all includes\n",
    "        expanded_content = self._expand_includes(content, file_path)\n",
    "        \n",
    "        return expanded_content\n",
    "    \n",
    "    def _expand_includes(self, content: str, current_file: Path) -> str:\n",
    "        \"\"\"Expand \\input and \\include commands in content\"\"\"\n",
    "        for match in self.PATTERN.finditer(content):\n",
    "            include_path = match.group(1)\n",
    "            resolved_path = self._resolve_path(include_path, current_file)\n",
    "            self._process_file(resolved_path)\n",
    "        \n",
    "        return content\n",
    "    \n",
    "    def get_merged_content(self) -> str:\n",
    "        \"\"\"Get all content merged into a single string with file markers\"\"\"\n",
    "        merged = []\n",
    "        for file_path in self.file_order:\n",
    "            content = self.file_contents.get(file_path, \"\")\n",
    "            # Add file marker for tracking\n",
    "            merged.append(f\"%%% FILE: {file_path} %%%\\n\")\n",
    "            merged.append(content)\n",
    "            merged.append(\"\\n\")\n",
    "        return \"\\n\".join(merged)\n",
    "    \n",
    "    def get_unused_files(self) -> Set[str]:\n",
    "        \"\"\"Get files that exist but are not part of compilation\"\"\"\n",
    "        all_files = set()\n",
    "        for tex_file in self.base_dir.rglob(\"*.tex\"):\n",
    "            all_files.add(str(tex_file.resolve()))\n",
    "        return all_files - self.included_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39fb3ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaTeXCleaner:\n",
    "    \"\"\"Cleans and normalizes LaTeX content\"\"\"\n",
    "    \n",
    "    # Commands to remove (no semantic meaning)\n",
    "    REMOVE_COMMANDS = [\n",
    "        r'\\\\centering',\n",
    "        r'\\\\raggedright',\n",
    "        r'\\\\raggedleft',\n",
    "        r'\\\\noindent',\n",
    "        r'\\\\smallskip',\n",
    "        r'\\\\medskip',\n",
    "        r'\\\\bigskip',\n",
    "        r'\\\\newpage',\n",
    "        r'\\\\clearpage',\n",
    "        r'\\\\pagebreak',\n",
    "        r'\\\\linebreak',\n",
    "        r'\\\\hfill',\n",
    "        r'\\\\vfill',\n",
    "        r'\\\\hspace\\{[^}]*\\}',\n",
    "        r'\\\\vspace\\{[^}]*\\}',\n",
    "        r'\\\\phantom\\{[^}]*\\}',\n",
    "        r'\\\\hphantom\\{[^}]*\\}',\n",
    "        r'\\\\vphantom\\{[^}]*\\}',\n",
    "\n",
    "        r'\\\\par',\n",
    "        r'\\\\parindent\\s*=?\\s*[^\\\\\\n]*',\n",
    "        r'\\\\parskip\\s*=?\\s*[^\\\\\\n]*',\n",
    "        r'\\\\baselineskip\\s*=?\\s*[^\\\\\\n]*',\n",
    "        r'\\\\stretch\\{[^}]*\\}',\n",
    "\n",
    "        # Font / style\n",
    "        r'\\\\textbf\\{([^}]*)\\}',\n",
    "        r'\\\\textit\\{([^}]*)\\}',\n",
    "        r'\\\\emph\\{([^}]*)\\}',\n",
    "        r'\\\\underline\\{([^}]*)\\}',\n",
    "        r'\\\\texttt\\{([^}]*)\\}',\n",
    "        r'\\\\bfseries',\n",
    "        r'\\\\itshape',\n",
    "        r'\\\\ttfamily',\n",
    "        r'\\\\footnotesize',\n",
    "        r'\\\\scriptsize',\n",
    "        r'\\\\tiny',\n",
    "        r'\\\\large',\n",
    "        r'\\\\Large',\n",
    "        r'\\\\LARGE',\n",
    "        r'\\\\huge',\n",
    "        r'\\\\Huge',\n",
    "    ]\n",
    "    \n",
    "    # Table formatting commands to remove\n",
    "    TABLE_FORMATTING = [\n",
    "        r'\\\\toprule',\n",
    "        r'\\\\midrule',\n",
    "        r'\\\\bottomrule',\n",
    "        \n",
    "        r'\\\\addlinespace',\n",
    "        r'\\\\cmidrule\\{[^}]*\\}', \n",
    "    ]\n",
    "    \n",
    "    # Figure/table placement specifiers\n",
    "    PLACEMENT_SPECS = re.compile(r'\\[([htbp!]+)\\]')\n",
    "    \n",
    "    # Inline math patterns (to normalize to $...$)\n",
    "    INLINE_MATH_PATTERNS = [\n",
    "        (re.compile(r'\\\\[(](.*?)\\\\[)]', re.DOTALL), r'$\\1$'),  # \\(...\\)\n",
    "        (re.compile(r'\\\\begin\\{math\\}(.*?)\\\\end\\{math\\}', re.DOTALL), r'$\\1$'),\n",
    "    ]\n",
    "    \n",
    "    # Block math environments (to normalize to equation)\n",
    "    BLOCK_MATH_ENVS = [\n",
    "        'displaymath', 'eqnarray', 'eqnarray*', 'align', 'align*',\n",
    "        'gather', 'gather*', 'multline', 'multline*', 'flalign', 'flalign*'\n",
    "    ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Compile removal patterns\n",
    "        self.remove_patterns = [re.compile(p) for p in self.REMOVE_COMMANDS + self.TABLE_FORMATTING]\n",
    "    \n",
    "    def clean(self, content: str) -> str:\n",
    "        \"\"\"Apply all cleaning operations\"\"\"\n",
    "        result = content\n",
    "        \n",
    "        # Remove comments (but preserve % in math mode)\n",
    "        result = self._remove_comments(result)\n",
    "        result = self.normalize_math(result)\n",
    "        \n",
    "        # Remove formatting commands\n",
    "        for pattern in self.remove_patterns:\n",
    "            result = pattern.sub('', result)\n",
    "        \n",
    "        # Remove placement specifiers\n",
    "        result = self.PLACEMENT_SPECS.sub('', result)\n",
    "        \n",
    "        # Normalize whitespace\n",
    "        result = self._normalize_whitespace(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def normalize_math(self, content: str) -> str:\n",
    "        \"\"\"Normalize all math expressions\"\"\"\n",
    "        result = content\n",
    "        \n",
    "        # Normalize inline math to $...$\n",
    "        for pattern, replacement in self.INLINE_MATH_PATTERNS:\n",
    "            result = pattern.sub(replacement, result)\n",
    "        \n",
    "        # Normalize $$...$$ to equation environment\n",
    "        result = re.sub(\n",
    "            r'\\$\\$(.*?)\\$\\$',\n",
    "            r'\\\\begin{equation}\\1\\\\end{equation}',\n",
    "            result,\n",
    "            flags=re.DOTALL\n",
    "        )\n",
    "\n",
    "        # Normalize \\[...\\] to equation environment\n",
    "        result = re.sub(\n",
    "            r'\\\\\\[(.*?)\\\\\\]',\n",
    "            r'\\\\begin{equation}\\1\\\\end{equation}',\n",
    "            result,\n",
    "            flags=re.DOTALL\n",
    "        )\n",
    "        \n",
    "        # Normalize other block math environments to equation\n",
    "        for env in self.BLOCK_MATH_ENVS:\n",
    "            pattern = re.compile(\n",
    "                rf'\\\\begin\\{{{env}\\}}(.*?)\\\\end\\{{{env}\\}}',\n",
    "                re.DOTALL\n",
    "            )\n",
    "            result = pattern.sub(r'\\\\begin{equation}\\1\\\\end{equation}', result)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def _remove_comments(self, content: str) -> str:\n",
    "        \"\"\"Remove LaTeX comments while preserving escaped %\"\"\"\n",
    "        lines = content.split('\\n')\n",
    "        result_lines = []\n",
    "        for line in lines:\n",
    "            # Find % that are not escaped\n",
    "            new_line = []\n",
    "            i = 0\n",
    "            while i < len(line):\n",
    "                if line[i] == '%' and (i == 0 or line[i-1] != '\\\\'):\n",
    "                    break  # Rest of line is comment\n",
    "                new_line.append(line[i])\n",
    "                i += 1\n",
    "            result_lines.append(''.join(new_line))\n",
    "        return '\\n'.join(result_lines)\n",
    "    \n",
    "    def _normalize_whitespace(self, content: str) -> str:\n",
    "        \"\"\"Normalize whitespace in content\"\"\"\n",
    "        # Replace multiple spaces with single space\n",
    "        result = re.sub(r'[ \\t]+', ' ', content)\n",
    "        # Replace multiple newlines with double newline\n",
    "        result = re.sub(r'\\n{3,}', '\\n\\n', result)\n",
    "        return result.strip()\n",
    "    \n",
    "    def extract_text_content(self, content: str) -> str:\n",
    "        \"\"\"Extract plain text from LaTeX, removing commands\"\"\"\n",
    "        result = content\n",
    "        \n",
    "        # Remove \\command{...} but keep the content in braces\n",
    "        result = re.sub(r'\\\\(?:textbf|textit|textrm|texttt|emph|underline)\\{([^}]*)\\}', r'\\1', result)\n",
    "        \n",
    "        # Remove \\command without braces\n",
    "        result = re.sub(r'\\\\(?:bf|it|rm|tt|em|sc)\\b', '', result)\n",
    "        \n",
    "        # Keep \\cite and \\ref commands as-is (don't replace with placeholders)\n",
    "        # Remove labels\n",
    "        result = re.sub(r'\\\\label\\{[^}]*\\}', '', result)\n",
    "        \n",
    "        # Remove remaining simple commands EXCEPT \\cite, \\citep, \\citet, \\ref, \\eqref, \\autoref\n",
    "        result = re.sub(r'\\\\(?!cite[pt]?\\{)(?!ref\\{)(?!eqref\\{)(?!autoref\\{)[a-zA-Z]+\\*?(?:\\[[^\\]]*\\])?(?:\\{[^}]*\\})?', '', result)\n",
    "        \n",
    "        # Clean up standalone braces (but not those in \\cite{} or \\ref{})\n",
    "        # Only remove braces that are not preceded by a backslash command\n",
    "        result = re.sub(r'(?<!\\\\cite)(?<!\\\\citep)(?<!\\\\citet)(?<!\\\\ref)(?<!\\\\eqref)(?<!\\\\autoref)\\{([^{}]*)\\}', r'\\1', result)\n",
    "        \n",
    "        return result.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a1052d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchyParser:\n",
    "    \"\"\"Parses LaTeX content into a hierarchical structure\"\"\"\n",
    "    \n",
    "    # Section command patterns with their hierarchy levels\n",
    "    SECTION_PATTERNS = [\n",
    "        (r'\\\\chapter\\*?\\{([^}]*)\\}', NodeType.CHAPTER),\n",
    "        (r'\\\\section\\*?\\{([^}]*)\\}', NodeType.SECTION),\n",
    "        (r'\\\\subsection\\*?\\{([^}]*)\\}', NodeType.SUBSECTION),\n",
    "        (r'\\\\subsubsection\\*?\\{([^}]*)\\}', NodeType.SUBSUBSECTION),\n",
    "        (r'\\\\paragraph\\*?\\{([^}]*)\\}', NodeType.PARAGRAPH),\n",
    "        (r'\\\\subparagraph\\*?\\{([^}]*)\\}', NodeType.SUBPARAGRAPH),\n",
    "    ]\n",
    "    \n",
    "    # Block math environments\n",
    "    BLOCK_MATH_ENVS = [\n",
    "        'equation', 'equation*', 'align', 'align*', 'gather', 'gather*',\n",
    "        'multline', 'multline*', 'eqnarray', 'eqnarray*', 'displaymath'\n",
    "    ]\n",
    "    \n",
    "    # Figure/table environments\n",
    "    FLOAT_ENVS = ['figure', 'figure*', 'table', 'table*']\n",
    "    \n",
    "    # References section patterns (to exclude)\n",
    "    REFERENCES_PATTERNS = [\n",
    "        r'\\\\begin\\{thebibliography\\}',\n",
    "        r'\\\\bibliography\\{',\n",
    "        r'\\\\printbibliography',\n",
    "        r'\\\\section\\*?\\{References\\}',\n",
    "        r'\\\\section\\*?\\{Bibliography\\}',\n",
    "        r'\\\\chapter\\*?\\{References\\}',\n",
    "        r'\\\\chapter\\*?\\{Bibliography\\}',\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, cleaner: LaTeXCleaner = None):\n",
    "        self.cleaner = cleaner or LaTeXCleaner()\n",
    "        self._compile_patterns()\n",
    "        \n",
    "    def _compile_patterns(self):\n",
    "        \"\"\"Compile regex patterns for efficiency\"\"\"\n",
    "        self.section_patterns = [\n",
    "            (re.compile(pattern, re.DOTALL), node_type)\n",
    "            for pattern, node_type in self.SECTION_PATTERNS\n",
    "        ]\n",
    "        self.references_pattern = re.compile(\n",
    "            '|'.join(self.REFERENCES_PATTERNS), re.IGNORECASE\n",
    "        )\n",
    "        \n",
    "        # Block math pattern\n",
    "        math_env_pattern = '|'.join(re.escape(env) for env in self.BLOCK_MATH_ENVS)\n",
    "        self.block_math_pattern = re.compile(\n",
    "            rf'\\\\begin\\{{({math_env_pattern})\\}}(.*?)\\\\end\\{{\\1\\}}|'\n",
    "            rf'\\$\\$(.*?)\\$\\$',\n",
    "            re.DOTALL\n",
    "        )\n",
    "        \n",
    "        # Figure/table pattern\n",
    "        float_env_pattern = '|'.join(re.escape(env) for env in self.FLOAT_ENVS)\n",
    "        self.float_pattern = re.compile(\n",
    "            rf'\\\\begin\\{{({float_env_pattern})\\}}(.*?)\\\\end\\{{\\1\\}}',\n",
    "            re.DOTALL\n",
    "        )\n",
    "        \n",
    "        # Label pattern\n",
    "        self.label_pattern = re.compile(r'\\\\label\\{([^}]*)\\}')\n",
    "        \n",
    "        # Caption pattern\n",
    "        self.caption_pattern = re.compile(r'\\\\caption\\{([^}]*)\\}')\n",
    "    \n",
    "    def _is_references_section(self, content: str) -> bool:\n",
    "        \"\"\"Check if content is a references/bibliography section\"\"\"\n",
    "        return bool(self.references_pattern.search(content))\n",
    "    \n",
    "    def _get_hierarchy_level(self, node_type: NodeType) -> int:\n",
    "        \"\"\"Get the hierarchy level of a node type\"\"\"\n",
    "        try:\n",
    "            return HIERARCHY_ORDER.index(node_type)\n",
    "        except ValueError:\n",
    "            return len(HIERARCHY_ORDER)  # Leaf nodes\n",
    "    \n",
    "    def _extract_sections(self, content: str) -> List[Tuple[int, NodeType, str, str]]:\n",
    "        \"\"\"\n",
    "        Extract all section markers from content.\n",
    "        Returns list of (position, node_type, title, full_match)\n",
    "        \"\"\"\n",
    "        sections = []\n",
    "        \n",
    "        # Find all section commands\n",
    "        for pattern, node_type in self.section_patterns:\n",
    "            for match in pattern.finditer(content):\n",
    "                title = match.group(1).strip()\n",
    "                sections.append((match.start(), node_type, title, match.group(0)))\n",
    "        \n",
    "        # Sort by position\n",
    "        sections.sort(key=lambda x: x[0])\n",
    "        return sections\n",
    "    \n",
    "    def _extract_block_formulas(self, content: str) -> List[Tuple[int, int, str]]:\n",
    "        \"\"\"Extract block formulas with their positions\"\"\"\n",
    "        formulas = []\n",
    "        for match in self.block_math_pattern.finditer(content):\n",
    "            formula_content = match.group(2) if match.group(2) else match.group(3)\n",
    "            formulas.append((match.start(), match.end(), formula_content.strip()))\n",
    "        return formulas\n",
    "    \n",
    "    def _extract_floats(self, content: str) -> List[Tuple[int, int, str, str, str]]:\n",
    "        \"\"\"Extract figures and tables with their positions\"\"\"\n",
    "        floats = []\n",
    "        for match in self.float_pattern.finditer(content):\n",
    "            env_type = match.group(1)\n",
    "            env_content = match.group(2)\n",
    "            \n",
    "            # Extract label\n",
    "            label_match = self.label_pattern.search(env_content)\n",
    "            label = label_match.group(1) if label_match else \"\"\n",
    "            \n",
    "            # Extract caption\n",
    "            caption_match = self.caption_pattern.search(env_content)\n",
    "            caption = caption_match.group(1) if caption_match else \"\"\n",
    "            \n",
    "            node_type = NodeType.FIGURE if 'figure' in env_type else NodeType.TABLE\n",
    "            floats.append((match.start(), match.end(), env_content, label, caption, node_type))\n",
    "        \n",
    "        return floats\n",
    "    \n",
    "    def _split_into_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into sentences\"\"\"\n",
    "        # Clean the text first\n",
    "        cleaned = self.cleaner.extract_text_content(text)\n",
    "        \n",
    "        # Remove \\n characters and normalize whitespace\n",
    "        cleaned = cleaned.replace('\\\\n', ' ')\n",
    "        cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "        \n",
    "        # Protect common patterns that shouldn't be split\n",
    "        # Store protected patterns with placeholders\n",
    "        protected_patterns = []\n",
    "        \n",
    "        # Protect [No. followed by anything until ] - common in funding/grant numbers\n",
    "        pattern = r'\\[No\\.\\s*[^\\]]*\\]'\n",
    "        matches = list(re.finditer(pattern, cleaned))\n",
    "        for i, match in enumerate(reversed(matches)):\n",
    "            placeholder = f\"<<<PROTECTED_NO_{len(matches)-i-1}>>>\"\n",
    "            protected_patterns.insert(0, match.group(0))\n",
    "            cleaned = cleaned[:match.start()] + placeholder + cleaned[match.end():]\n",
    "        \n",
    "        # Protect other common abbreviations in brackets\n",
    "        for abbrev in ['e.g.', 'i.e.', 'et al.', 'vs.', 'cf.']:\n",
    "            pattern = re.escape(f'[{abbrev}')\n",
    "            cleaned = cleaned.replace(f'[{abbrev}', f'[{abbrev.replace(\".\", \"<<<DOT>>>\")}')\n",
    "        \n",
    "        # Use NLTK's sentence tokenizer for better sentence splitting\n",
    "        sentences = sent_tokenize(cleaned)\n",
    "        \n",
    "        # Restore protected patterns\n",
    "        for i, protected_text in enumerate(protected_patterns):\n",
    "            placeholder = f\"<<<PROTECTED_NO_{i}>>>\"\n",
    "            sentences = [s.replace(placeholder, protected_text) for s in sentences]\n",
    "        \n",
    "        # Restore dots in abbreviations\n",
    "        sentences = [s.replace('<<<DOT>>>', '.') for s in sentences]\n",
    "        \n",
    "        # Filter empty sentences and strip whitespace\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        return sentences\n",
    "    \n",
    "    def _create_leaf_nodes(self, content: str, source_file: str = \"\") -> List[HierarchyNode]:\n",
    "        \"\"\"Create leaf nodes (sentences, formulas, figures) from content\"\"\"\n",
    "        nodes = []\n",
    "        \n",
    "        # Check if this is a references section\n",
    "        if self._is_references_section(content):\n",
    "            return nodes\n",
    "        \n",
    "        # Extract formulas and floats first\n",
    "        formulas = self._extract_block_formulas(content)\n",
    "        floats = self._extract_floats(content)\n",
    "        \n",
    "        # Mark positions of formulas and floats\n",
    "        excluded_ranges = []\n",
    "        \n",
    "        for start, end, formula_content in formulas:\n",
    "            excluded_ranges.append((start, end))\n",
    "            nodes.append(HierarchyNode(\n",
    "                node_type=NodeType.BLOCK_FORMULA,\n",
    "                content=formula_content,\n",
    "                source_file=source_file\n",
    "            ))\n",
    "        \n",
    "        for start, end, float_content, label, caption, node_type in floats:\n",
    "            excluded_ranges.append((start, end))\n",
    "            nodes.append(HierarchyNode(\n",
    "                node_type=node_type,\n",
    "                title=caption,\n",
    "                content=float_content,\n",
    "                label=label,\n",
    "                source_file=source_file\n",
    "            ))\n",
    "        \n",
    "        # Sort excluded ranges\n",
    "        excluded_ranges.sort()\n",
    "        \n",
    "        # Extract text between excluded ranges\n",
    "        text_segments = []\n",
    "        last_end = 0\n",
    "        for start, end in excluded_ranges:\n",
    "            if start > last_end:\n",
    "                text_segments.append(content[last_end:start])\n",
    "            last_end = max(last_end, end)\n",
    "        if last_end < len(content):\n",
    "            text_segments.append(content[last_end:])\n",
    "        \n",
    "        # Split text into sentences\n",
    "        full_text = ' '.join(text_segments)\n",
    "        sentences = self._split_into_sentences(full_text)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if sentence and len(sentence) > 10:  # Filter very short fragments\n",
    "                nodes.append(HierarchyNode(\n",
    "                    node_type=NodeType.SENTENCE,\n",
    "                    content=sentence,\n",
    "                    source_file=source_file\n",
    "                ))\n",
    "        \n",
    "        return nodes\n",
    "    \n",
    "    def _extract_abstract(self, content: str, source_file: str):\n",
    "        pattern = re.compile(\n",
    "            r'\\\\begin\\{abstract\\}(.*?)\\\\end\\{abstract\\}',\n",
    "            re.DOTALL | re.IGNORECASE\n",
    "        )\n",
    "        match = pattern.search(content)\n",
    "        if not match:\n",
    "            return content, None\n",
    "\n",
    "        abstract_raw = match.group(1)\n",
    "\n",
    "        abstract_node = HierarchyNode(\n",
    "            node_type=NodeType.ABSTRACT,\n",
    "            title=\"Abstract\",\n",
    "            source_file=source_file\n",
    "        )\n",
    "\n",
    "        abstract_node.children = self._create_leaf_nodes(\n",
    "            abstract_raw, source_file\n",
    "        )\n",
    "\n",
    "        # remove abstract khỏi content chính\n",
    "        content = content[:match.start()] + content[match.end():]\n",
    "\n",
    "        return content, abstract_node\n",
    "    \n",
    "    def _extract_acknowledgments_content(self, content: str, start: int) -> str:\n",
    "        \"\"\"\n",
    "        Extract acknowledgments content until references/bibliography\n",
    "        \"\"\"\n",
    "        tail = content[start:]\n",
    "\n",
    "        ref_match = self.references_pattern.search(tail)\n",
    "        if ref_match:\n",
    "            return tail[:ref_match.start()]\n",
    "        return tail\n",
    "\n",
    "\n",
    "    \n",
    "    def parse(self, content: str, source_file: str = \"\") -> HierarchyNode:\n",
    "        \"\"\"Parse LaTeX content into a hierarchical tree\"\"\"\n",
    "        # Create root document node\n",
    "        root = HierarchyNode(\n",
    "            node_type=NodeType.DOCUMENT,\n",
    "            title=\"Document\",\n",
    "            source_file=source_file\n",
    "        )\n",
    "        \n",
    "        # Clean content\n",
    "        cleaned_content = self.cleaner.clean(content)\n",
    "\n",
    "        cleaned_content, abstract_node = self._extract_abstract(cleaned_content, source_file)\n",
    "        if abstract_node:\n",
    "            root.children.append(abstract_node)\n",
    "        \n",
    "        # Extract sections\n",
    "        sections = self._extract_sections(cleaned_content)\n",
    "        \n",
    "        if not sections:\n",
    "            # No sections found, create leaf nodes directly under root\n",
    "            root.children = self._create_leaf_nodes(cleaned_content, source_file)\n",
    "            return root\n",
    "        \n",
    "        # Build hierarchy using a stack\n",
    "        stack = [(root, -1)]  # (node, hierarchy_level)\n",
    "        \n",
    "        for i, (pos, node_type, title, full_match) in enumerate(sections):\n",
    "            # Check if this is a references section\n",
    "            is_ack = 'acknowledg' in title.lower()\n",
    "\n",
    "            if self._is_references_section(title) and not is_ack:\n",
    "                continue\n",
    "\n",
    "            # Get content until next section\n",
    "            if i + 1 < len(sections):\n",
    "                next_pos = sections[i + 1][0]\n",
    "            else:\n",
    "                next_pos = len(cleaned_content)\n",
    "            \n",
    "            content_start = pos + len(full_match)\n",
    "\n",
    "            if is_ack:\n",
    "                section_content = self._extract_acknowledgments_content(\n",
    "                    cleaned_content, content_start\n",
    "                )\n",
    "                node_type = NodeType.ACKNOWLEDGMENTS\n",
    "            else:\n",
    "                section_content = cleaned_content[content_start:next_pos]\n",
    "            \n",
    "            level = self._get_hierarchy_level(node_type)\n",
    "            \n",
    "            # Find parent node\n",
    "            while stack and stack[-1][1] >= level:\n",
    "                stack.pop()\n",
    "            \n",
    "            parent = stack[-1][0] if stack else root\n",
    "            \n",
    "            # Create section node\n",
    "            section_node = HierarchyNode(\n",
    "                node_type=node_type,\n",
    "                title=title,\n",
    "                source_file=source_file\n",
    "            )\n",
    "            \n",
    "            # Extract label if present at start of section\n",
    "            label_match = self.label_pattern.search(section_content[:200])\n",
    "            if label_match:\n",
    "                section_node.label = label_match.group(1)\n",
    "            \n",
    "            # Add leaf nodes (sentences, formulas, figures)\n",
    "            section_node.children = self._create_leaf_nodes(section_content, source_file)\n",
    "            \n",
    "            parent.children.append(section_node)\n",
    "            stack.append((section_node, level))\n",
    "        \n",
    "        return root\n",
    "    \n",
    "    def parse_with_file_markers(self, merged_content: str) -> HierarchyNode:\n",
    "        \"\"\"Parse merged content that has file markers\"\"\"\n",
    "        # Split by file markers\n",
    "        file_pattern = re.compile(r'%%% FILE: (.+?) %%%\\n')\n",
    "        parts = file_pattern.split(merged_content)\n",
    "        \n",
    "        # Create root\n",
    "        root = HierarchyNode(\n",
    "            node_type=NodeType.DOCUMENT,\n",
    "            title=\"Document\"\n",
    "        )\n",
    "        \n",
    "        # Process each file's content\n",
    "        current_file = \"\"\n",
    "        for i, part in enumerate(parts):\n",
    "            if i % 2 == 1:  # File path\n",
    "                current_file = part\n",
    "            elif part.strip():  # Content\n",
    "                file_root = self.parse(part, current_file)\n",
    "                # Merge children into main root\n",
    "                root.children.extend(file_root.children)\n",
    "        \n",
    "        return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac6c187",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BibTeXExtractor:\n",
    "    \"\"\"Extract and convert bibliography entries from .bbl files and .bib files\"\"\"\n",
    "    \n",
    "    # Pattern to match \\bibitem entries in .bbl files\n",
    "    BIBITEM_PATTERN = re.compile(\n",
    "        r'\\\\bibitem\\[([^\\]]*)\\]\\{([^}]+)\\}\\s*(.*?)(?=\\\\bibitem|\\Z|\\\\end\\{thebibliography\\})',\n",
    "        re.DOTALL\n",
    "    )\n",
    "    \n",
    "    # Alternative pattern without optional argument\n",
    "    BIBITEM_SIMPLE_PATTERN = re.compile(\n",
    "        r'\\\\bibitem\\{([^}]+)\\}\\s*(.*?)(?=\\\\bibitem|\\Z|\\\\end\\{thebibliography\\})',\n",
    "        re.DOTALL\n",
    "    )\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.entries: Dict[str, BibEntry] = {}\n",
    "    \n",
    "    def _parse_bib_entry(self, content: str, start_pos: int) -> Optional[Tuple[str, str, str, int]]:\n",
    "        \"\"\"\n",
    "        Parse a single BibTeX entry starting from @type{key,...}\n",
    "        Returns (entry_type, key, fields_content, end_pos) or None\n",
    "        Handles nested braces correctly.\n",
    "        \"\"\"\n",
    "        # Match @type{key,\n",
    "        entry_start = re.match(r'@(\\w+)\\s*\\{\\s*([^,\\s]+)\\s*,', content[start_pos:])\n",
    "        if not entry_start:\n",
    "            return None\n",
    "        \n",
    "        entry_type = entry_start.group(1).lower()\n",
    "        key = entry_start.group(2).strip()\n",
    "        \n",
    "        # Find matching closing brace by counting\n",
    "        brace_count = 1\n",
    "        pos = start_pos + entry_start.end()\n",
    "        \n",
    "        while pos < len(content) and brace_count > 0:\n",
    "            if content[pos] == '{':\n",
    "                brace_count += 1\n",
    "            elif content[pos] == '}':\n",
    "                brace_count -= 1\n",
    "            pos += 1\n",
    "        \n",
    "        if brace_count != 0:\n",
    "            return None\n",
    "        \n",
    "        # Extract fields content (between key, and final })\n",
    "        fields_start = start_pos + entry_start.end()\n",
    "        fields_content = content[fields_start:pos-1]\n",
    "        \n",
    "        return (entry_type, key, fields_content, pos)\n",
    "    \n",
    "    def _parse_field_value(self, content: str, start_pos: int) -> Tuple[str, int]:\n",
    "        \"\"\"\n",
    "        Parse a field value that can be:\n",
    "        - {nested {braces} allowed}\n",
    "        - \"quoted string\"\n",
    "        - bare_number\n",
    "        Returns (value, end_pos)\n",
    "        \"\"\"\n",
    "        pos = start_pos\n",
    "        \n",
    "        # Skip whitespace\n",
    "        while pos < len(content) and content[pos] in ' \\t\\n\\r':\n",
    "            pos += 1\n",
    "        \n",
    "        if pos >= len(content):\n",
    "            return (\"\", pos)\n",
    "        \n",
    "        # Case 1: Braced value {....}\n",
    "        if content[pos] == '{':\n",
    "            brace_count = 1\n",
    "            value_start = pos + 1\n",
    "            pos += 1\n",
    "            while pos < len(content) and brace_count > 0:\n",
    "                if content[pos] == '{':\n",
    "                    brace_count += 1\n",
    "                elif content[pos] == '}':\n",
    "                    brace_count -= 1\n",
    "                pos += 1\n",
    "            value = content[value_start:pos-1]\n",
    "            return (value, pos)\n",
    "        \n",
    "        # Case 2: Quoted value \"...\"\n",
    "        elif content[pos] == '\"':\n",
    "            value_start = pos + 1\n",
    "            pos += 1\n",
    "            while pos < len(content) and content[pos] != '\"':\n",
    "                if content[pos] == '\\\\' and pos + 1 < len(content):\n",
    "                    pos += 2  # Skip escaped char\n",
    "                else:\n",
    "                    pos += 1\n",
    "            value = content[value_start:pos]\n",
    "            if pos < len(content):\n",
    "                pos += 1  # Skip closing quote\n",
    "            return (value, pos)\n",
    "        \n",
    "        # Case 3: Bare value (number or string constant)\n",
    "        else:\n",
    "            value_start = pos\n",
    "            while pos < len(content) and content[pos] not in ',}\\n':\n",
    "                pos += 1\n",
    "            value = content[value_start:pos].strip()\n",
    "            return (value, pos)\n",
    "    \n",
    "    def _parse_fields(self, fields_content: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Parse all fields from the content between @type{key, ... }\n",
    "        Handles multi-line values and nested braces.\n",
    "        \"\"\"\n",
    "        fields = {}\n",
    "        pos = 0\n",
    "        \n",
    "        while pos < len(fields_content):\n",
    "            # Skip whitespace and commas\n",
    "            while pos < len(fields_content) and fields_content[pos] in ' \\t\\n\\r,':\n",
    "                pos += 1\n",
    "            \n",
    "            if pos >= len(fields_content):\n",
    "                break\n",
    "            \n",
    "            # Match field name followed by =\n",
    "            field_match = re.match(r'(\\w+)\\s*=\\s*', fields_content[pos:])\n",
    "            if not field_match:\n",
    "                pos += 1\n",
    "                continue\n",
    "            \n",
    "            field_name = field_match.group(1).lower()\n",
    "            pos += field_match.end()\n",
    "            \n",
    "            # Parse field value\n",
    "            value, pos = self._parse_field_value(fields_content, pos)\n",
    "            \n",
    "            # Handle string concatenation with #\n",
    "            while pos < len(fields_content):\n",
    "                # Skip whitespace\n",
    "                temp_pos = pos\n",
    "                while temp_pos < len(fields_content) and fields_content[temp_pos] in ' \\t\\n\\r':\n",
    "                    temp_pos += 1\n",
    "                \n",
    "                if temp_pos < len(fields_content) and fields_content[temp_pos] == '#':\n",
    "                    temp_pos += 1\n",
    "                    additional_value, temp_pos = self._parse_field_value(fields_content, temp_pos)\n",
    "                    value += additional_value\n",
    "                    pos = temp_pos\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # Clean up the value\n",
    "            value = re.sub(r'\\s+', ' ', value).strip()\n",
    "            fields[field_name] = value\n",
    "        \n",
    "        return fields\n",
    "        \n",
    "    def parse_bib_file(self, content: str) -> Dict[str, BibEntry]:\n",
    "        \"\"\"Parse a .bib file and extract entries with robust multi-line support\"\"\"\n",
    "        entries = {}\n",
    "        \n",
    "        # Find all @type{ patterns\n",
    "        entry_pattern = re.compile(r'@\\w+\\s*\\{', re.IGNORECASE)\n",
    "        \n",
    "        for match in entry_pattern.finditer(content):\n",
    "            result = self._parse_bib_entry(content, match.start())\n",
    "            if result:\n",
    "                entry_type, key, fields_content, end_pos = result\n",
    "                \n",
    "                # Skip comments\n",
    "                if entry_type == 'comment':\n",
    "                    continue\n",
    "                \n",
    "                # Parse fields\n",
    "                fields = self._parse_fields(fields_content)\n",
    "                \n",
    "                if key and (fields or entry_type in ['string', 'preamble']):\n",
    "                    entries[key] = BibEntry(key=key, entry_type=entry_type, fields=fields)\n",
    "        \n",
    "        return entries\n",
    "    \n",
    "    def parse_bbl_file(self, content: str) -> Dict[str, BibEntry]:\n",
    "        \"\"\"Parse a .bbl file and convert bibitem entries to BibTeX format\"\"\"\n",
    "        entries = {}\n",
    "        \n",
    "        # Try pattern with optional citation label\n",
    "        for match in self.BIBITEM_PATTERN.finditer(content):\n",
    "            cite_label = match.group(1)\n",
    "            key = match.group(2).strip()\n",
    "            entry_content = match.group(3).strip()\n",
    "            \n",
    "            entry = self._parse_bibitem_content(key, entry_content)\n",
    "            if entry:\n",
    "                entries[key] = entry\n",
    "        \n",
    "        # If no entries found, try simple pattern\n",
    "        if not entries:\n",
    "            for match in self.BIBITEM_SIMPLE_PATTERN.finditer(content):\n",
    "                key = match.group(1).strip()\n",
    "                entry_content = match.group(2).strip()\n",
    "                \n",
    "                entry = self._parse_bibitem_content(key, entry_content)\n",
    "                if entry:\n",
    "                    entries[key] = entry\n",
    "        \n",
    "        return entries\n",
    "    \n",
    "    def _parse_bibitem_content(self, key: str, content: str) -> Optional[BibEntry]:\n",
    "        \"\"\"\n",
    "        Parse the content of a bibitem and extract bibliographic fields.\n",
    "        \n",
    "        Uses \\newblock as anchor points to separate:\n",
    "        - Author (before first \\newblock)\n",
    "        - Title (second \\newblock)\n",
    "        - Publication info (remaining \\newblocks)\n",
    "        \"\"\"\n",
    "        fields = {}\n",
    "        original_content = content\n",
    "        \n",
    "        content = re.sub(r'\\s+', ' ', content).strip()\n",
    "        \n",
    "        parts = content.split('\\\\newblock')\n",
    "        parts = [p.strip() for p in parts if p.strip()]\n",
    "        \n",
    "        # If no \\newblock found or content is too short, treat as unstructured\n",
    "        if len(parts) < 2 or len(content) < 50:\n",
    "            # Save cleaned content as note\n",
    "            cleaned = self._clean_latex(content)\n",
    "            if cleaned:\n",
    "                fields['note'] = cleaned[:500]\n",
    "            # Try to extract year anyway\n",
    "            year_match = re.search(r'\\b(19|20)\\d{2}\\b', content)\n",
    "            if year_match:\n",
    "                fields['year'] = year_match.group(0)\n",
    "            return BibEntry(key=key, entry_type='misc', fields=fields)\n",
    "        \n",
    "        # Extract Author (first part)\n",
    "        author_part = parts[0]\n",
    "        # Remove trailing period if present\n",
    "        author_part = re.sub(r'\\.\\s*$', '', author_part)\n",
    "        # Clean LaTeX formatting commands but keep the text\n",
    "        author_part = re.sub(r'\\\\[a-zA-Z]+\\{([^}]*)\\}', r'\\1', author_part)\n",
    "        author_part = re.sub(r'[{}]', '', author_part)\n",
    "        if author_part:\n",
    "            fields['author'] = author_part\n",
    "        \n",
    "        # Extract Title (second part)\n",
    "        if len(parts) >= 2:\n",
    "            title_part = parts[1]\n",
    "            # Remove trailing period\n",
    "            title_part = re.sub(r'\\.\\s*$', '', title_part)\n",
    "            # Clean LaTeX but keep text\n",
    "            title_part = re.sub(r'\\\\[a-zA-Z]+\\{([^}]*)\\}', r'\\1', title_part)\n",
    "            title_part = re.sub(r'[{}]', '', title_part).strip()\n",
    "            if title_part:\n",
    "                # Wrap in braces to preserve capitalization in BibTeX\n",
    "                fields['title'] = '{' + title_part + '}'\n",
    "        \n",
    "        # Extract Publication Info (remaining parts)\n",
    "        full_content = content  # Use full content for extraction\n",
    "        \n",
    "        # Extract year (4 digits: 19xx or 20xx)\n",
    "        year_match = re.search(r'\\b(19|20)\\d{2}\\b', full_content)\n",
    "        if year_match:\n",
    "            fields['year'] = year_match.group(0)\n",
    "        \n",
    "        # Extract journal from \\emph{...} or {\\em ...}\n",
    "        journal_match = re.search(r'\\\\emph\\{([^}]+)\\}', full_content)\n",
    "        if not journal_match:\n",
    "            journal_match = re.search(r'\\{\\\\em\\s+([^}]+)\\}', full_content)\n",
    "        if journal_match:\n",
    "            venue = journal_match.group(1).strip()\n",
    "            # Determine if journal or conference proceedings\n",
    "            if any(kw in venue.lower() for kw in ['proc', 'conf', 'workshop', 'symposium', 'international']):\n",
    "                fields['booktitle'] = venue\n",
    "            else:\n",
    "                fields['journal'] = venue\n",
    "        \n",
    "        # Extract Volume:Page format (e.g., \"87:085115\" or \"87:1--50\")\n",
    "        vol_page_match = re.search(r'(\\d+):(\\d+(?:--?\\d+)?)', full_content)\n",
    "        if vol_page_match:\n",
    "            fields['volume'] = vol_page_match.group(1)\n",
    "            pages = vol_page_match.group(2).replace('–', '--').replace('-', '--')\n",
    "            # Normalize single dash to double dash\n",
    "            if '--' not in pages and re.match(r'\\d+\\d+', pages):\n",
    "                pass  # Single page number, keep as is\n",
    "            fields['pages'] = pages\n",
    "        else:\n",
    "            # Try separate volume and pages patterns\n",
    "            volume_match = re.search(r'vol(?:ume)?\\.?\\s*(\\d+)', full_content, re.IGNORECASE)\n",
    "            if volume_match:\n",
    "                fields['volume'] = volume_match.group(1)\n",
    "            \n",
    "            pages_match = re.search(r'pages?\\s*[:\\s]*(\\d+(?:\\s*[-–]\\s*\\d+)?)', full_content, re.IGNORECASE)\n",
    "            if pages_match:\n",
    "                fields['pages'] = pages_match.group(1).replace('–', '--')\n",
    "        \n",
    "        # Extract URL\n",
    "        url_match = re.search(r'\\\\url\\{([^}]+)\\}', full_content)\n",
    "        if url_match:\n",
    "            fields['url'] = url_match.group(1)\n",
    "        \n",
    "        # Extract DOI\n",
    "        doi_match = re.search(r'doi[:\\s]*([^\\s,}]+)', full_content, re.IGNORECASE)\n",
    "        if doi_match:\n",
    "            fields['doi'] = doi_match.group(1)\n",
    "        \n",
    "        # Determine entry type\n",
    "        entry_type = self._guess_entry_type(fields, full_content)\n",
    "        \n",
    "        # If still missing key fields, add note with original content\n",
    "        if 'title' not in fields and 'author' not in fields:\n",
    "            fields['note'] = self._clean_latex(original_content)[:500]\n",
    "        \n",
    "        return BibEntry(key=key, entry_type=entry_type, fields=fields)\n",
    "    \n",
    "    def _clean_latex(self, content: str) -> str:\n",
    "        \"\"\"Remove common LaTeX formatting commands\"\"\"\n",
    "        # Remove \\newblock\n",
    "        content = re.sub(r'\\\\newblock\\s*', '', content)\n",
    "        # Remove common formatting\n",
    "        content = re.sub(r'\\\\textit\\{([^}]*)\\}', r'\\1', content)\n",
    "        content = re.sub(r'\\\\textbf\\{([^}]*)\\}', r'\\1', content)\n",
    "        content = re.sub(r'\\\\texttt\\{([^}]*)\\}', r'\\1', content)\n",
    "        content = re.sub(r'\\\\emph\\{([^}]*)\\}', r'\\1', content)\n",
    "        # Remove escaped characters\n",
    "        content = content.replace('\\\\&', '&')\n",
    "        content = content.replace('\\\\~', '~')\n",
    "        content = content.replace('\\\\{', '{')\n",
    "        content = content.replace('\\\\}', '}')\n",
    "        # Normalize whitespace\n",
    "        content = re.sub(r'\\s+', ' ', content)\n",
    "        return content.strip()\n",
    "    \n",
    "    def _guess_entry_type(self, fields: Dict[str, str], content: str) -> str:\n",
    "        \"\"\"Guess the BibTeX entry type based on available fields\"\"\"\n",
    "        content_lower = content.lower()\n",
    "        \n",
    "        if 'booktitle' in fields or 'proceedings' in content_lower or 'conference' in content_lower:\n",
    "            return 'inproceedings'\n",
    "        elif 'journal' in fields:\n",
    "            return 'article'\n",
    "        elif 'url' in fields and ('howpublished' in content_lower or 'accessed' in content_lower):\n",
    "            return 'misc'\n",
    "        elif 'publisher' in content_lower or 'press' in content_lower:\n",
    "            return 'book'\n",
    "        elif 'thesis' in content_lower:\n",
    "            if 'phd' in content_lower or 'doctoral' in content_lower:\n",
    "                return 'phdthesis'\n",
    "            elif 'master' in content_lower:\n",
    "                return 'mastersthesis'\n",
    "        elif 'arxiv' in content_lower:\n",
    "            return 'article'\n",
    "        \n",
    "        return 'misc'\n",
    "    \n",
    "    def parse_tex_bibitems(self, content: str) -> Dict[str, BibEntry]:\n",
    "        \"\"\"\n",
    "        Parse \\bibitem entries directly from .tex file content.\n",
    "        This handles cases where bibliography is defined inline in the tex file.\n",
    "        \"\"\"\n",
    "        entries = {}\n",
    "        \n",
    "        # Look for thebibliography environment\n",
    "        bib_env_pattern = re.compile(\n",
    "            r'\\\\begin\\{thebibliography\\}.*?(.*?)\\\\end\\{thebibliography\\}',\n",
    "            re.DOTALL\n",
    "        )\n",
    "        \n",
    "        bib_match = bib_env_pattern.search(content)\n",
    "        if bib_match:\n",
    "            bib_content = bib_match.group(1)\n",
    "            \n",
    "            # Try pattern with optional citation label first\n",
    "            for match in self.BIBITEM_PATTERN.finditer(bib_content):\n",
    "                cite_label = match.group(1)\n",
    "                key = match.group(2).strip()\n",
    "                entry_content = match.group(3).strip()\n",
    "                \n",
    "                entry = self._parse_bibitem_content(key, entry_content)\n",
    "                if entry:\n",
    "                    entries[key] = entry\n",
    "            \n",
    "            # If no entries found, try simple pattern\n",
    "            if not entries:\n",
    "                for match in self.BIBITEM_SIMPLE_PATTERN.finditer(bib_content):\n",
    "                    key = match.group(1).strip()\n",
    "                    entry_content = match.group(2).strip()\n",
    "                    \n",
    "                    entry = self._parse_bibitem_content(key, entry_content)\n",
    "                    if entry:\n",
    "                        entries[key] = entry\n",
    "        \n",
    "        return entries\n",
    "    \n",
    "    def extract_citation_keys(self, content: str) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Extract all citation keys from \\cite, \\citep, \\citet commands.\n",
    "        Handles multiple keys in one command: \\cite{key1,key2,key3}\n",
    "        \"\"\"\n",
    "        citation_keys = set()\n",
    "        \n",
    "        # Pattern to match \\cite, \\citep, \\citet with optional arguments\n",
    "        # Matches: \\cite{key1,key2}, \\citep[pre][post]{key1,key2}, etc.\n",
    "        cite_patterns = [\n",
    "            re.compile(r'\\\\cite(?:p|t)?(?:\\[[^\\]]*\\])*(?:\\[[^\\]]*\\])?\\{([^}]+)\\}', re.IGNORECASE),\n",
    "            re.compile(r'\\\\citep(?:\\[[^\\]]*\\])*(?:\\[[^\\]]*\\])?\\{([^}]+)\\}', re.IGNORECASE),\n",
    "            re.compile(r'\\\\citet(?:\\[[^\\]]*\\])*(?:\\[[^\\]]*\\])?\\{([^}]+)\\}', re.IGNORECASE),\n",
    "        ]\n",
    "        \n",
    "        for pattern in cite_patterns:\n",
    "            for match in pattern.finditer(content):\n",
    "                keys_str = match.group(1)\n",
    "                # Split by comma and strip whitespace\n",
    "                keys = [k.strip() for k in keys_str.split(',') if k.strip()]\n",
    "                citation_keys.update(keys)\n",
    "        \n",
    "        return citation_keys\n",
    "    \n",
    "    def load_from_directory(self, base_dir: str, used_citation_keys: Optional[Set[str]] = None) -> Dict[str, BibEntry]:\n",
    "        \"\"\"\n",
    "        Load all bibliography entries from .bib, .bbl, and .tex files in directory.\n",
    "        If used_citation_keys is provided, only load entries that are actually cited.\n",
    "        \"\"\"\n",
    "        base_path = Path(base_dir)\n",
    "        \n",
    "        # Load .bib files (highest priority - already in BibTeX format)\n",
    "        for bib_file in base_path.rglob('*.bib'):\n",
    "            try:\n",
    "                with open(bib_file, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                    content = f.read()\n",
    "                entries = self.parse_bib_file(content)\n",
    "                \n",
    "                # Filter by used keys if provided\n",
    "                if used_citation_keys is not None:\n",
    "                    entries = {k: v for k, v in entries.items() if k in used_citation_keys}\n",
    "                \n",
    "                self.entries.update(entries)\n",
    "                if entries:\n",
    "                    print(f\"    Loaded {len(entries)} entries from {bib_file.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {bib_file}: {e}\")\n",
    "\n",
    "        \n",
    "        # Load \\bibitem entries from .tex files\n",
    "        for tex_file in base_path.rglob('*.tex'):\n",
    "            try:\n",
    "                with open(tex_file, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                # Only parse if file contains thebibliography environment\n",
    "                if r'\\begin{thebibliography}' in content:\n",
    "                    entries = self.parse_tex_bibitems(content)\n",
    "                    \n",
    "                    # Filter by used keys if provided\n",
    "                    if used_citation_keys is not None:\n",
    "                        entries = {k: v for k, v in entries.items() if k in used_citation_keys}\n",
    "                    \n",
    "                    # Only add entries not already present\n",
    "                    new_count = 0\n",
    "                    for key, entry in entries.items():\n",
    "                        if key not in self.entries:\n",
    "                            self.entries[key] = entry\n",
    "                            new_count += 1\n",
    "                    if new_count:\n",
    "                        print(f\"    Loaded {new_count} bibitem entries from {tex_file.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading bibitems from {tex_file}: {e}\")\n",
    "        \n",
    "        return self.entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502eb56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deduplicator:\n",
    "    \"\"\"Handles deduplication of references and content\"\"\"\n",
    "    \n",
    "    # Minimum similarity threshold for title fuzzy matching\n",
    "    TITLE_SIMILARITY_THRESHOLD = 0.6\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.content_hashes: Dict[str, str] = {}  # hash -> unique_id\n",
    "        self.reference_hashes: Dict[str, str] = {}  # content_hash -> key\n",
    "        self.key_mappings: Dict[str, str] = {}  # old_key -> canonical_key\n",
    "        \n",
    "    # Reference Deduplication\n",
    "    \n",
    "    def _titles_are_similar(self, entry1: BibEntry, entry2: BibEntry) -> bool:\n",
    "        \"\"\"\n",
    "        Check if two entries have similar titles using fuzzy matching.\n",
    "        Returns True if titles are similar enough to be considered the same reference.\n",
    "        Returns True if either entry lacks a title (can't verify, allow merge based on other fields).\n",
    "        \"\"\"\n",
    "        title1 = entry1.get_normalized_title()\n",
    "        title2 = entry2.get_normalized_title()\n",
    "        \n",
    "        # If either is empty, we can't compare - allow based on other matching criteria\n",
    "        if not title1 or not title2:\n",
    "            return True\n",
    "        \n",
    "        # Quick exact match\n",
    "        if title1 == title2:\n",
    "            return True\n",
    "        \n",
    "        # Compute Jaccard similarity on words\n",
    "        words1 = set(title1.split())\n",
    "        words2 = set(title2.split())\n",
    "        \n",
    "        if not words1 or not words2:\n",
    "            return True\n",
    "        \n",
    "        intersection = len(words1 & words2)\n",
    "        union = len(words1 | words2)\n",
    "        \n",
    "        similarity = intersection / union if union > 0 else 0.0\n",
    "        \n",
    "        return similarity >= self.TITLE_SIMILARITY_THRESHOLD\n",
    "    \n",
    "    def _create_merged_entry(self, canonical: BibEntry, duplicate: BibEntry) -> BibEntry:\n",
    "        \"\"\"\n",
    "        Create a new merged entry without mutating the originals.\n",
    "        Fields from duplicate are only added if canonical lacks them.\n",
    "        \"\"\"\n",
    "        # Create a copy of canonical's fields\n",
    "        merged_fields = dict(canonical.fields)\n",
    "        \n",
    "        # Add missing fields from duplicate (don't overwrite existing)\n",
    "        for field, value in duplicate.fields.items():\n",
    "            if field not in merged_fields or not merged_fields[field].strip():\n",
    "                if value and value.strip():\n",
    "                    merged_fields[field] = value\n",
    "        \n",
    "        # Return new BibEntry (don't modify original)\n",
    "        return BibEntry(\n",
    "            key=canonical.key,\n",
    "            entry_type=canonical.entry_type,\n",
    "            fields=merged_fields\n",
    "        )\n",
    "    \n",
    "    def deduplicate_references(self, entries: Dict[str, BibEntry]) -> Dict[str, BibEntry]:\n",
    "        \"\"\"\n",
    "        Deduplicate bibliography entries safely.\n",
    "        \n",
    "        - Creates new objects instead of mutating originals\n",
    "        - Verifies title similarity before merging (fuzzy match)\n",
    "        - Only merges fields that are missing in canonical entry\n",
    "        \"\"\"\n",
    "        deduplicated: Dict[str, BibEntry] = {}\n",
    "        hash_to_key: Dict[str, str] = {}\n",
    "        self.key_mappings.clear()\n",
    "        \n",
    "        for key, entry in entries.items():\n",
    "            content_hash = entry.content_hash()\n",
    "            \n",
    "            if content_hash in hash_to_key:\n",
    "                canonical_key = hash_to_key[content_hash]\n",
    "                canonical_entry = deduplicated[canonical_key]\n",
    "                \n",
    "                # SAFETY CHECK: Verify titles are actually similar\n",
    "                if not self._titles_are_similar(canonical_entry, entry):\n",
    "                    # Hash collision but different content - treat as unique\n",
    "                    # Use a modified hash to differentiate\n",
    "                    unique_hash = f\"{content_hash}_{key}\"\n",
    "                    hash_to_key[unique_hash] = key\n",
    "                    # Create a copy to avoid mutation\n",
    "                    deduplicated[key] = BibEntry(\n",
    "                        key=entry.key,\n",
    "                        entry_type=entry.entry_type,\n",
    "                        fields=dict(entry.fields)\n",
    "                    )\n",
    "                    self.key_mappings[key] = key\n",
    "                    continue\n",
    "                \n",
    "                # Titles match - safe to merge\n",
    "                # Create new merged entry (don't mutate canonical)\n",
    "                merged_entry = self._create_merged_entry(canonical_entry, entry)\n",
    "                deduplicated[canonical_key] = merged_entry\n",
    "                \n",
    "                self.key_mappings[key] = canonical_key\n",
    "                print(f\"Merged duplicate: {key} -> {canonical_key}\")\n",
    "            else:\n",
    "                # New unique entry - create a copy\n",
    "                hash_to_key[content_hash] = key\n",
    "                deduplicated[key] = BibEntry(\n",
    "                    key=entry.key,\n",
    "                    entry_type=entry.entry_type,\n",
    "                    fields=dict(entry.fields)\n",
    "                )\n",
    "                self.key_mappings[key] = key\n",
    "\n",
    "        self.reference_hashes = hash_to_key\n",
    "        return deduplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06918bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaTeXParser:\n",
    "    \"\"\"\n",
    "    Main LaTeX parser that integrates all components:\n",
    "    - Multi-file gathering\n",
    "    - Hierarchy construction\n",
    "    - Reference extraction\n",
    "    - Deduplication\n",
    "    \"\"\"\n",
    "    \n",
    "    ARXIV_ID_PATTERN = re.compile(r'\\d{4}-\\d{4,5}')\n",
    "    \n",
    "    def __init__(self, base_dir: str, paper_id: Optional[str] = None):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.gatherer = LaTeXFileGatherer(base_dir)\n",
    "        self.cleaner = LaTeXCleaner()\n",
    "        self.hierarchy_parser = HierarchyParser(self.cleaner)\n",
    "        self.bib_extractor = BibTeXExtractor()\n",
    "        self.deduplicator = Deduplicator()\n",
    "        \n",
    "        # Results\n",
    "        self.hierarchy: Optional[HierarchyNode] = None\n",
    "        self.references: Dict[str, BibEntry] = {}\n",
    "        self.file_contents: Dict[str, str] = {}\n",
    "        self.paper_id = paper_id or self._infer_paper_id()\n",
    "        \n",
    "    def _infer_paper_id(self) -> Optional[str]:\n",
    "        \"\"\"Best-effort extraction of the arXiv-style paper id from the directory path.\"\"\"\n",
    "        search_paths = [self.base_dir] + list(self.base_dir.parents)\n",
    "        for path in search_paths:\n",
    "            matches = self.ARXIV_ID_PATTERN.findall(str(path))\n",
    "            if matches:\n",
    "                return matches[-1]\n",
    "        return None\n",
    "\n",
    "    def _apply_node_ids(self, node: Optional[HierarchyNode]):\n",
    "        \"\"\"Prefix all node IDs with the paper id to keep them globally unique.\"\"\"\n",
    "        if not node or not self.paper_id:\n",
    "            return\n",
    "\n",
    "        prefix = self.paper_id.strip()\n",
    "        if not prefix:\n",
    "            return\n",
    "\n",
    "        def assign(current: HierarchyNode):\n",
    "            if current.unique_id:\n",
    "                if not current.unique_id.startswith(f\"{prefix}|\"):\n",
    "                    current.unique_id = f\"{prefix}|{current.unique_id}\"\n",
    "            else:\n",
    "                current.unique_id = prefix\n",
    "            for child in current.children:\n",
    "                assign(child)\n",
    "\n",
    "        assign(node)\n",
    "        \n",
    "    def parse(self, main_file: str = \"main.tex\") -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Parse the LaTeX document starting from the main file.\n",
    "        Returns a dictionary with hierarchy, references, and statistics.\n",
    "        \"\"\"\n",
    "        print(f\"Parsing LaTeX document from: {self.base_dir / main_file}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Multi-file gathering\n",
    "        print(\"\\n[1] Gathering files...\")\n",
    "        self.file_contents = self.gatherer.gather_files(main_file)\n",
    "        print(f\"    Found {len(self.file_contents)} files in compilation path\")\n",
    "        \n",
    "        # Report unused files\n",
    "        unused = self.gatherer.get_unused_files()\n",
    "        if unused:\n",
    "            print(f\"    Unused files ({len(unused)}):\")\n",
    "            for f in list(unused)[:5]:\n",
    "                print(f\"      - {Path(f).name}\")\n",
    "            if len(unused) > 5:\n",
    "                print(f\"      ... and {len(unused) - 5} more\")\n",
    "        \n",
    "        # Step 2: Build hierarchy\n",
    "        print(\"\\n[2] Building hierarchy...\")\n",
    "        merged_content = self.gatherer.get_merged_content()\n",
    "        self.hierarchy = self.hierarchy_parser.parse_with_file_markers(merged_content)\n",
    "        \n",
    "        hierarchy_stats = self._count_hierarchy_nodes(self.hierarchy)\n",
    "        print(f\"    Built hierarchy with {hierarchy_stats['total']} nodes:\")\n",
    "        for node_type, count in hierarchy_stats['by_type'].items():\n",
    "            print(f\"      - {node_type}: {count}\")\n",
    "        \n",
    "        # Step 3: Extract citation keys and references\n",
    "        print(\"\\n[3] Extracting references...\")\n",
    "        # Extract all citation keys from the merged content\n",
    "        used_citation_keys = self.bib_extractor.extract_citation_keys(merged_content)\n",
    "        print(f\"    Found {len(used_citation_keys)} unique citation keys in document\")\n",
    "        \n",
    "        # Only load references that are actually cited\n",
    "        self.references = self.bib_extractor.load_from_directory(str(self.base_dir), used_citation_keys=used_citation_keys)\n",
    "        print(f\"    Loaded {len(self.references)} bibliography entries (only cited references)\")\n",
    "        \n",
    "        # Finalize IDs with paper prefix\n",
    "        self._apply_node_ids(self.hierarchy)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Parsing complete!\")\n",
    "        \n",
    "        return {\n",
    "            'hierarchy': self.hierarchy,\n",
    "            'references': self.references,\n",
    "            'file_contents': self.file_contents,\n",
    "            'stats': {\n",
    "                'files': len(self.file_contents),\n",
    "                'unused_files': len(unused),\n",
    "                'hierarchy_nodes': hierarchy_stats,\n",
    "                'references': len(self.references)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _count_hierarchy_nodes(self, node: HierarchyNode) -> Dict[str, Any]:\n",
    "        \"\"\"Count nodes in hierarchy by type\"\"\"\n",
    "        stats = {'total': 0, 'by_type': defaultdict(int)}\n",
    "        \n",
    "        def count(n: HierarchyNode):\n",
    "            stats['total'] += 1\n",
    "            stats['by_type'][n.node_type.value] += 1\n",
    "            for child in n.children:\n",
    "                count(child)\n",
    "        \n",
    "        count(node)\n",
    "        return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1336c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MilestoneExporter:\n",
    "    def __init__(self):\n",
    "        self.elements: Dict[str, str] = {}  # unique_id -> cleaned content\n",
    "        self.hierarchy: Dict[str, Dict[str, str]] = {}  # version -> {child_id: parent_id}\n",
    "        \n",
    "    def export_document(self, root: HierarchyNode, version: str = \"1\") -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Export a parsed document to the milestone format.\n",
    "        \n",
    "        Args:\n",
    "            root: The root HierarchyNode of the parsed document\n",
    "            version: Version identifier (e.g., \"1\", \"2\", etc.)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary in the required format\n",
    "        \"\"\"\n",
    "        self.elements.clear()\n",
    "        self.hierarchy.clear()\n",
    "        \n",
    "        # Process the hierarchy tree\n",
    "        self._process_node(root, parent_id=None, version=version)\n",
    "        \n",
    "        return {\n",
    "            \"elements\": self.elements,\n",
    "            \"hierarchy\": self.hierarchy\n",
    "        }\n",
    "    \n",
    "    def _process_node(self, node: HierarchyNode, parent_id: Optional[str], version: str):\n",
    "        \"\"\"Recursively process nodes to extract elements and build hierarchy\"\"\"\n",
    "        \n",
    "        # Initialize version hierarchy if not exists\n",
    "        if version not in self.hierarchy:\n",
    "            self.hierarchy[version] = {}\n",
    "        \n",
    "        current_id = node.unique_id\n",
    "        \n",
    "        # For leaf nodes, store the content in elements\n",
    "        if node.node_type in LEAF_TYPES:\n",
    "            if node.content:\n",
    "                # Only store if not already present (deduplication)\n",
    "                if current_id not in self.elements:\n",
    "                    self.elements[current_id] = node.content\n",
    "                \n",
    "                # Add to hierarchy (child -> parent relationship)\n",
    "                if parent_id:\n",
    "                    self.hierarchy[version][current_id] = parent_id\n",
    "        else:\n",
    "            # For non-leaf nodes, store section/chapter as full LaTeX command\n",
    "            if node.title:\n",
    "                # Map node type to LaTeX command\n",
    "                type_to_cmd = {\n",
    "                    NodeType.CHAPTER: \"chapter\",\n",
    "                    NodeType.SECTION: \"section\",\n",
    "                    NodeType.SUBSECTION: \"subsection\",\n",
    "                    NodeType.SUBSUBSECTION: \"subsubsection\",\n",
    "                    NodeType.PARAGRAPH: \"paragraph\",\n",
    "                    NodeType.SUBPARAGRAPH: \"subparagraph\",\n",
    "                    NodeType.ABSTRACT: \"abstract\",\n",
    "                    NodeType.ACKNOWLEDGMENTS: \"section\",\n",
    "                    NodeType.APPENDIX: \"appendix\",\n",
    "                }\n",
    "                cmd = type_to_cmd.get(node.node_type, node.node_type.value)\n",
    "                content = f\"\\\\{cmd}{{{node.title}}}\"\n",
    "                if current_id not in self.elements:\n",
    "                    self.elements[current_id] = content\n",
    "            \n",
    "            # Add to hierarchy\n",
    "            if parent_id:\n",
    "                self.hierarchy[version][current_id] = parent_id\n",
    "                \n",
    "        # Process children\n",
    "        for child in node.children:\n",
    "            self._process_node(child, current_id, version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02e40886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common main file names\n",
    "MAIN_FILE_CANDIDATES = [\n",
    "    'main.tex', 'paper.tex', 'article.tex', 'manuscript.tex',\n",
    "    'thesis.tex', 'document.tex', 'root.tex'\n",
    "]\n",
    "\n",
    "# Document class pattern to identify main files\n",
    "DOCUMENT_CLASS_PATTERN = re.compile(r'\\\\documentclass', re.IGNORECASE)\n",
    "BEGIN_DOCUMENT_PATTERN = re.compile(r'\\\\begin\\{document\\}', re.IGNORECASE)\n",
    "AUTHOR_DOCUMENT_PATTERN = re.compile(r'\\\\author', re.IGNORECASE)\n",
    "\n",
    "def find_main_file(tex_dir: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Find the main LaTeX file in a directory.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Look for common main file names\n",
    "    2. Look for files with \\\\documentclass that also have \\\\begin{document}\n",
    "    3. If only one .tex file at root level, use that\n",
    "    \"\"\"\n",
    "    tex_path = Path(tex_dir)\n",
    "    \n",
    "    if not tex_path.exists():\n",
    "        return None\n",
    "    \n",
    "    # Strategy 1: Check common names\n",
    "    for candidate in MAIN_FILE_CANDIDATES:\n",
    "        candidate_path = tex_path / candidate\n",
    "        if candidate_path.exists():\n",
    "            return candidate\n",
    "    \n",
    "    # Strategy 2: Find files with documentclass AND begin{document}\n",
    "    root_tex_files = list(tex_path.glob('*.tex'))\n",
    "    main_candidates = []\n",
    "    \n",
    "    for tex_file in root_tex_files:\n",
    "        try:\n",
    "            with open(tex_file, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            has_docclass = DOCUMENT_CLASS_PATTERN.search(content)\n",
    "            has_begin_doc = BEGIN_DOCUMENT_PATTERN.search(content)\n",
    "            has_author = AUTHOR_DOCUMENT_PATTERN.search(content)\n",
    "            \n",
    "            if has_docclass and has_begin_doc and has_author:\n",
    "                main_candidates.append(tex_file.name)\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    if len(main_candidates) == 1:\n",
    "        return main_candidates[0]\n",
    "    elif len(main_candidates) > 1:\n",
    "        # Prefer shorter filenames (less likely to be appendix, etc.)\n",
    "        return min(main_candidates, key=len)\n",
    "    \n",
    "    # Strategy 3: If only one tex file at root level\n",
    "    if len(root_tex_files) == 1:\n",
    "        return root_tex_files[0].name\n",
    "    \n",
    "    # Strategy 4: Look for any tex file with documentclass\n",
    "    for tex_file in root_tex_files:\n",
    "        try:\n",
    "            with open(tex_file, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                content = f.read()\n",
    "            if DOCUMENT_CLASS_PATTERN.search(content):\n",
    "                return tex_file.name\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e40886",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiVersionProcessor:\n",
    "    \"\"\"Process multiple versions of the same paper.\"\"\"\n",
    "    \n",
    "    def __init__(self, paper_dir: str):\n",
    "        self.paper_dir = Path(paper_dir)\n",
    "        self.arxiv_id = self.paper_dir.name\n",
    "        # self.tex_dir = self.paper_dir / 'tex'\n",
    "        \n",
    "        # Check if tex/ subdirectory exists, otherwise use paper_dir directly\n",
    "        tex_dir_candidate = self.paper_dir / 'tex'\n",
    "        self.tex_dir = tex_dir_candidate if tex_dir_candidate.exists() else self.paper_dir\n",
    "        \n",
    "        self.versions: Dict[str, Path] = {}\n",
    "        self.results: Dict[str, Any] = {}\n",
    "        \n",
    "    def discover_versions(self) -> List[str]:\n",
    "        \"\"\"Find all version directories (e.g., 2305-14596v1, 2305-14596v2).\"\"\"\n",
    "        self.versions.clear()\n",
    "        \n",
    "        if not self.tex_dir.exists():\n",
    "            return []\n",
    "        \n",
    "        # Look for version folders\n",
    "        version_pattern = re.compile(rf'{re.escape(self.arxiv_id)}v(\\d+)', re.IGNORECASE)\n",
    "        \n",
    "        for item in self.tex_dir.iterdir():\n",
    "            if item.is_dir():\n",
    "                match = version_pattern.match(item.name)\n",
    "                if match:\n",
    "                    version_num = match.group(1)\n",
    "                    self.versions[version_num] = item\n",
    "        \n",
    "        # Sort versions numerically\n",
    "        return sorted(self.versions.keys(), key=int)\n",
    "    \n",
    "    def parse_version(self, version: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Parse a specific version of the paper.\"\"\"\n",
    "        if version not in self.versions:\n",
    "            return None\n",
    "        \n",
    "        version_dir = self.versions[version]\n",
    "        main_file = find_main_file(str(version_dir))\n",
    "        \n",
    "        if not main_file:\n",
    "            print(f\"  Warning: Could not find main file in {version_dir}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            parser = LaTeXParser(str(version_dir))\n",
    "            result = parser.parse(main_file)\n",
    "            return {\n",
    "                'parser': parser,\n",
    "                'result': result,\n",
    "                'main_file': main_file\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"  Error parsing version {version}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_all_versions(self) -> Dict[str, Any]:\n",
    "        \"\"\"Process all versions and return combined results.\"\"\"\n",
    "        versions = self.discover_versions()\n",
    "        \n",
    "        if not versions:\n",
    "            print(f\"  No versions found for {self.arxiv_id}\")\n",
    "            return {}\n",
    "        \n",
    "        print(f\"  Found {len(versions)} version(s): {versions}\")\n",
    "        \n",
    "        for version in versions:\n",
    "            print(f\"\\n  Processing version {version}...\")\n",
    "            result = self.parse_version(version)\n",
    "            if result:\n",
    "                self.results[version] = result\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def export_combined(self, output_dir: str) -> Optional[Path]:\n",
    "        \"\"\"\n",
    "        Export combined results for all versions.\n",
    "        Elements are deduplicated across versions.\n",
    "        Each version has its own hierarchy.\n",
    "        References are deduplicated and merged across versions.\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            return None\n",
    "        \n",
    "        exporter = MilestoneExporter()\n",
    "        combined_elements = {}\n",
    "        combined_hierarchy = {}\n",
    "        \n",
    "        # Collect all references from all versions for cross-version deduplication\n",
    "        # Note: Each version's parser.references already contains only cited references\n",
    "        # Create copies to avoid mutating original parser references\n",
    "        all_references: Dict[str, BibEntry] = {}\n",
    "        for version, data in self.results.items():\n",
    "            parser = data['parser']\n",
    "            for key, entry in parser.references.items():\n",
    "                if key not in all_references:\n",
    "                    # Create a copy of the entry\n",
    "                    all_references[key] = BibEntry(\n",
    "                        key=entry.key,\n",
    "                        entry_type=entry.entry_type,\n",
    "                        fields=dict(entry.fields)\n",
    "                    )\n",
    "                else:\n",
    "                    # Merge fields from this version's entry (unionize)\n",
    "                    existing = all_references[key]\n",
    "                    for field, value in entry.fields.items():\n",
    "                        if field not in existing.fields or not existing.fields[field].strip():\n",
    "                            if value and value.strip():\n",
    "                                existing.fields[field] = value\n",
    "        \n",
    "        # Deduplicate references across all versions\n",
    "        # This handles entries with DIFFERENT keys but SAME content\n",
    "        deduplicator = Deduplicator()\n",
    "        original_count = len(all_references)\n",
    "        deduplicated_refs = deduplicator.deduplicate_references(all_references)\n",
    "        \n",
    "        if original_count > len(deduplicated_refs):\n",
    "            print(f\"  Cross-version reference deduplication: {original_count} -> {len(deduplicated_refs)} entries\")\n",
    "        \n",
    "        print(f\"  Exporting {len(deduplicated_refs)} unique cited references to .bib file\")\n",
    "        \n",
    "        for version, data in self.results.items():\n",
    "            parser = data['parser']\n",
    "            if parser.hierarchy:\n",
    "                # Export this version\n",
    "                version_data = exporter.export_document(parser.hierarchy, version=version)\n",
    "                \n",
    "                # Merge elements (deduplicated by ID)\n",
    "                combined_elements.update(version_data['elements'])\n",
    "                \n",
    "                # Add version hierarchy\n",
    "                combined_hierarchy.update(version_data['hierarchy'])\n",
    "        \n",
    "        # Create output structure\n",
    "        output_data = {\n",
    "            'elements': combined_elements,\n",
    "            'hierarchy': combined_hierarchy\n",
    "        }\n",
    "        \n",
    "        # Create output directory\n",
    "        out_path = Path(output_dir) / self.arxiv_id\n",
    "        out_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save main JSON file\n",
    "        main_json = out_path / \"hierarchy.json\"\n",
    "        with open(main_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "        # Export deduplicated BibTeX references (already merged across versions)\n",
    "        all_bibtex = {}\n",
    "        for key, entry in deduplicated_refs.items():\n",
    "            all_bibtex[key] = BibEntry(\n",
    "                key=key,\n",
    "                entry_type=entry.entry_type,\n",
    "                fields=entry.fields\n",
    "            )\n",
    "        \n",
    "        if all_bibtex:\n",
    "            bibtex_json = out_path / \"refs.bib\"\n",
    "            \n",
    "            with open(bibtex_json, 'w', encoding='utf-8') as f:\n",
    "                for entry in all_bibtex.values():\n",
    "                    f.write(entry.to_bibtex() + \"\\n\\n\")\n",
    "        \n",
    "        return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b4f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchProcessor:\n",
    "    def __init__(self, folder: str, output_folder: str = None):\n",
    "        self.folder = Path(folder)\n",
    "        self.output_folder = Path(output_folder) if output_folder else self.folder.parent / f\"{self.folder.name}_output\"\n",
    "        \n",
    "        self.papers: List[Path] = []\n",
    "        self.results: Dict[str, Any] = {}\n",
    "        \n",
    "        self.lock = threading.Lock()\n",
    "        self.stats = {\n",
    "            'total_papers': 0,\n",
    "            'processed': 0,\n",
    "            'failed': 0,\n",
    "            'skipped': 0,\n",
    "            'total_elements': 0,\n",
    "            'total_versions': 0\n",
    "        }\n",
    "    \n",
    "    def discover_papers(self) -> List[str]:\n",
    "        \"\"\"Find all paper directories in the student folder.\"\"\"\n",
    "        self.papers.clear()\n",
    "        \n",
    "        # Look for directories matching arXiv ID pattern\n",
    "        arxiv_pattern = re.compile(r'\\d{4}-\\d{4,5}')\n",
    "        \n",
    "        for item in sorted(self.folder.iterdir()):\n",
    "            if item.is_dir() and arxiv_pattern.match(item.name):\n",
    "                self.papers.append(item)\n",
    "        \n",
    "        self.stats['total_papers'] = len(self.papers)\n",
    "        return [p.name for p in self.papers]\n",
    "    \n",
    "    def process_paper(self, paper_dir: Path) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Process a single paper with all its versions.\"\"\"\n",
    "        processor = MultiVersionProcessor(str(paper_dir))\n",
    "        results = processor.process_all_versions()\n",
    "        \n",
    "        # Create output folder for this paper regardless of whether it has tex files\n",
    "        paper_output_path = self.output_folder / paper_dir.name\n",
    "        paper_output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Always copy metadata.json and references.json if they exist\n",
    "        metadata_src = paper_dir / 'metadata.json'\n",
    "        if metadata_src.exists():\n",
    "            shutil.copy(metadata_src, paper_output_path / 'metadata.json')\n",
    "        \n",
    "        references_src = paper_dir / 'references.json'\n",
    "        if references_src.exists():\n",
    "            shutil.copy(references_src, paper_output_path / 'references.json')\n",
    "        \n",
    "        if results:\n",
    "            output_path = processor.export_combined(str(self.output_folder))\n",
    "            \n",
    "            # Calculate stats\n",
    "            total_elements = 0\n",
    "            for version_data in results.values():\n",
    "                parser = version_data.get('parser')\n",
    "                if parser and parser.hierarchy:\n",
    "                    def count_nodes(node):\n",
    "                        count = 1\n",
    "                        for child in node.children:\n",
    "                            count += count_nodes(child)\n",
    "                        return count\n",
    "                    total_elements += count_nodes(parser.hierarchy)\n",
    "            \n",
    "            return {\n",
    "                'arxiv_id': paper_dir.name,\n",
    "                'versions_processed': len(results),\n",
    "                'output_path': output_path,\n",
    "                'total_elements': total_elements,\n",
    "                'has_tex': True\n",
    "            }\n",
    "        else:\n",
    "            # Paper has no tex files, but we still copied metadata/references\n",
    "            return {\n",
    "                'arxiv_id': paper_dir.name,\n",
    "                'versions_processed': 0,\n",
    "                'output_path': paper_output_path,\n",
    "                'total_elements': 0,\n",
    "                'has_tex': False\n",
    "            }\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _safe_process_wrapper(self, paper_dir):\n",
    "        \"\"\"Wrapper để bắt lỗi trong luồng con và trả về kết quả an toàn.\"\"\"\n",
    "        try:\n",
    "            return self.process_paper(paper_dir), None\n",
    "        except Exception as e:\n",
    "            return None, e\n",
    "        \n",
    "    def process_all(self, limit: int = None, max_workers: int = 20) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process all papers in the folder using Multi-threading.\n",
    "        \n",
    "        Args:\n",
    "            limit: Maximum number of papers to process\n",
    "            max_workers: Number of threads (default: 10)\n",
    "        \"\"\"\n",
    "        papers = self.discover_papers()\n",
    "        \n",
    "        if not papers:\n",
    "            print(f\"No papers found in {self.folder}\")\n",
    "            return self.stats\n",
    "        \n",
    "        print(f\"\\nFound {len(papers)} papers. Processing with {max_workers} threads...\")\n",
    "        print(f\"Output folder: {self.output_folder}\")\n",
    "        print()\n",
    "        \n",
    "        self.output_folder.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        papers_to_process = self.papers[:limit] if limit else self.papers\n",
    "        \n",
    "        # ThreadPoolExecutor\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_paper = {\n",
    "                executor.submit(self._safe_process_wrapper, p): p \n",
    "                for p in papers_to_process\n",
    "            }\n",
    "            \n",
    "            for i, future in enumerate(concurrent.futures.as_completed(future_to_paper)):\n",
    "                \n",
    "                paper_dir = future_to_paper[future]\n",
    "                arxiv_id = paper_dir.name\n",
    "                \n",
    "                # In tiến độ thủ công (Optional - để biết code vẫn đang chạy)\n",
    "                print(f\"[{i+1}/{len(papers_to_process)}] Finished {arxiv_id}\")\n",
    "\n",
    "                result, error = future.result()\n",
    "                \n",
    "                with self.lock:\n",
    "                    if error:\n",
    "                        self.stats['failed'] += 1\n",
    "                        print(f\"Error processing {arxiv_id}: {error}\")\n",
    "                    elif result:\n",
    "                        self.results[arxiv_id] = result\n",
    "                        if result.get('has_tex', True):\n",
    "                            self.stats['processed'] += 1\n",
    "                            self.stats['total_versions'] += result['versions_processed']\n",
    "                            self.stats['total_elements'] += result['total_elements']\n",
    "                        else:\n",
    "                            # Paper processed but no tex files (only metadata copy)\n",
    "                            self.stats['processed'] += 1\n",
    "                    else:\n",
    "                        self.stats['skipped'] += 1\n",
    "\n",
    "        print(\"Final Stats:\", self.stats)\n",
    "        \n",
    "        return self.stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b60bb5",
   "metadata": {},
   "source": [
    "# Main execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4c4f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOLDER = \"23127238/2304-14745\"\n",
    "# OUTPUT_FOLDER = \"23127238_output\"\n",
    "\n",
    "# # Initialize batch processor\n",
    "# batch_processor = BatchProcessor(FOLDER, OUTPUT_FOLDER)\n",
    "\n",
    "# paper = batch_processor.process_paper(Path(FOLDER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146c28ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18 papers in 231272388\n",
      "\n",
      "Found 18 papers. Processing with 4 threads...\n",
      "Output folder: 231272388_output\n",
      "\n",
      "  Found 2 version(s): ['1', '2']\n",
      "\n",
      "  Processing version 1...\n",
      "  Found 1 version(s): ['1']\n",
      "\n",
      "  Processing version 1...\n",
      "  Found 3 version(s): ['1', '2', '3']\n",
      "\n",
      "  Processing version 1...\n",
      "  Found 2 version(s): ['1', '2']\n",
      "\n",
      "  Processing version 1...\n",
      "Parsing LaTeX document from: 231272388\\2305-02118\\tex\\2305-02118v1\\main.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "Parsing LaTeX document from: 231272388\\2305-02105\\tex\\2305-02105v1\\acl2023.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "Parsing LaTeX document from: 231272388\\2305-02104\\tex\\2305-02104v1\\old.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "    Unused files (2):\n",
      "      - acl2023.tex\n",
      "      - notes.tex\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 169 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 148\n",
      "      - section: 11\n",
      "      - figure: 1\n",
      "      - table: 4\n",
      "      - subsection: 3\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 23 unique citation keys in document\n",
      "    Built hierarchy with 201 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 158\n",
      "      - section: 13\n",
      "      - figure: 8\n",
      "      - subsection: 13\n",
      "      - block_formula: 1\n",
      "      - subsubsection: 2\n",
      "      - table: 4\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 42 unique citation keys in document\n",
      "    Found 8 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 284 nodes:\n",
      "      - document: 1\n",
      "      - section: 8\n",
      "      - figure: 10\n",
      "      - sentence: 226\n",
      "      - block_formula: 16\n",
      "      - subsection: 14\n",
      "      - table: 8\n",
      "      - acknowledgments: 1\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 55 unique citation keys in document\n",
      "Parsing LaTeX document from: 231272388\\2305-02073\\tex\\2305-02073v1\\acl2023.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 11 files in compilation path\n",
      "    Unused files (1):\n",
      "      - problem_state.tex\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 241 nodes:\n",
      "      - document: 1\n",
      "      - section: 7\n",
      "      - sentence: 202\n",
      "      - subsection: 11\n",
      "      - block_formula: 11\n",
      "      - figure: 1\n",
      "      - subsubsection: 3\n",
      "      - table: 5\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 41 unique citation keys in document\n",
      "    Loaded 1 entries from anthology.bib\n",
      "    Loaded 22 entries from Bibliography.bib\n",
      "    Loaded 2 entries from anthology.bib\n",
      "    Loaded 39 entries from custom.bib\n",
      "    Loaded 23 bibliography entries (only cited references)\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "  Exporting 23 unique cited references to .bib file\n",
      "[1/18] Finished 2305-02104\n",
      "  Found 2 version(s): ['1', '2']\n",
      "\n",
      "  Processing version 1...\n",
      "Parsing LaTeX document from: 231272388\\2305-02151\\tex\\2305-02151v1\\eacl2023.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 4 files in compilation path\n",
      "    Loaded 41 bibliography entries (only cited references)\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "\n",
      "  Processing version 2...\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 142 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 113\n",
      "      - section: 11\n",
      "      - subsection: 8\n",
      "      - block_formula: 2\n",
      "      - figure: 3\n",
      "      - table: 3\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 29 unique citation keys in document\n",
      "Parsing LaTeX document from: 231272388\\2305-02073\\tex\\2305-02073v2\\acl2023.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 11 files in compilation path\n",
      "    Unused files (1):\n",
      "      - problem_state.tex\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 244 nodes:\n",
      "      - document: 1\n",
      "      - section: 7\n",
      "      - sentence: 205\n",
      "      - subsection: 11\n",
      "      - block_formula: 11\n",
      "      - figure: 1\n",
      "      - subsubsection: 3\n",
      "      - table: 5\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 41 unique citation keys in document\n",
      "    Loaded 22 entries from anthology.bib\n",
      "    Loaded 20 entries from custom.bib\n",
      "    Loaded 42 bibliography entries (only cited references)\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "\n",
      "  Processing version 2...\n",
      "Parsing LaTeX document from: 231272388\\2305-02105\\tex\\2305-02105v2\\acl2023.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 212 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 164\n",
      "      - section: 11\n",
      "      - figure: 10\n",
      "      - subsection: 16\n",
      "      - block_formula: 1\n",
      "      - subsubsection: 2\n",
      "      - table: 6\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 42 unique citation keys in document\n",
      "    Loaded 8 entries from anthology.bib\n",
      "    Loaded 47 entries from custom.bib\n",
      "    Loaded 55 bibliography entries (only cited references)\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "\n",
      "  Processing version 2...\n",
      "Parsing LaTeX document from: 231272388\\2305-02118\\tex\\2305-02118v2\\main.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 8 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 281 nodes:\n",
      "      - document: 1\n",
      "      - section: 8\n",
      "      - figure: 10\n",
      "      - sentence: 223\n",
      "      - block_formula: 16\n",
      "      - subsection: 14\n",
      "      - table: 8\n",
      "      - acknowledgments: 1\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 59 unique citation keys in document\n",
      "    Loaded 29 entries from export-data.bib\n",
      "    Loaded 29 bibliography entries (only cited references)\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "\n",
      "  Processing version 2...\n",
      "Parsing LaTeX document from: 231272388\\2305-02151\\tex\\2305-02151v2\\eacl2023.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 4 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 142 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 113\n",
      "      - section: 11\n",
      "      - subsection: 8\n",
      "      - block_formula: 2\n",
      "      - figure: 3\n",
      "      - table: 3\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 29 unique citation keys in document\n",
      "    Loaded 2 entries from anthology.bib\n",
      "    Loaded 39 entries from custom.bib\n",
      "    Loaded 41 bibliography entries (only cited references)\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "  Exporting 41 unique cited references to .bib file\n",
      "[2/18] Finished 2305-02073\n",
      "  Found 1 version(s): ['1']\n",
      "\n",
      "  Processing version 1...\n",
      "Parsing LaTeX document from: 231272388\\2305-02160\\tex\\2305-02160v1\\acl2023.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 9 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 412 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 337\n",
      "      - section: 16\n",
      "      - figure: 15\n",
      "      - subsection: 22\n",
      "      - block_formula: 12\n",
      "      - table: 8\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 46 unique citation keys in document\n",
      "    Loaded 22 entries from anthology.bib\n",
      "    Loaded 20 entries from custom.bib\n",
      "    Loaded 42 bibliography entries (only cited references)\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "\n",
      "  Processing version 3...\n",
      "Parsing LaTeX document from: 231272388\\2305-02105\\tex\\2305-02105v3\\acl2023.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 215 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 167\n",
      "      - section: 11\n",
      "      - figure: 10\n",
      "      - subsection: 16\n",
      "      - block_formula: 1\n",
      "      - subsubsection: 2\n",
      "      - table: 6\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 42 unique citation keys in document\n",
      "    Loaded 8 entries from anthology.bib\n",
      "    Loaded 51 entries from custom.bib\n",
      "    Loaded 59 bibliography entries (only cited references)\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "  Exporting 59 unique cited references to .bib file\n",
      "[3/18] Finished 2305-02118\n",
      "  Found 3 version(s): ['1', '2', '3']\n",
      "\n",
      "  Processing version 1...\n",
      "Parsing LaTeX document from: 231272388\\2305-02170\\tex\\2305-02170v1\\acl2023.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 320 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 269\n",
      "      - section: 10\n",
      "      - subsection: 16\n",
      "      - figure: 13\n",
      "      - block_formula: 3\n",
      "      - table: 1\n",
      "      - acknowledgments: 1\n",
      "      - subsubsection: 5\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 65 unique citation keys in document\n",
      "    Loaded 29 entries from export-data.bib\n",
      "    Loaded 29 bibliography entries (only cited references)\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "  Exporting 29 unique cited references to .bib file\n",
      "[4/18] Finished 2305-02151\n",
      "  Found 2 version(s): ['1', '2']\n",
      "\n",
      "  Processing version 1...\n",
      "Parsing LaTeX document from: 231272388\\2305-02176\\tex\\2305-02176v1\\acl2023.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 7 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 214 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 176\n",
      "      - section: 7\n",
      "      - figure: 4\n",
      "      - block_formula: 11\n",
      "      - subsection: 11\n",
      "      - table: 3\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 31 unique citation keys in document\n",
      "    Loaded 3 entries from anthology.bib\n",
      "    Loaded 44 entries from custom.bib\n",
      "    Loaded 46 bibliography entries (only cited references)\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "  Exporting 46 unique cited references to .bib file\n",
      "[5/18] Finished 2305-02160\n",
      "  Found 4 version(s): ['1', '2', '3', '4']\n",
      "\n",
      "  Processing version 1...\n",
      "Parsing LaTeX document from: 231272388\\2305-02177\\tex\\2305-02177v1\\acl2023.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 209 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 175\n",
      "      - section: 7\n",
      "      - figure: 4\n",
      "      - block_formula: 12\n",
      "      - subsection: 5\n",
      "      - table: 4\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 50 unique citation keys in document\n",
      "    Loaded 50 entries from acl2023.bib\n",
      "    Loaded 22 entries from anthology.bib\n",
      "    Loaded 20 entries from custom.bib\n",
      "    Loaded 42 bibliography entries (only cited references)\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "  Exporting 42 unique cited references to .bib file\n",
      "[6/18] Finished 2305-02105\n",
      "  Found 2 version(s): ['1', '2']\n",
      "\n",
      "  Processing version 1...\n",
      "Parsing LaTeX document from: 231272388\\2305-02215\\tex\\2305-02215v1\\acl2023.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 3 files in compilation path\n",
      "    Unused files (2):\n",
      "      - review_ACL2023.tex\n",
      "      - risposte_ACL2023.tex\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 206 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 176\n",
      "      - section: 8\n",
      "      - table: 4\n",
      "      - subsection: 6\n",
      "      - block_formula: 6\n",
      "      - figure: 4\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 56 unique citation keys in document\n",
      "    Loaded 62 entries from custom.bib\n",
      "    Loaded 62 bibliography entries (only cited references)\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "\n",
      "  Processing version 2...\n",
      "Parsing LaTeX document from: 231272388\\2305-02170\\tex\\2305-02170v2\\acl2023.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 320 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 269\n",
      "      - section: 10\n",
      "      - subsection: 16\n",
      "      - figure: 13\n",
      "      - block_formula: 3\n",
      "      - table: 1\n",
      "      - acknowledgments: 1\n",
      "      - subsubsection: 5\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 65 unique citation keys in document\n",
      "    Loaded 6 entries from anthology.bib\n",
      "    Loaded 25 entries from custom.bib\n",
      "    Loaded 31 bibliography entries (only cited references)\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "\n",
      "  Processing version 2...\n",
      "Parsing LaTeX document from: 231272388\\2305-02176\\tex\\2305-02176v2\\emnlp2023.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 8 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 228 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 187\n",
      "      - section: 9\n",
      "      - figure: 4\n",
      "      - block_formula: 11\n",
      "      - subsection: 11\n",
      "      - acknowledgments: 1\n",
      "      - table: 3\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 35 unique citation keys in document\n",
      "    Loaded 34 entries from custom.bib\n",
      "    Loaded 34 bibliography entries (only cited references)\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "  Exporting 36 unique cited references to .bib file\n",
      "[7/18] Finished 2305-02176\n",
      "  Found 2 version(s): ['1', '2']\n",
      "\n",
      "  Processing version 1...\n",
      "Parsing LaTeX document from: 231272388\\2305-02220\\tex\\2305-02220v1\\acl2023.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 189 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 149\n",
      "      - section: 12\n",
      "      - figure: 6\n",
      "      - subsection: 11\n",
      "      - table: 6\n",
      "      - acknowledgments: 1\n",
      "      - block_formula: 2\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 33 unique citation keys in document\n",
      "    Loaded 50 bibliography entries (only cited references)\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "\n",
      "  Processing version 2...\n",
      "Parsing LaTeX document from: 231272388\\2305-02177\\tex\\2305-02177v2\\acl2023.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 217 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 182\n",
      "      - section: 7\n",
      "      - figure: 4\n",
      "      - block_formula: 12\n",
      "      - subsection: 5\n",
      "      - table: 4\n",
      "      - acknowledgments: 1\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 50 unique citation keys in document\n",
      "    Loaded 50 entries from acl2023.bib\n",
      "    Loaded 19 entries from anthology.bib\n",
      "    Loaded 36 entries from custom.bib\n",
      "    Loaded 62 entries from custom.bib\n",
      "    Loaded 62 bibliography entries (only cited references)\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "\n",
      "  Processing version 3...\n",
      "Parsing LaTeX document from: 231272388\\2305-02170\\tex\\2305-02170v3\\acl2023.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 320 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 269\n",
      "      - section: 10\n",
      "      - subsection: 16\n",
      "      - figure: 13\n",
      "      - block_formula: 3\n",
      "      - table: 1\n",
      "      - acknowledgments: 1\n",
      "      - subsubsection: 5\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 65 unique citation keys in document\n",
      "    Loaded 55 bibliography entries (only cited references)\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "\n",
      "  Processing version 2...\n",
      "Parsing LaTeX document from: 231272388\\2305-02215\\tex\\2305-02215v2\\cameraReady_emnlp2023.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 2 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 253 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 219\n",
      "      - section: 8\n",
      "      - table: 4\n",
      "      - subsection: 10\n",
      "      - block_formula: 6\n",
      "      - figure: 4\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 59 unique citation keys in document\n",
      "    Loaded 16 entries from anthology.bib\n",
      "    Loaded 17 entries from custom.bib\n",
      "    Loaded 33 bibliography entries (only cited references)\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "\n",
      "  Processing version 2...\n",
      "Parsing LaTeX document from: 231272388\\2305-02220\\tex\\2305-02220v2\\acl2023.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 184 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 145\n",
      "      - section: 12\n",
      "      - figure: 6\n",
      "      - subsection: 10\n",
      "      - table: 6\n",
      "      - acknowledgments: 1\n",
      "      - block_formula: 2\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 33 unique citation keys in document\n",
      "    Loaded 50 bibliography entries (only cited references)\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "\n",
      "  Processing version 3...\n",
      "Parsing LaTeX document from: 231272388\\2305-02177\\tex\\2305-02177v3\\acl2023.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 217 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 182\n",
      "      - section: 7\n",
      "      - figure: 4\n",
      "      - block_formula: 12\n",
      "      - subsection: 5\n",
      "      - table: 4\n",
      "      - acknowledgments: 1\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 50 unique citation keys in document\n",
      "    Loaded 50 entries from acl2023.bib\n",
      "    Loaded 22 entries from anthology.bib\n",
      "    Loaded 37 entries from custom.bib\n",
      "    Loaded 59 bibliography entries (only cited references)\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "  Exporting 59 unique cited references to .bib file\n",
      "[8/18] Finished 2305-02215\n",
      "  Found 1 version(s): ['1']\n",
      "\n",
      "  Processing version 1...\n",
      "Parsing LaTeX document from: 231272388\\2305-02235\\tex\\2305-02235v1\\acl2023.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 289 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 239\n",
      "      - section: 13\n",
      "      - figure: 8\n",
      "      - subsection: 16\n",
      "      - block_formula: 4\n",
      "      - table: 7\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 19 unique citation keys in document\n",
      "    Loaded 62 entries from custom.bib\n",
      "    Loaded 62 bibliography entries (only cited references)\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "  Exporting 62 unique cited references to .bib file\n",
      "[9/18] Finished 2305-02170\n",
      "  Found 3 version(s): ['1', '2', '3']\n",
      "\n",
      "  Processing version 1...\n",
      "Parsing LaTeX document from: 231272388\\2305-02317\\tex\\2305-02317v1\\acl_latex.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 209 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 162\n",
      "      - section: 10\n",
      "      - figure: 16\n",
      "      - block_formula: 6\n",
      "      - subsection: 10\n",
      "      - table: 2\n",
      "      - subsubsection: 1\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 38 unique citation keys in document\n",
      "    Loaded 50 bibliography entries (only cited references)\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "\n",
      "  Processing version 4...\n",
      "Parsing LaTeX document from: 231272388\\2305-02177\\tex\\2305-02177v4\\acl2023.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Loaded 16 entries from anthology.bib\n",
      "    Loaded 17 entries from custom.bib\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 229 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 194\n",
      "      - section: 7\n",
      "      - figure: 4\n",
      "      - block_formula: 12\n",
      "      - subsection: 5\n",
      "      - table: 4\n",
      "      - acknowledgments: 1\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 56 unique citation keys in document\n",
      "    Loaded 56 entries from acl2023.bib\n",
      "    Loaded 33 bibliography entries (only cited references)\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "  Exporting 33 unique cited references to .bib file\n",
      "[10/18] Finished 2305-02220\n",
      "  Found 2 version(s): ['1', '2']\n",
      "\n",
      "  Processing version 1...\n",
      "Parsing LaTeX document from: 231272388\\2305-02363\\tex\\2305-02363v1\\arxiv_v1.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 285 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 248\n",
      "      - section: 12\n",
      "      - figure: 5\n",
      "      - subsection: 11\n",
      "      - table: 6\n",
      "      - acknowledgments: 1\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 39 unique citation keys in document\n"
     ]
    }
   ],
   "source": [
    "FOLDER = \"231272388\"\n",
    "OUTPUT_FOLDER = \"231272388_output\"\n",
    "\n",
    "# Initialize batch processor\n",
    "batch_processor = BatchProcessor(FOLDER, OUTPUT_FOLDER)\n",
    "\n",
    "papers = batch_processor.discover_papers()\n",
    "print(f\"Found {len(papers)} papers in {FOLDER}\")\n",
    "\n",
    "stats = batch_processor.process_all(max_workers=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
