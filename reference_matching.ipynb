{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d388f0ac",
   "metadata": {},
   "source": [
    "# Reference Matching Pipeline\n",
    "\n",
    "## Pipeline Steps:\n",
    "1. Data Cleaning\n",
    "2. Data Labelling (Manual + Automatic)\n",
    "3. Feature Engineering\n",
    "4. Data Modeling\n",
    "5. Model Evaluation (MRR metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c30b7d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For string similarity\n",
    "from difflib import SequenceMatcher\n",
    "import Levenshtein\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1108405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bib_file(bib_path: Path) -> Dict[str, Dict[str, str]]:\n",
    "    entries = {}\n",
    "\n",
    "    try:\n",
    "        with open(bib_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        pos = 0\n",
    "        while pos < len(content):\n",
    "            # Find @entry{\n",
    "            entry_start = content.find('@', pos)\n",
    "            if entry_start == -1:\n",
    "                break\n",
    "\n",
    "            # Find entry type and key\n",
    "            brace_start = content.find('{', entry_start)\n",
    "            if brace_start == -1:\n",
    "                break\n",
    "\n",
    "            entry_type = content[entry_start+1:brace_start].strip()\n",
    "            if not entry_type:\n",
    "                pos = brace_start + 1\n",
    "                continue\n",
    "\n",
    "            # Find key (until comma or newline)\n",
    "            comma_pos = content.find(',', brace_start)\n",
    "            if comma_pos == -1:\n",
    "                break\n",
    "\n",
    "            key = content[brace_start+1:comma_pos].strip()\n",
    "\n",
    "            # Find matching closing brace\n",
    "            brace_count = 1\n",
    "            field_start = comma_pos + 1\n",
    "            pos = field_start\n",
    "\n",
    "            while pos < len(content) and brace_count > 0:\n",
    "                if content[pos] == '{':\n",
    "                    brace_count += 1\n",
    "                elif content[pos] == '}':\n",
    "                    brace_count -= 1\n",
    "                pos += 1\n",
    "            \n",
    "            if brace_count == 0:\n",
    "                fields_text = content[field_start:pos-1]\n",
    "                \n",
    "                # Parse fields\n",
    "                fields = {}\n",
    "                # Match field = {value} or field = \"value\"\n",
    "                field_pattern = re.compile(r'(\\w+)\\s*=\\s*(\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}|\"[^\"]*\"|\\d+)', re.DOTALL)\n",
    "                for field_match in field_pattern.finditer(fields_text):\n",
    "                    field_name = field_match.group(1).lower()\n",
    "                    field_value = field_match.group(2).strip()\n",
    "                    \n",
    "                    # Remove braces/quotes\n",
    "                    if field_value.startswith('{') and field_value.endswith('}'):\n",
    "                        field_value = field_value[1:-1]\n",
    "                    elif field_value.startswith('\"') and field_value.endswith('\"'):\n",
    "                        field_value = field_value[1:-1]\n",
    "                    \n",
    "                    fields[field_name] = field_value\n",
    "                \n",
    "                entries[key] = fields\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {bib_path}: {e}\")\n",
    "    \n",
    "    return entries\n",
    "\n",
    "def load_all_bibtex(output_dir: Path) -> Dict[str, Dict[str, Dict[str, str]]]:\n",
    "    all_bibtex = {}\n",
    "\n",
    "    for paper_dir in output_dir.iterdir():\n",
    "        if not paper_dir.is_dir():\n",
    "            continue\n",
    "\n",
    "        arxiv_id = paper_dir.name\n",
    "        bib_files = list(paper_dir.glob(\"*_bibtex.bib\"))\n",
    "\n",
    "        if bib_files:\n",
    "            bib_path = bib_files[0]\n",
    "            entries = parse_bib_file(bib_path)\n",
    "            if entries:\n",
    "                all_bibtex[arxiv_id] = entries\n",
    "    \n",
    "    return all_bibtex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1325eeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_references(output_dir: Path) -> Dict[str, Dict[str, Dict[str, Any]]]:\n",
    "    \"\"\"Load all references.json files\"\"\"\n",
    "    all_references = {}\n",
    "    \n",
    "    for paper_dir in output_dir.iterdir():\n",
    "        if not paper_dir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        arxiv_id = paper_dir.name\n",
    "        ref_path = paper_dir / \"references.json\"\n",
    "        \n",
    "        if ref_path.exists():\n",
    "            try:\n",
    "                with open(ref_path, 'r', encoding='utf-8-sig') as f:\n",
    "                    references = json.load(f)\n",
    "                all_references[arxiv_id] = references\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {ref_path}: {e}\")\n",
    "    \n",
    "    return all_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36e9182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize text for comparison\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove LaTeX commands\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{([^}]*)\\}', r'\\1', text)\n",
    "    \n",
    "    # Remove special characters but keep spaces\n",
    "    text = re.sub(r'[{}]', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_authors(author_str: str) -> List[str]:\n",
    "    \"\"\"Extract individual authors from author string\"\"\"\n",
    "    if not author_str:\n",
    "        return []\n",
    "    \n",
    "    # Split by common delimiters\n",
    "    authors = re.split(r'\\s+and\\s+|,|&', author_str.lower())\n",
    "    authors = [a.strip() for a in authors if a.strip()]\n",
    "    \n",
    "    return authors\n",
    "\n",
    "def get_first_author_last_name(author_str: str) -> str:\n",
    "    \"\"\"Extract last name of first author\"\"\"\n",
    "    authors = extract_authors(author_str)\n",
    "    if not authors:\n",
    "        return \"\"\n",
    "    \n",
    "    first_author = authors[0]\n",
    "    # Last name is typically the last word\n",
    "    parts = first_author.split()\n",
    "    if len(parts) > 0:\n",
    "        return parts[-1]\n",
    "    return first_author\n",
    "\n",
    "def normalize_year(year_str: str) -> Optional[int]:\n",
    "    \"\"\"Extract year as integer\"\"\"\n",
    "    if not year_str:\n",
    "        return None\n",
    "    \n",
    "    # Extract 4-digit year\n",
    "    match = re.search(r'\\b(19|20)\\d{2}\\b', str(year_str))\n",
    "    if match:\n",
    "        return int(match.group(0))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "872ea73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "_embedding_cache = {}\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    if text not in _embedding_cache:\n",
    "        _embedding_cache[text] = embedder.encode(\n",
    "            text, convert_to_numpy=True, normalize_embeddings=True\n",
    "        )\n",
    "    return _embedding_cache[text]\n",
    "\n",
    "\n",
    "def extract_features(bibtex_entry: Dict[str, str], reference_entry: Dict[str, Any]) -> Dict[str, float]:\n",
    "    \"\"\"Extract matching features between BibTeX and reference entry\"\"\"\n",
    "    features = {}\n",
    "\n",
    "    bib_title = normalize_text(bibtex_entry.get('title', ''))\n",
    "    ref_title = normalize_text(reference_entry.get('paper_title', ''))\n",
    "\n",
    "    features['title_exact_match'] = int(bib_title == ref_title)\n",
    "    features['title_ratio'] = SequenceMatcher(None, bib_title, ref_title).ratio()\n",
    "\n",
    "    try:\n",
    "        features['title_levenshtein'] = 1.0 - (Levenshtein.distance(bib_title, ref_title) / max(len(bib_title), len(ref_title), 1))\n",
    "    except:\n",
    "        features['title_levenshtein'] = features['title_ratio']\n",
    "\n",
    "    # Word overlap\n",
    "    bib_words = set(bib_title.split())\n",
    "    ref_words = set(ref_title.split())\n",
    "    if bib_words or ref_words:\n",
    "        features['title_jaccard'] = len(bib_words & ref_words) / len(bib_words | ref_words) if (bib_words | ref_words) else 0.0\n",
    "        features['title_word_overlap'] = len(bib_words & ref_words) / max(len(bib_words), 1)\n",
    "    else:\n",
    "        features['title_jaccard'] = 0.0\n",
    "        features['title_word_overlap'] = 0.0\n",
    "    \n",
    "    # Author features\n",
    "    bib_author = bibtex_entry.get('author', '')\n",
    "    ref_authors = reference_entry.get('authors', [])\n",
    "    \n",
    "    bib_authors_list = extract_authors(bib_author)\n",
    "    ref_authors_normalized = [normalize_text(a) for a in ref_authors]\n",
    "\n",
    "    # First author match\n",
    "    if bib_authors_list and ref_authors_normalized:\n",
    "        bib_first_last = get_first_author_last_name(bib_author)\n",
    "        ref_first_parts = ref_authors_normalized[0].split()\n",
    "        ref_first_last = ref_first_parts[-1] if ref_first_parts else \"\"\n",
    "        features['first_author_lastname_match'] = 1.0 if bib_first_last == ref_first_last else 0.0\n",
    "    else:\n",
    "        features['first_author_lastname_match'] = 0.0\n",
    "\n",
    "    # Author overlap\n",
    "    bib_author_set = set(bib_authors_list)\n",
    "    ref_author_set = set(ref_authors_normalized)\n",
    "    if bib_author_set or ref_author_set:\n",
    "        features['author_jaccard'] = len(bib_author_set & ref_author_set) / max(len(bib_author_set | ref_author_set), 1)\n",
    "    else:\n",
    "        features['author_jaccard'] = 0.0\n",
    "\n",
    "    # Year features\n",
    "    bib_year = normalize_year(bibtex_entry.get('year', ''))\n",
    "    ref_year = normalize_year(reference_entry.get('submission_date', ''))\n",
    "\n",
    "    if bib_year and ref_year:\n",
    "        year_gap = abs(bib_year - ref_year)\n",
    "        features['year_similarity'] = np.exp(-(year_gap ** 2) / 2)\n",
    "    else:\n",
    "        features['year_similarity'] = 0.0\n",
    "\n",
    "    # Combined features\n",
    "    features['title_author_score'] = (features['title_ratio'] + features['author_jaccard']) / 2.0\n",
    "    features['composite_score'] = (\n",
    "        features['title_ratio'] * 0.4 +\n",
    "        features['author_jaccard'] * 0.3 +\n",
    "        features['year_similarity'] * 0.2 +\n",
    "        features['first_author_lastname_match'] * 0.1\n",
    "    )\n",
    "\n",
    "    # Embedding similarity\n",
    "    if bib_title and ref_title:\n",
    "        try:\n",
    "            bib_emb = get_embedding(bib_title)\n",
    "            ref_emb = get_embedding(ref_title)\n",
    "            # Cosine similarity\n",
    "            features[\"title_embedding_sim\"] = float(np.dot(bib_emb, ref_emb))\n",
    "        except:\n",
    "            features['title_embedding_sim'] = 0.0\n",
    "    else:\n",
    "        features['title_embedding_sim'] = 0.0\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7362b69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BibTeX from 2107 papers\n",
      "Loaded references from 2893 papers\n",
      "\n",
      "Papers with both BibTeX and references: 2104\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "OUTPUT_DIR = Path(\"23127238_output\")\n",
    "\n",
    "all_bibtex = load_all_bibtex(OUTPUT_DIR)\n",
    "print(f\"Loaded BibTeX from {len(all_bibtex)} papers\")\n",
    "\n",
    "all_references = load_all_references(OUTPUT_DIR)\n",
    "print(f\"Loaded references from {len(all_references)} papers\")\n",
    "\n",
    "# Filter papers that have both BibTeX and references\n",
    "papers_with_both = set(all_bibtex.keys()) & set(all_references.keys())\n",
    "print(f\"\\nPapers with both BibTeX and references: {len(papers_with_both)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed430e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_bibtex_to_references(bibtex_entry: Dict[str, str],\n",
    "                                  references: Dict[str, Dict[str, Any]]) -> Optional[str]:\n",
    "    \"\"\"Match a single BibTeX entry to best reference\"\"\"\n",
    "    best_match = None\n",
    "    best_score = -1.0\n",
    "    \n",
    "    for arxiv_id, ref_entry in references.items():\n",
    "        features = extract_features(bibtex_entry, ref_entry)\n",
    "        \n",
    "        # Use composite score\n",
    "        score = features['composite_score']\n",
    "        \n",
    "        # Additional constraints\n",
    "        if features['title_ratio'] < 0.3:  # Too different titles\n",
    "            continue\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_match = arxiv_id\n",
    "    \n",
    "    # Only return if score is above threshold\n",
    "    if best_score >= 0.5:\n",
    "        return best_match\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2dfe908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2304-14610: 20 labels\n",
      "  2304-14656: 32 labels\n",
      "  2304-14693: 22 labels\n",
      "  2304-14796: 22 labels\n",
      "  2304-14999: 22 labels\n",
      "\n",
      "Total manual labels: 118\n"
     ]
    }
   ],
   "source": [
    "with open('manual_label.json', 'r', encoding='utf-8') as f:\n",
    "    manual_content = json.load(f)\n",
    "\n",
    "manual_labels = {}\n",
    "for paper_id, refs in manual_content.items():\n",
    "    paper_labels = {}\n",
    "    for bib_key, arxiv_id in refs.items():\n",
    "        paper_labels[bib_key] = arxiv_id\n",
    "\n",
    "    if paper_labels:\n",
    "        manual_labels[paper_id] = paper_labels\n",
    "        print(f\"  {paper_id}: {len(paper_labels)} labels\")\n",
    "\n",
    "print(f\"\\nTotal manual labels: {sum(len(labels) for labels in manual_labels.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40e5ab47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating automatic labels for 209 papers...\n",
      "  2305-00836: 4 labels\n",
      "  2305-00708: 10 labels\n",
      "  2305-02504: 24 labels\n",
      "  2305-00681: 10 labels\n",
      "  2305-00909: 40 labels\n",
      "  2305-01532: 2 labels\n",
      "  2305-01290: 4 labels\n",
      "  2305-01145: 9 labels\n",
      "  2305-01505: 23 labels\n",
      "  2305-00982: 15 labels\n",
      "  2305-02389: 10 labels\n",
      "  2305-00477: 31 labels\n",
      "  2305-00281: 9 labels\n",
      "  2305-00755: 12 labels\n",
      "  2305-01849: 4 labels\n",
      "  2305-00184: 3 labels\n",
      "  2305-01694: 51 labels\n",
      "  2305-00872: 10 labels\n",
      "  2305-01454: 8 labels\n",
      "  2305-01510: 3 labels\n",
      "  2305-01121: 24 labels\n",
      "  2304-14793: 5 labels\n",
      "  2305-00071: 24 labels\n",
      "  2305-00860: 2 labels\n",
      "  2305-01777: 26 labels\n",
      "  2305-01879: 22 labels\n",
      "  2305-00975: 5 labels\n",
      "  2305-00607: 38 labels\n",
      "  2305-01989: 2 labels\n",
      "  2305-01549: 33 labels\n",
      "  2305-01795: 54 labels\n",
      "  2305-01191: 19 labels\n",
      "  2304-14942: 16 labels\n",
      "  2304-14631: 1 labels\n",
      "  2305-02288: 1 labels\n",
      "  2305-01151: 12 labels\n",
      "  2305-01624: 37 labels\n",
      "  2305-00977: 12 labels\n",
      "  2305-02480: 1 labels\n",
      "  2305-01741: 11 labels\n",
      "  2305-01157: 22 labels\n",
      "  2305-02281: 40 labels\n",
      "  2305-01516: 2 labels\n",
      "  2305-00445: 28 labels\n",
      "  2305-00925: 22 labels\n",
      "  2305-01754: 36 labels\n",
      "  2305-01775: 34 labels\n",
      "  2305-01041: 16 labels\n",
      "  2305-01154: 39 labels\n",
      "  2305-01236: 27 labels\n",
      "  2304-14778: 4 labels\n",
      "  2304-15005: 8 labels\n",
      "  2304-14658: 6 labels\n",
      "  2305-01618: 70 labels\n",
      "  2305-01070: 7 labels\n",
      "  2305-01390: 2 labels\n",
      "  2305-01841: 19 labels\n",
      "  2305-00207: 2 labels\n",
      "  2305-01586: 47 labels\n",
      "  2305-00634: 13 labels\n",
      "  2305-01170: 17 labels\n",
      "  2305-00346: 12 labels\n",
      "  2305-00518: 3 labels\n",
      "  2305-00076: 9 labels\n",
      "  2305-01273: 4 labels\n",
      "  2305-01908: 61 labels\n",
      "  2305-00157: 34 labels\n",
      "  2305-01920: 27 labels\n",
      "  2305-00363: 15 labels\n",
      "  2305-01175: 7 labels\n",
      "  2305-02050: 130 labels\n",
      "  2305-00605: 2 labels\n",
      "  2305-01133: 3 labels\n",
      "  2305-01166: 25 labels\n",
      "  2305-01643: 38 labels\n",
      "  2305-02034: 38 labels\n",
      "  2305-01691: 12 labels\n",
      "  2305-01882: 36 labels\n",
      "  2305-01933: 30 labels\n",
      "  2304-14810: 21 labels\n",
      "  2305-01338: 13 labels\n",
      "  2305-00335: 13 labels\n",
      "  2305-01844: 11 labels\n",
      "  2305-02337: 24 labels\n",
      "  2305-00757: 4 labels\n",
      "  2305-00256: 39 labels\n",
      "  2305-01831: 12 labels\n",
      "  2305-01274: 8 labels\n",
      "  2304-14807: 2 labels\n",
      "  2305-00231: 5 labels\n",
      "  2305-01192: 17 labels\n",
      "  2305-00623: 36 labels\n",
      "  2305-01050: 8 labels\n",
      "  2305-00148: 1 labels\n",
      "  2305-01645: 52 labels\n",
      "  2304-14766: 35 labels\n",
      "  2305-00811: 7 labels\n",
      "  2305-00461: 11 labels\n",
      "  2305-00110: 13 labels\n",
      "  2304-14995: 6 labels\n",
      "  2305-01057: 19 labels\n",
      "  2305-01323: 22 labels\n",
      "  2305-01892: 13 labels\n",
      "  2305-01895: 76 labels\n",
      "  2305-01808: 21 labels\n",
      "  2304-14667: 61 labels\n",
      "  2305-01198: 3 labels\n",
      "  2305-01264: 5 labels\n",
      "  2305-02342: 40 labels\n",
      "  2305-01556: 15 labels\n",
      "  2305-02324: 39 labels\n",
      "  2305-01263: 44 labels\n",
      "  2305-02382: 18 labels\n",
      "  2305-00767: 57 labels\n",
      "  2305-01536: 5 labels\n",
      "  2305-00052: 22 labels\n",
      "  2305-01034: 33 labels\n",
      "  2305-01518: 10 labels\n",
      "  2305-01689: 1 labels\n",
      "  2305-00463: 14 labels\n",
      "  2305-02508: 5 labels\n",
      "  2305-01876: 34 labels\n",
      "  2304-14848: 8 labels\n",
      "  2305-00446: 3 labels\n",
      "  2305-01319: 48 labels\n",
      "  2304-14952: 2 labels\n",
      "  2305-00827: 38 labels\n",
      "  2305-01487: 28 labels\n",
      "  2305-00077: 9 labels\n",
      "  2305-01646: 23 labels\n",
      "  2304-14668: 39 labels\n",
      "  2304-14849: 13 labels\n",
      "  2304-14769: 38 labels\n",
      "  2305-00018: 43 labels\n",
      "  2305-01513: 5 labels\n",
      "  2304-14971: 12 labels\n",
      "  2305-01602: 96 labels\n",
      "  2305-00844: 2 labels\n",
      "  2305-02197: 8 labels\n",
      "  2305-00094: 25 labels\n",
      "  2305-01569: 14 labels\n",
      "  2304-14684: 15 labels\n",
      "  2305-00857: 21 labels\n",
      "\n",
      "Total automatic labels: 2959\n"
     ]
    }
   ],
   "source": [
    "# Automatic labeling for remaining papers (at least 10%)\n",
    "remaining_papers = papers_with_both - set(manual_labels.keys())\n",
    "num_auto_papers = max(1, int(len(remaining_papers) * 0.1))\n",
    "auto_papers = list(remaining_papers)[:num_auto_papers]\n",
    "\n",
    "print(f\"Creating automatic labels for {len(auto_papers)} papers...\")\n",
    "\n",
    "automatic_labels = {}\n",
    "for paper_id in auto_papers:\n",
    "    if paper_id not in all_bibtex or paper_id not in all_references:\n",
    "        continue\n",
    "    \n",
    "    bibtex_entries = all_bibtex[paper_id]\n",
    "    references = all_references[paper_id]\n",
    "    \n",
    "    paper_labels = {}\n",
    "    for bib_key, bib_entry in bibtex_entries.items():\n",
    "        match = match_bibtex_to_references(bib_entry, references)\n",
    "        if match:\n",
    "            paper_labels[bib_key] = match\n",
    "    \n",
    "    if paper_labels:\n",
    "        automatic_labels[paper_id] = paper_labels\n",
    "        print(f\"  {paper_id}: {len(paper_labels)} labels\")\n",
    "\n",
    "print(f\"\\nTotal automatic labels: {sum(len(labels) for labels in automatic_labels.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0390616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "automatic_labels = dict(\n",
    "    sorted(automatic_labels.items(), key=lambda x: x[0])\n",
    ")\n",
    "\n",
    "with open('automatic_label.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(\n",
    "        automatic_labels,\n",
    "        f,\n",
    "        ensure_ascii=False,\n",
    "        indent=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e52bee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(labels_dict: Dict[str, Dict[str, str]], \n",
    "                        all_bibtex: Dict, all_references: Dict,\n",
    "                        negative_ratio: float = 1.0) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create training data with positive and negative examples.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # Positive examples (matches)\n",
    "    for paper_id, labels in labels_dict.items():\n",
    "        if paper_id not in all_bibtex or paper_id not in all_references:\n",
    "            continue\n",
    "        \n",
    "        bibtex_entries = all_bibtex[paper_id]\n",
    "        references = all_references[paper_id]\n",
    "        \n",
    "        for bib_key, correct_arxiv_id in labels.items():\n",
    "            if bib_key not in bibtex_entries:\n",
    "                continue\n",
    "            if correct_arxiv_id not in references:\n",
    "                continue\n",
    "            \n",
    "            bib_entry = bibtex_entries[bib_key]\n",
    "            ref_entry = references[correct_arxiv_id]\n",
    "            \n",
    "            features = extract_features(bib_entry, ref_entry)\n",
    "            X.append(list(features.values()))\n",
    "            y.append(1)  # Positive match\n",
    "            \n",
    "            # Negative examples (random incorrect matches)\n",
    "            num_negatives = int(negative_ratio)\n",
    "            other_arxiv_ids = [aid for aid in references.keys() if aid != correct_arxiv_id]\n",
    "            \n",
    "            if other_arxiv_ids:\n",
    "                np.random.seed(42)  # For reproducibility\n",
    "                for _ in range(min(num_negatives, len(other_arxiv_ids))):\n",
    "                    wrong_arxiv_id = np.random.choice(other_arxiv_ids)\n",
    "                    wrong_ref = references[wrong_arxiv_id]\n",
    "                    \n",
    "                    neg_features = extract_features(bib_entry, wrong_ref)\n",
    "                    X.append(list(neg_features.values()))\n",
    "                    y.append(0)  # Negative match\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6581ec92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split:\n",
      "  Test: 2304-14610, 2304-14631\n",
      "  Validation: 2304-14656, 2304-14658\n",
      "  Train: 3 manual, 141 auto\n"
     ]
    }
   ],
   "source": [
    "# Split data according to requirements:\n",
    "# Test: 1 manual + 1 automatic\n",
    "# Validation: 1 manual + 1 automatic  \n",
    "# Train: rest\n",
    "\n",
    "manual_paper_list = list(manual_labels.keys())\n",
    "auto_paper_list = list(automatic_labels.keys())\n",
    "\n",
    "test_manual = manual_paper_list[0] if len(manual_paper_list) > 0 else None\n",
    "test_auto = auto_paper_list[0] if len(auto_paper_list) > 0 else None\n",
    "\n",
    "val_manual = manual_paper_list[1] if len(manual_paper_list) > 1 else None\n",
    "val_auto = auto_paper_list[1] if len(auto_paper_list) > 1 else None\n",
    "\n",
    "train_manual = [p for p in manual_paper_list if p not in [test_manual, val_manual]]\n",
    "train_auto = [p for p in auto_paper_list if p not in [test_auto, val_auto]]\n",
    "\n",
    "print(\"Data split:\")\n",
    "print(f\"  Test: {test_manual}, {test_auto}\")\n",
    "print(f\"  Validation: {val_manual}, {val_auto}\")\n",
    "print(f\"  Train: {len(train_manual)} manual, {len(train_auto)} auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f4d0d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full training set: 9035 examples\n",
      "\n",
      "Training random_forest...\n",
      "  Training accuracy: 0.9990\n",
      "\n",
      "Training gradient_boosting...\n",
      "  Training accuracy: 0.9989\n",
      "\n",
      "Training logistic_regression...\n",
      "  Training accuracy: 0.9942\n",
      "\n",
      "Selected model: RandomForestClassifier(n_jobs=-1, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data\n",
    "train_papers = {p: manual_labels[p] for p in train_manual if p in manual_labels}\n",
    "train_papers.update({p: automatic_labels[p] for p in train_auto if p in automatic_labels})\n",
    "\n",
    "X_train_full, y_train_full = create_training_data(\n",
    "    train_papers, all_bibtex, all_references, negative_ratio=2.0\n",
    ")\n",
    "\n",
    "print(f\"Full training set: {len(X_train_full)} examples\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_full)\n",
    "\n",
    "# Train multiple models\n",
    "models = {\n",
    "    'random_forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'gradient_boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'logistic_regression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "}\n",
    "\n",
    "trained_models = {}\n",
    "max_acc = 0\n",
    "name_max = ''\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train_scaled, y_train_full)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Quick validation on training data\n",
    "    y_pred = model.predict(X_train_scaled)\n",
    "    acc = accuracy_score(y_train_full, y_pred)\n",
    "    if acc > max_acc:\n",
    "        max_acc = acc\n",
    "        name_max = name\n",
    "\n",
    "    print(f\"  Training accuracy: {acc:.4f}\")\n",
    "\n",
    "\n",
    "best_model = trained_models[name_max]\n",
    "print(f\"\\nSelected model: {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df457aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set...\n",
      "  2304-14610: MRR = 1.0000\n",
      "  2304-14631: MRR = 1.0000\n",
      "\n",
      "Overall Test MRR: 1.0000\n"
     ]
    }
   ],
   "source": [
    "def calculate_mrr(model, scaler,\n",
    "                 bibtex_entries: Dict, references: Dict,\n",
    "                 ground_truth: Dict[str, str], top_k: int = 5) -> float:\n",
    "    \"\"\"\n",
    "    Calculate MRR for a single paper.\n",
    "    \n",
    "    Returns:\n",
    "        MRR score (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    if not ground_truth:\n",
    "        return 0.0\n",
    "    \n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    for bib_key, correct_arxiv_id in ground_truth.items():\n",
    "        if bib_key not in bibtex_entries:\n",
    "            continue\n",
    "        \n",
    "        bib_entry = bibtex_entries[bib_key]\n",
    "        \n",
    "        # Get scores for all candidates\n",
    "        candidate_scores = []\n",
    "        for arxiv_id, ref_entry in references.items():\n",
    "            features = extract_features(bib_entry, ref_entry)\n",
    "            X = np.array([list(features.values())])\n",
    "            X_scaled = scaler.transform(X)\n",
    "            \n",
    "            # Get probability of positive class\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                score = model.predict_proba(X_scaled)[0][1]\n",
    "            else:\n",
    "                score = model.decision_function(X_scaled)[0]\n",
    "            \n",
    "            candidate_scores.append((arxiv_id, score))\n",
    "        \n",
    "        # Sort by score descending\n",
    "        candidate_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Find rank of correct answer\n",
    "        ranked_arxiv_ids = [aid for aid, _ in candidate_scores[:top_k]]\n",
    "        \n",
    "        if correct_arxiv_id in ranked_arxiv_ids:\n",
    "            rank = ranked_arxiv_ids.index(correct_arxiv_id) + 1\n",
    "            reciprocal_ranks.append(1.0 / rank)\n",
    "        else:\n",
    "            reciprocal_ranks.append(0.0)\n",
    "    \n",
    "    if not reciprocal_ranks:\n",
    "        return 0.0\n",
    "    \n",
    "    return np.mean(reciprocal_ranks)\n",
    "\n",
    "\n",
    "# Evaluate on test set\n",
    "test_papers = {}\n",
    "if test_manual:\n",
    "    test_papers[test_manual] = manual_labels[test_manual]\n",
    "if test_auto:\n",
    "    test_papers[test_auto] = automatic_labels[test_auto]\n",
    "\n",
    "print(\"Evaluating on test set...\")\n",
    "test_mrr_scores = []\n",
    "\n",
    "for paper_id, ground_truth in test_papers.items():\n",
    "    if paper_id not in all_bibtex or paper_id not in all_references:\n",
    "        continue\n",
    "    \n",
    "    mrr = calculate_mrr(\n",
    "        best_model, scaler,\n",
    "        all_bibtex[paper_id], all_references[paper_id],\n",
    "        ground_truth, top_k=5\n",
    "    )\n",
    "    \n",
    "    test_mrr_scores.append(mrr)\n",
    "    print(f\"  {paper_id}: MRR = {mrr:.4f}\")\n",
    "\n",
    "overall_mrr = np.mean(test_mrr_scores) if test_mrr_scores else 0.0\n",
    "print(f\"\\nOverall Test MRR: {overall_mrr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30f79ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n",
      "  Saved 2305-00836 (train): 21 predictions\n",
      "  Saved 2305-00256 (train): 50 predictions\n",
      "  Saved 2305-01831 (train): 53 predictions\n",
      "  Saved 2305-01274 (train): 97 predictions\n",
      "  Saved 2304-14807 (train): 56 predictions\n",
      "  Saved 2305-00708 (train): 15 predictions\n",
      "  Saved 2305-02504 (train): 38 predictions\n",
      "  Saved 2305-00681 (train): 15 predictions\n",
      "  Saved 2305-00231 (train): 31 predictions\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     49\u001b[39m     ground_truth = manual_labels.get(paper_id, {}) \u001b[38;5;129;01mor\u001b[39;00m automatic_labels.get(paper_id, {})\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Generate predictions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m predictions = \u001b[43mgenerate_predictions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_bibtex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpaper_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_references\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpaper_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\n\u001b[32m     56\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Save to pred.json\u001b[39;00m\n\u001b[32m     59\u001b[39m output_path = OUTPUT_DIR / paper_id / \u001b[33m\"\u001b[39m\u001b[33mpred.json\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mgenerate_predictions\u001b[39m\u001b[34m(model, scaler, bibtex_entries, references, top_k)\u001b[39m\n\u001b[32m     13\u001b[39m X_scaled = scaler.transform(X)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m'\u001b[39m\u001b[33mpredict_proba\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     score = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_scaled\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m][\u001b[32m1\u001b[39m]\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     18\u001b[39m     score = model.decision_function(X_scaled)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda3\\envs\\min_ds-env\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:956\u001b[39m, in \u001b[36mForestClassifier.predict_proba\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    951\u001b[39m all_proba = [\n\u001b[32m    952\u001b[39m     np.zeros((X.shape[\u001b[32m0\u001b[39m], j), dtype=np.float64)\n\u001b[32m    953\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m np.atleast_1d(\u001b[38;5;28mself\u001b[39m.n_classes_)\n\u001b[32m    954\u001b[39m ]\n\u001b[32m    955\u001b[39m lock = threading.Lock()\n\u001b[32m--> \u001b[39m\u001b[32m956\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequire\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msharedmem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_accumulate_prediction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_proba\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mestimators_\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m proba \u001b[38;5;129;01min\u001b[39;00m all_proba:\n\u001b[32m    962\u001b[39m     proba /= \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.estimators_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda3\\envs\\min_ds-env\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda3\\envs\\min_ds-env\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda3\\envs\\min_ds-env\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda3\\envs\\min_ds-env\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def generate_predictions(model, scaler,\n",
    "                        bibtex_entries: Dict, references: Dict,\n",
    "                        top_k: int = 5) -> Dict[str, List[str]]:\n",
    "    \"\"\"Generate ranked predictions for all BibTeX entries\"\"\"\n",
    "    predictions = {}\n",
    "    \n",
    "    for bib_key, bib_entry in bibtex_entries.items():\n",
    "        candidate_scores = []\n",
    "        \n",
    "        for arxiv_id, ref_entry in references.items():\n",
    "            features = extract_features(bib_entry, ref_entry)\n",
    "            X = np.array([list(features.values())])\n",
    "            X_scaled = scaler.transform(X)\n",
    "            \n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                score = model.predict_proba(X_scaled)[0][1]\n",
    "            else:\n",
    "                score = model.decision_function(X_scaled)[0]\n",
    "            \n",
    "            candidate_scores.append((arxiv_id, score))\n",
    "        \n",
    "        # Sort and take top k\n",
    "        candidate_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        ranked_candidates = [aid for aid, _ in candidate_scores[:top_k]]\n",
    "        \n",
    "        predictions[bib_key] = ranked_candidates\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Generate predictions for all papers in train/val/test sets\n",
    "all_labeled_papers = set(manual_labels.keys()) | set(automatic_labels.keys())\n",
    "\n",
    "print(\"Generating predictions...\")\n",
    "\n",
    "for paper_id in all_labeled_papers:\n",
    "    if paper_id not in all_bibtex or paper_id not in all_references:\n",
    "        continue\n",
    "    \n",
    "    # Determine partition\n",
    "    if paper_id == test_manual or paper_id == test_auto:\n",
    "        partition = \"test\"\n",
    "        ground_truth = manual_labels.get(paper_id, {}) or automatic_labels.get(paper_id, {})\n",
    "    elif paper_id == val_manual or paper_id == val_auto:\n",
    "        partition = \"valid\"\n",
    "        ground_truth = manual_labels.get(paper_id, {}) or automatic_labels.get(paper_id, {})\n",
    "    else:\n",
    "        partition = \"train\"\n",
    "        ground_truth = manual_labels.get(paper_id, {}) or automatic_labels.get(paper_id, {})\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = generate_predictions(\n",
    "        best_model, scaler,\n",
    "        all_bibtex[paper_id], all_references[paper_id],\n",
    "        top_k=5\n",
    "    )\n",
    "    \n",
    "    # Save to pred.json\n",
    "    output_path = OUTPUT_DIR / paper_id / \"pred.json\"\n",
    "    output_data = {\n",
    "        \"partition\": partition,\n",
    "        \"groundtruth\": ground_truth,\n",
    "        \"prediction\": predictions\n",
    "    }\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"  Saved {paper_id} ({partition}): {len(predictions)} predictions\")\n",
    "\n",
    "print(\"\\nAll predictions saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
