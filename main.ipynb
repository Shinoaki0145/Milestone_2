{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1dab0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Dict, Any, Set, Tuple\n",
    "from enum import Enum\n",
    "from collections import defaultdict\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87fb41ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeType(Enum):\n",
    "    \"\"\"Types of nodes in the document hierarchy\"\"\"\n",
    "    DOCUMENT = \"document\"\n",
    "    CHAPTER = \"chapter\"\n",
    "    SECTION = \"section\"\n",
    "    SUBSECTION = \"subsection\"\n",
    "    SUBSUBSECTION = \"subsubsection\"\n",
    "    PARAGRAPH = \"paragraph\"\n",
    "    SUBPARAGRAPH = \"subparagraph\"\n",
    "    # Leaf nodes\n",
    "    SENTENCE = \"sentence\"\n",
    "    BLOCK_FORMULA = \"block_formula\"\n",
    "    FIGURE = \"figure\"\n",
    "    TABLE = \"table\"\n",
    "    # Special\n",
    "    ABSTRACT = \"abstract\"\n",
    "    ACKNOWLEDGMENTS = \"acknowledgments\"\n",
    "    APPENDIX = \"appendix\"\n",
    "\n",
    "\n",
    "# Hierarchy order for determining nesting levels\n",
    "HIERARCHY_ORDER = [\n",
    "    NodeType.DOCUMENT,\n",
    "    NodeType.CHAPTER,\n",
    "    NodeType.SECTION,\n",
    "    NodeType.SUBSECTION,\n",
    "    NodeType.SUBSUBSECTION,\n",
    "    NodeType.PARAGRAPH,\n",
    "    NodeType.SUBPARAGRAPH,\n",
    "]\n",
    "\n",
    "# Leaf node types (smallest elements)\n",
    "LEAF_TYPES = {NodeType.SENTENCE, NodeType.BLOCK_FORMULA, NodeType.FIGURE, NodeType.TABLE}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HierarchyNode:\n",
    "    \"\"\"A node in the document hierarchy tree\"\"\"\n",
    "    node_type: NodeType\n",
    "    title: str = \"\"\n",
    "    content: str = \"\"\n",
    "    children: List['HierarchyNode'] = field(default_factory=list)\n",
    "    label: str = \"\"\n",
    "    source_file: str = \"\"\n",
    "    content_hash: str = \"\"\n",
    "    unique_id: str = \"\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # Generate content hash first (used for deduplication)\n",
    "        if not self.content_hash and self.content:\n",
    "            # Normalize content for consistent hashing\n",
    "            normalized_content = re.sub(r'\\s+', ' ', self.content.strip().lower())\n",
    "            self.content_hash = hashlib.md5(normalized_content.encode()).hexdigest()\n",
    "        \n",
    "        # Generate unique_id\n",
    "        if not self.unique_id:\n",
    "            # For leaf nodes (sentences, formulas, figures, tables), use content-based ID\n",
    "            # This ensures identical content across versions gets the same ID\n",
    "            if self.node_type in LEAF_TYPES and self.content_hash:\n",
    "                self.unique_id = self.content_hash[:12]\n",
    "            else:\n",
    "                # For structural nodes, use type + title + content prefix\n",
    "                self.unique_id = hashlib.md5(\n",
    "                    f\"{self.node_type.value}:{self.title}:{self.content[:100]}\".encode()\n",
    "                ).hexdigest()[:12]\n",
    "    \n",
    "\n",
    "\n",
    "@dataclass \n",
    "class BibEntry:\n",
    "    \"\"\"A bibliography entry\"\"\"\n",
    "    key: str\n",
    "    entry_type: str\n",
    "    fields: Dict[str, str] = field(default_factory=dict)\n",
    "    \n",
    "    def _normalize_field(self, value: str) -> str:\n",
    "        \"\"\"Normalize a field value for comparison\"\"\"\n",
    "        if not value:\n",
    "            return \"\"\n",
    "        # Lowercase, strip whitespace, remove extra spaces\n",
    "        normalized = value.lower().strip()\n",
    "        # Remove LaTeX commands and braces\n",
    "        normalized = re.sub(r'\\\\[a-zA-Z]+\\{([^}]*)\\}', r'\\1', normalized)\n",
    "        # Remove punctuation for comparison\n",
    "        normalized = re.sub(r'[{}\"\\'.,;:\\-]+', ' ', normalized)\n",
    "        # Collapse whitespace\n",
    "        normalized = re.sub(r'\\s+', ' ', normalized).strip()\n",
    "        return normalized\n",
    "    \n",
    "    def get_normalized_title(self) -> str:\n",
    "        \"\"\"Get normalized title for fuzzy comparison\"\"\"\n",
    "        return self._normalize_field(self.fields.get('title', ''))\n",
    "    \n",
    "    def get_normalized_author(self) -> str:\n",
    "        \"\"\"Get normalized author for fuzzy comparison\"\"\"\n",
    "        return self._normalize_field(self.fields.get('author', ''))\n",
    "    \n",
    "    def has_sufficient_fields(self) -> bool:\n",
    "        \"\"\"Check if entry has enough fields for content-based deduplication\"\"\"\n",
    "        title = self.fields.get('title', '').strip()\n",
    "        author = self.fields.get('author', '').strip()\n",
    "        year = self.fields.get('year', '').strip()\n",
    "        doi = self.fields.get('doi', '').strip()\n",
    "        \n",
    "        # Need at least title, or (author + year), or doi\n",
    "        has_title = len(title) > 5\n",
    "        has_author_year = len(author) > 3 and len(year) >= 4\n",
    "        has_doi = len(doi) > 5\n",
    "        \n",
    "        return has_title or has_author_year or has_doi\n",
    "    \n",
    "    def content_hash(self) -> str:\n",
    "        \"\"\"\n",
    "        Generate hash based on content for deduplication.\n",
    "        Returns unique key-based hash if fields are insufficient.\n",
    "        \"\"\"\n",
    "        # If fields are empty or insufficient, return key-based hash to prevent false merges\n",
    "        if not self.fields or not self.has_sufficient_fields():\n",
    "            # Use key as unique identifier - no two entries with same key should merge incorrectly\n",
    "            unique_str = f\"__KEY_ONLY__:{self.key}:{self.entry_type}\"\n",
    "            return hashlib.md5(unique_str.encode()).hexdigest()\n",
    "        \n",
    "        # Build hash from normalized key identifying fields\n",
    "        hash_parts = []\n",
    "        \n",
    "        # Title (primary identifier)\n",
    "        title = self.get_normalized_title()\n",
    "        if title:\n",
    "            hash_parts.append(f\"title:{title}\")\n",
    "        \n",
    "        # Author (secondary identifier)  \n",
    "        author = self.get_normalized_author()\n",
    "        if author:\n",
    "            # Extract first author's last name for more robust matching\n",
    "            first_author = author.split(' and ')[0].strip()\n",
    "            hash_parts.append(f\"author:{first_author}\")\n",
    "        \n",
    "        # Year\n",
    "        year = self.fields.get('year', '').strip()\n",
    "        if year:\n",
    "            hash_parts.append(f\"year:{year}\")\n",
    "        \n",
    "        # DOI (very reliable identifier if present)\n",
    "        doi = self._normalize_field(self.fields.get('doi', ''))\n",
    "        if doi:\n",
    "            hash_parts.append(f\"doi:{doi}\")\n",
    "        \n",
    "        if not hash_parts:\n",
    "            # Fallback to key-based hash\n",
    "            unique_str = f\"__KEY_ONLY__:{self.key}:{self.entry_type}\"\n",
    "            return hashlib.md5(unique_str.encode()).hexdigest()\n",
    "        \n",
    "        content = \"|\".join(sorted(hash_parts))\n",
    "        return hashlib.md5(content.encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbdf1a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaTeXFileGatherer:\n",
    "    # Patterns to match \\input and \\include commands\n",
    "    PATTERN = re.compile(r'(?m)^(?![^%\\n]*%).*\\\\(?:input|include)\\{([^}]+)\\}')\n",
    "    \n",
    "    def __init__(self, base_dir: str):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.included_files: Set[str] = set()\n",
    "        self.file_contents: Dict[str, str] = {}\n",
    "        self.file_order: List[str] = []\n",
    "        \n",
    "    def gather_files(self, main_file: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Recursively gather all files starting from the main file.\n",
    "        Returns a dict mapping file paths to their contents.\n",
    "        \"\"\"\n",
    "        self.included_files.clear()\n",
    "        self.file_contents.clear()\n",
    "        self.file_order.clear()\n",
    "        \n",
    "        main_path = self.base_dir / main_file\n",
    "        self._process_file(main_path)\n",
    "        \n",
    "        return self.file_contents\n",
    "    \n",
    "    def _resolve_path(self, include_path: str, current_file: Path) -> Path:\n",
    "        \"\"\"Resolve the path of an included file\"\"\"\n",
    "        # Add .tex extension if not present\n",
    "        if not include_path.endswith('.tex'):\n",
    "            include_path += '.tex'\n",
    "        \n",
    "        # Try relative to current file first\n",
    "        relative_path = current_file.parent / include_path\n",
    "        if relative_path.exists():\n",
    "            return relative_path\n",
    "        \n",
    "        # Try relative to base directory\n",
    "        base_relative = self.base_dir / include_path\n",
    "        if base_relative.exists():\n",
    "            return base_relative\n",
    "        \n",
    "        return relative_path  # Return even if doesn't exist for error reporting\n",
    "    \n",
    "    def _process_file(self, file_path: Path) -> str:\n",
    "        \"\"\"Process a single file and recursively process includes\"\"\"\n",
    "        # Normalize path for tracking\n",
    "        normalized_path = str(file_path.resolve())\n",
    "        \n",
    "        if normalized_path in self.included_files:\n",
    "            return \"\"  # Already processed\n",
    "        \n",
    "        if not file_path.exists():\n",
    "            print(f\"Warning: File not found: {file_path}\")\n",
    "            return \"\"\n",
    "        \n",
    "        self.included_files.add(normalized_path)\n",
    "        self.file_order.append(normalized_path)\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                content = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            return \"\"\n",
    "        \n",
    "        # Store original content\n",
    "        self.file_contents[normalized_path] = content\n",
    "        \n",
    "        # Find and process all includes\n",
    "        expanded_content = self._expand_includes(content, file_path)\n",
    "        \n",
    "        return expanded_content\n",
    "    \n",
    "    def _expand_includes(self, content: str, current_file: Path) -> str:\n",
    "        \"\"\"Expand \\input and \\include commands in content\"\"\"\n",
    "        for match in self.PATTERN.finditer(content):\n",
    "            include_path = match.group(1)\n",
    "            resolved_path = self._resolve_path(include_path, current_file)\n",
    "            self._process_file(resolved_path)\n",
    "        \n",
    "        return content\n",
    "    \n",
    "    def get_merged_content(self) -> str:\n",
    "        \"\"\"Get all content merged into a single string with file markers\"\"\"\n",
    "        merged = []\n",
    "        for file_path in self.file_order:\n",
    "            content = self.file_contents.get(file_path, \"\")\n",
    "            # Add file marker for tracking\n",
    "            merged.append(f\"%%% FILE: {file_path} %%%\\n\")\n",
    "            merged.append(content)\n",
    "            merged.append(\"\\n\")\n",
    "        return \"\\n\".join(merged)\n",
    "    \n",
    "    def get_unused_files(self) -> Set[str]:\n",
    "        \"\"\"Get files that exist but are not part of compilation\"\"\"\n",
    "        all_files = set()\n",
    "        for tex_file in self.base_dir.rglob(\"*.tex\"):\n",
    "            all_files.add(str(tex_file.resolve()))\n",
    "        return all_files - self.included_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39fb3ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaTeXCleaner:\n",
    "    \"\"\"Cleans and normalizes LaTeX content\"\"\"\n",
    "    \n",
    "    # Commands to remove (no semantic meaning)\n",
    "    REMOVE_COMMANDS = [\n",
    "        r'\\\\centering',\n",
    "        r'\\\\raggedright',\n",
    "        r'\\\\raggedleft',\n",
    "        r'\\\\noindent',\n",
    "        r'\\\\smallskip',\n",
    "        r'\\\\medskip',\n",
    "        r'\\\\bigskip',\n",
    "        r'\\\\newpage',\n",
    "        r'\\\\clearpage',\n",
    "        r'\\\\pagebreak',\n",
    "        r'\\\\linebreak',\n",
    "        r'\\\\hfill',\n",
    "        r'\\\\vfill',\n",
    "        r'\\\\hspace\\{[^}]*\\}',\n",
    "        r'\\\\vspace\\{[^}]*\\}',\n",
    "        r'\\\\phantom\\{[^}]*\\}',\n",
    "        r'\\\\hphantom\\{[^}]*\\}',\n",
    "        r'\\\\vphantom\\{[^}]*\\}',\n",
    "\n",
    "        r'\\\\par',\n",
    "        r'\\\\parindent\\s*=?\\s*[^\\\\\\n]*',\n",
    "        r'\\\\parskip\\s*=?\\s*[^\\\\\\n]*',\n",
    "        r'\\\\baselineskip\\s*=?\\s*[^\\\\\\n]*',\n",
    "        r'\\\\stretch\\{[^}]*\\}',\n",
    "\n",
    "        # Font / style\n",
    "        r'\\\\textbf\\{([^}]*)\\}',\n",
    "        r'\\\\textit\\{([^}]*)\\}',\n",
    "        r'\\\\emph\\{([^}]*)\\}',\n",
    "        r'\\\\underline\\{([^}]*)\\}',\n",
    "        r'\\\\texttt\\{([^}]*)\\}',\n",
    "        r'\\\\bfseries',\n",
    "        r'\\\\itshape',\n",
    "        r'\\\\ttfamily',\n",
    "        r'\\\\footnotesize',\n",
    "        r'\\\\scriptsize',\n",
    "        r'\\\\tiny',\n",
    "        r'\\\\large',\n",
    "        r'\\\\Large',\n",
    "        r'\\\\LARGE',\n",
    "        r'\\\\huge',\n",
    "        r'\\\\Huge',\n",
    "    ]\n",
    "    \n",
    "    # Table formatting commands to remove\n",
    "    TABLE_FORMATTING = [\n",
    "        r'\\\\toprule',\n",
    "        r'\\\\midrule',\n",
    "        r'\\\\bottomrule',\n",
    "        r'\\\\hline',\n",
    "        r'\\\\cline\\{[^}]*\\}',\n",
    "        \n",
    "        r'\\\\addlinespace',\n",
    "        r'\\\\cmidrule\\{[^}]*\\}', \n",
    "    ]\n",
    "    \n",
    "    # Figure/table placement specifiers\n",
    "    PLACEMENT_SPECS = re.compile(r'\\[([htbp!]+)\\]')\n",
    "    \n",
    "    # Inline math patterns (to normalize to $...$)\n",
    "    INLINE_MATH_PATTERNS = [\n",
    "        (re.compile(r'\\\\[(](.*?)\\\\[)]', re.DOTALL), r'$\\1$'),  # \\(...\\)\n",
    "        (re.compile(r'\\\\begin\\{math\\}(.*?)\\\\end\\{math\\}', re.DOTALL), r'$\\1$'),\n",
    "    ]\n",
    "    \n",
    "    # Block math environments (to normalize to equation)\n",
    "    BLOCK_MATH_ENVS = [\n",
    "        'displaymath', 'eqnarray', 'eqnarray*', 'align', 'align*',\n",
    "        'gather', 'gather*', 'multline', 'multline*', 'flalign', 'flalign*'\n",
    "    ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Compile removal patterns\n",
    "        self.remove_patterns = [re.compile(p) for p in self.REMOVE_COMMANDS + self.TABLE_FORMATTING]\n",
    "    \n",
    "    def clean(self, content: str) -> str:\n",
    "        \"\"\"Apply all cleaning operations\"\"\"\n",
    "        result = content\n",
    "        \n",
    "        # Remove comments (but preserve % in math mode)\n",
    "        result = self._remove_comments(result)\n",
    "        result = self.normalize_math(result)\n",
    "        \n",
    "        # Remove formatting commands\n",
    "        for pattern in self.remove_patterns:\n",
    "            result = pattern.sub('', result)\n",
    "        \n",
    "        # Remove placement specifiers\n",
    "        result = self.PLACEMENT_SPECS.sub('', result)\n",
    "        \n",
    "        # Normalize whitespace\n",
    "        result = self._normalize_whitespace(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def normalize_math(self, content: str) -> str:\n",
    "        \"\"\"Normalize all math expressions\"\"\"\n",
    "        result = content\n",
    "        \n",
    "        # Normalize inline math to $...$\n",
    "        for pattern, replacement in self.INLINE_MATH_PATTERNS:\n",
    "            result = pattern.sub(replacement, result)\n",
    "        \n",
    "        # Normalize $$...$$ to equation environment\n",
    "        result = re.sub(\n",
    "            r'\\$\\$(.*?)\\$\\$',\n",
    "            r'\\\\begin{equation}\\1\\\\end{equation}',\n",
    "            result,\n",
    "            flags=re.DOTALL\n",
    "        )\n",
    "\n",
    "        # Normalize \\[...\\] to equation environment\n",
    "        result = re.sub(\n",
    "            r'\\\\\\[(.*?)\\\\\\]',\n",
    "            r'\\\\begin{equation}\\1\\\\end{equation}',\n",
    "            result,\n",
    "            flags=re.DOTALL\n",
    "        )\n",
    "        \n",
    "        # Normalize other block math environments to equation\n",
    "        for env in self.BLOCK_MATH_ENVS:\n",
    "            pattern = re.compile(\n",
    "                rf'\\\\begin\\{{{env}\\}}(.*?)\\\\end\\{{{env}\\}}',\n",
    "                re.DOTALL\n",
    "            )\n",
    "            result = pattern.sub(r'\\\\begin{equation}\\1\\\\end{equation}', result)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def _remove_comments(self, content: str) -> str:\n",
    "        \"\"\"Remove LaTeX comments while preserving escaped %\"\"\"\n",
    "        lines = content.split('\\n')\n",
    "        result_lines = []\n",
    "        for line in lines:\n",
    "            # Find % that are not escaped\n",
    "            new_line = []\n",
    "            i = 0\n",
    "            while i < len(line):\n",
    "                if line[i] == '%' and (i == 0 or line[i-1] != '\\\\'):\n",
    "                    break  # Rest of line is comment\n",
    "                new_line.append(line[i])\n",
    "                i += 1\n",
    "            result_lines.append(''.join(new_line))\n",
    "        return '\\n'.join(result_lines)\n",
    "    \n",
    "    def _normalize_whitespace(self, content: str) -> str:\n",
    "        \"\"\"Normalize whitespace in content\"\"\"\n",
    "        # Replace multiple spaces with single space\n",
    "        result = re.sub(r'[ \\t]+', ' ', content)\n",
    "        # Replace multiple newlines with double newline\n",
    "        result = re.sub(r'\\n{3,}', '\\n\\n', result)\n",
    "        return result.strip()\n",
    "    \n",
    "    def extract_text_content(self, content: str) -> str:\n",
    "        \"\"\"Extract plain text from LaTeX, removing commands\"\"\"\n",
    "        result = content\n",
    "        \n",
    "        # Remove \\command{...} but keep the content in braces\n",
    "        result = re.sub(r'\\\\(?:textbf|textit|textrm|texttt|emph|underline)\\{([^}]*)\\}', r'\\1', result)\n",
    "        \n",
    "        # Remove \\command without braces\n",
    "        result = re.sub(r'\\\\(?:bf|it|rm|tt|em|sc)\\b', '', result)\n",
    "        \n",
    "        # Remove citations and references (keep for now, mark them)\n",
    "        result = re.sub(r'\\\\cite[pt]?\\{[^}]*\\}', '[CITATION]', result)\n",
    "        result = re.sub(r'\\\\ref\\{[^}]*\\}', '[REF]', result)\n",
    "        \n",
    "        # Remove labels\n",
    "        result = re.sub(r'\\\\label\\{[^}]*\\}', '', result)\n",
    "        \n",
    "        # Remove remaining simple commands\n",
    "        result = re.sub(r'\\\\[a-zA-Z]+\\*?(?:\\[[^\\]]*\\])?(?:\\{[^}]*\\})?', '', result)\n",
    "        \n",
    "        # Clean up braces\n",
    "        result = re.sub(r'[{}]', '', result)\n",
    "        \n",
    "        return result.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a1052d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchyParser:\n",
    "    \"\"\"Parses LaTeX content into a hierarchical structure\"\"\"\n",
    "    \n",
    "    # Section command patterns with their hierarchy levels\n",
    "    SECTION_PATTERNS = [\n",
    "        (r'\\\\chapter\\*?\\{([^}]*)\\}', NodeType.CHAPTER),\n",
    "        (r'\\\\section\\*?\\{([^}]*)\\}', NodeType.SECTION),\n",
    "        (r'\\\\subsection\\*?\\{([^}]*)\\}', NodeType.SUBSECTION),\n",
    "        (r'\\\\subsubsection\\*?\\{([^}]*)\\}', NodeType.SUBSUBSECTION),\n",
    "        (r'\\\\paragraph\\*?\\{([^}]*)\\}', NodeType.PARAGRAPH),\n",
    "        (r'\\\\subparagraph\\*?\\{([^}]*)\\}', NodeType.SUBPARAGRAPH),\n",
    "    ]\n",
    "    \n",
    "    # Special sections to detect\n",
    "    SPECIAL_PATTERNS = [\n",
    "        (r'\\\\begin\\{abstract\\}(.*?)\\\\end\\{abstract\\}', NodeType.ABSTRACT),\n",
    "        (r'\\\\chapter\\*?\\{Acknowledgments?\\}', NodeType.ACKNOWLEDGMENTS),\n",
    "        (r'\\\\section\\*?\\{Acknowledgments?\\}', NodeType.ACKNOWLEDGMENTS),\n",
    "        (r'\\\\appendix', NodeType.APPENDIX),\n",
    "    ]\n",
    "    \n",
    "    # Block math environments\n",
    "    BLOCK_MATH_ENVS = [\n",
    "        'equation', 'equation*', 'align', 'align*', 'gather', 'gather*',\n",
    "        'multline', 'multline*', 'eqnarray', 'eqnarray*', 'displaymath'\n",
    "    ]\n",
    "    \n",
    "    # Figure/table environments\n",
    "    FLOAT_ENVS = ['figure', 'figure*', 'table', 'table*']\n",
    "    \n",
    "    # References section patterns (to exclude)\n",
    "    REFERENCES_PATTERNS = [\n",
    "        r'\\\\begin\\{thebibliography\\}',\n",
    "        r'\\\\bibliography\\{',\n",
    "        r'\\\\printbibliography',\n",
    "        r'\\\\section\\*?\\{References\\}',\n",
    "        r'\\\\section\\*?\\{Bibliography\\}',\n",
    "        r'\\\\chapter\\*?\\{References\\}',\n",
    "        r'\\\\chapter\\*?\\{Bibliography\\}',\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, cleaner: LaTeXCleaner = None):\n",
    "        self.cleaner = cleaner or LaTeXCleaner()\n",
    "        self._compile_patterns()\n",
    "        \n",
    "    def _compile_patterns(self):\n",
    "        \"\"\"Compile regex patterns for efficiency\"\"\"\n",
    "        self.section_patterns = [\n",
    "            (re.compile(pattern, re.DOTALL), node_type)\n",
    "            for pattern, node_type in self.SECTION_PATTERNS\n",
    "        ]\n",
    "        self.special_patterns = [\n",
    "            (re.compile(pattern, re.DOTALL | re.IGNORECASE), node_type)\n",
    "            for pattern, node_type in self.SPECIAL_PATTERNS\n",
    "        ]\n",
    "        self.references_pattern = re.compile(\n",
    "            '|'.join(self.REFERENCES_PATTERNS), re.IGNORECASE\n",
    "        )\n",
    "        \n",
    "        # Block math pattern\n",
    "        math_env_pattern = '|'.join(re.escape(env) for env in self.BLOCK_MATH_ENVS)\n",
    "        self.block_math_pattern = re.compile(\n",
    "            rf'\\\\begin\\{{({math_env_pattern})\\}}(.*?)\\\\end\\{{\\1\\}}|'\n",
    "            rf'\\$\\$(.*?)\\$\\$',\n",
    "            re.DOTALL\n",
    "        )\n",
    "        \n",
    "        # Figure/table pattern\n",
    "        float_env_pattern = '|'.join(re.escape(env) for env in self.FLOAT_ENVS)\n",
    "        self.float_pattern = re.compile(\n",
    "            rf'\\\\begin\\{{({float_env_pattern})\\}}(.*?)\\\\end\\{{\\1\\}}',\n",
    "            re.DOTALL\n",
    "        )\n",
    "        \n",
    "        # Label pattern\n",
    "        self.label_pattern = re.compile(r'\\\\label\\{([^}]*)\\}')\n",
    "        \n",
    "        # Caption pattern\n",
    "        self.caption_pattern = re.compile(r'\\\\caption\\{([^}]*)\\}')\n",
    "    \n",
    "    def _is_references_section(self, content: str) -> bool:\n",
    "        \"\"\"Check if content is a references/bibliography section\"\"\"\n",
    "        return bool(self.references_pattern.search(content))\n",
    "    \n",
    "    def _get_hierarchy_level(self, node_type: NodeType) -> int:\n",
    "        \"\"\"Get the hierarchy level of a node type\"\"\"\n",
    "        try:\n",
    "            return HIERARCHY_ORDER.index(node_type)\n",
    "        except ValueError:\n",
    "            return len(HIERARCHY_ORDER)  # Leaf nodes\n",
    "    \n",
    "    def _extract_sections(self, content: str) -> List[Tuple[int, NodeType, str, str]]:\n",
    "        \"\"\"\n",
    "        Extract all section markers from content.\n",
    "        Returns list of (position, node_type, title, full_match)\n",
    "        \"\"\"\n",
    "        sections = []\n",
    "        \n",
    "        # Find all section commands\n",
    "        for pattern, node_type in self.section_patterns:\n",
    "            for match in pattern.finditer(content):\n",
    "                title = match.group(1).strip()\n",
    "                sections.append((match.start(), node_type, title, match.group(0)))\n",
    "        \n",
    "        # Sort by position\n",
    "        sections.sort(key=lambda x: x[0])\n",
    "        return sections\n",
    "    \n",
    "    def _extract_block_formulas(self, content: str) -> List[Tuple[int, int, str]]:\n",
    "        \"\"\"Extract block formulas with their positions\"\"\"\n",
    "        formulas = []\n",
    "        for match in self.block_math_pattern.finditer(content):\n",
    "            formula_content = match.group(2) if match.group(2) else match.group(3)\n",
    "            formulas.append((match.start(), match.end(), formula_content.strip()))\n",
    "        return formulas\n",
    "    \n",
    "    def _extract_floats(self, content: str) -> List[Tuple[int, int, str, str, str]]:\n",
    "        \"\"\"Extract figures and tables with their positions\"\"\"\n",
    "        floats = []\n",
    "        for match in self.float_pattern.finditer(content):\n",
    "            env_type = match.group(1)\n",
    "            env_content = match.group(2)\n",
    "            \n",
    "            # Extract label\n",
    "            label_match = self.label_pattern.search(env_content)\n",
    "            label = label_match.group(1) if label_match else \"\"\n",
    "            \n",
    "            # Extract caption\n",
    "            caption_match = self.caption_pattern.search(env_content)\n",
    "            caption = caption_match.group(1) if caption_match else \"\"\n",
    "            \n",
    "            node_type = NodeType.FIGURE if 'figure' in env_type else NodeType.TABLE\n",
    "            floats.append((match.start(), match.end(), env_content, label, caption, node_type))\n",
    "        \n",
    "        return floats\n",
    "    \n",
    "    def _split_into_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into sentences\"\"\"\n",
    "        # Clean the text first\n",
    "        cleaned = self.cleaner.extract_text_content(text)\n",
    "        \n",
    "        # Simple sentence splitting (handles most cases)\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', cleaned)\n",
    "        \n",
    "        # Filter empty sentences and strip whitespace\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        return sentences\n",
    "    \n",
    "    def _create_leaf_nodes(self, content: str, source_file: str = \"\") -> List[HierarchyNode]:\n",
    "        \"\"\"Create leaf nodes (sentences, formulas, figures) from content\"\"\"\n",
    "        nodes = []\n",
    "        \n",
    "        # Check if this is a references section\n",
    "        if self._is_references_section(content):\n",
    "            return nodes\n",
    "        \n",
    "        # Extract formulas and floats first\n",
    "        formulas = self._extract_block_formulas(content)\n",
    "        floats = self._extract_floats(content)\n",
    "        \n",
    "        # Mark positions of formulas and floats\n",
    "        excluded_ranges = []\n",
    "        \n",
    "        for start, end, formula_content in formulas:\n",
    "            excluded_ranges.append((start, end))\n",
    "            nodes.append(HierarchyNode(\n",
    "                node_type=NodeType.BLOCK_FORMULA,\n",
    "                content=formula_content,\n",
    "                source_file=source_file\n",
    "            ))\n",
    "        \n",
    "        for start, end, float_content, label, caption, node_type in floats:\n",
    "            excluded_ranges.append((start, end))\n",
    "            nodes.append(HierarchyNode(\n",
    "                node_type=node_type,\n",
    "                title=caption,\n",
    "                content=float_content,\n",
    "                label=label,\n",
    "                source_file=source_file\n",
    "            ))\n",
    "        \n",
    "        # Sort excluded ranges\n",
    "        excluded_ranges.sort()\n",
    "        \n",
    "        # Extract text between excluded ranges\n",
    "        text_segments = []\n",
    "        last_end = 0\n",
    "        for start, end in excluded_ranges:\n",
    "            if start > last_end:\n",
    "                text_segments.append(content[last_end:start])\n",
    "            last_end = max(last_end, end)\n",
    "        if last_end < len(content):\n",
    "            text_segments.append(content[last_end:])\n",
    "        \n",
    "        # Split text into sentences\n",
    "        full_text = ' '.join(text_segments)\n",
    "        sentences = self._split_into_sentences(full_text)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if sentence and len(sentence) > 10:  # Filter very short fragments\n",
    "                nodes.append(HierarchyNode(\n",
    "                    node_type=NodeType.SENTENCE,\n",
    "                    content=sentence,\n",
    "                    source_file=source_file\n",
    "                ))\n",
    "        \n",
    "        return nodes\n",
    "    \n",
    "    def _extract_abstract(self, content: str, source_file: str):\n",
    "        pattern = re.compile(\n",
    "            r'\\\\begin\\{abstract\\}(.*?)\\\\end\\{abstract\\}',\n",
    "            re.DOTALL | re.IGNORECASE\n",
    "        )\n",
    "        match = pattern.search(content)\n",
    "        if not match:\n",
    "            return content, None\n",
    "\n",
    "        abstract_raw = match.group(1)\n",
    "\n",
    "        abstract_node = HierarchyNode(\n",
    "            node_type=NodeType.ABSTRACT,\n",
    "            title=\"Abstract\",\n",
    "            source_file=source_file\n",
    "        )\n",
    "\n",
    "        abstract_node.children = self._create_leaf_nodes(\n",
    "            abstract_raw, source_file\n",
    "        )\n",
    "\n",
    "        # remove abstract khỏi content chính\n",
    "        content = content[:match.start()] + content[match.end():]\n",
    "\n",
    "        return content, abstract_node\n",
    "    \n",
    "    def _extract_acknowledgments_content(self, content: str, start: int) -> str:\n",
    "        \"\"\"\n",
    "        Extract acknowledgments content until references/bibliography\n",
    "        \"\"\"\n",
    "        tail = content[start:]\n",
    "\n",
    "        ref_match = self.references_pattern.search(tail)\n",
    "        if ref_match:\n",
    "            return tail[:ref_match.start()]\n",
    "        return tail\n",
    "\n",
    "\n",
    "    \n",
    "    def parse(self, content: str, source_file: str = \"\") -> HierarchyNode:\n",
    "        \"\"\"Parse LaTeX content into a hierarchical tree\"\"\"\n",
    "        # Create root document node\n",
    "        root = HierarchyNode(\n",
    "            node_type=NodeType.DOCUMENT,\n",
    "            title=\"Document\",\n",
    "            source_file=source_file\n",
    "        )\n",
    "        \n",
    "        # Clean content\n",
    "        cleaned_content = self.cleaner.clean(content)\n",
    "\n",
    "        cleaned_content, abstract_node = self._extract_abstract(cleaned_content, source_file)\n",
    "        if abstract_node:\n",
    "            root.children.append(abstract_node)\n",
    "        \n",
    "        # Extract sections\n",
    "        sections = self._extract_sections(cleaned_content)\n",
    "        \n",
    "        if not sections:\n",
    "            # No sections found, create leaf nodes directly under root\n",
    "            root.children = self._create_leaf_nodes(cleaned_content, source_file)\n",
    "            return root\n",
    "        \n",
    "        # Build hierarchy using a stack\n",
    "        stack = [(root, -1)]  # (node, hierarchy_level)\n",
    "        \n",
    "        for i, (pos, node_type, title, full_match) in enumerate(sections):\n",
    "            # Check if this is a references section\n",
    "            is_ack = 'acknowledg' in title.lower()\n",
    "\n",
    "            if self._is_references_section(title) and not is_ack:\n",
    "                continue\n",
    "\n",
    "            # Get content until next section\n",
    "            if i + 1 < len(sections):\n",
    "                next_pos = sections[i + 1][0]\n",
    "            else:\n",
    "                next_pos = len(cleaned_content)\n",
    "            \n",
    "            content_start = pos + len(full_match)\n",
    "\n",
    "            if is_ack:\n",
    "                section_content = self._extract_acknowledgments_content(\n",
    "                    cleaned_content, content_start\n",
    "                )\n",
    "                node_type = NodeType.ACKNOWLEDGMENTS\n",
    "            else:\n",
    "                section_content = cleaned_content[content_start:next_pos]\n",
    "            \n",
    "            level = self._get_hierarchy_level(node_type)\n",
    "            \n",
    "            # Find parent node\n",
    "            while stack and stack[-1][1] >= level:\n",
    "                stack.pop()\n",
    "            \n",
    "            parent = stack[-1][0] if stack else root\n",
    "            \n",
    "            # Create section node\n",
    "            section_node = HierarchyNode(\n",
    "                node_type=node_type,\n",
    "                title=title,\n",
    "                source_file=source_file\n",
    "            )\n",
    "            \n",
    "            # Extract label if present at start of section\n",
    "            label_match = self.label_pattern.search(section_content[:200])\n",
    "            if label_match:\n",
    "                section_node.label = label_match.group(1)\n",
    "            \n",
    "            # Add leaf nodes (sentences, formulas, figures)\n",
    "            section_node.children = self._create_leaf_nodes(section_content, source_file)\n",
    "            \n",
    "            parent.children.append(section_node)\n",
    "            stack.append((section_node, level))\n",
    "        \n",
    "        return root\n",
    "    \n",
    "    def parse_with_file_markers(self, merged_content: str) -> HierarchyNode:\n",
    "        \"\"\"Parse merged content that has file markers\"\"\"\n",
    "        # Split by file markers\n",
    "        file_pattern = re.compile(r'%%% FILE: (.+?) %%%\\n')\n",
    "        parts = file_pattern.split(merged_content)\n",
    "        \n",
    "        # Create root\n",
    "        root = HierarchyNode(\n",
    "            node_type=NodeType.DOCUMENT,\n",
    "            title=\"Document\"\n",
    "        )\n",
    "        \n",
    "        # Process each file's content\n",
    "        current_file = \"\"\n",
    "        for i, part in enumerate(parts):\n",
    "            if i % 2 == 1:  # File path\n",
    "                current_file = part\n",
    "            elif part.strip():  # Content\n",
    "                file_root = self.parse(part, current_file)\n",
    "                # Merge children into main root\n",
    "                root.children.extend(file_root.children)\n",
    "        \n",
    "        return root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ac6c187",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BibTeXExtractor:\n",
    "    \"\"\"Extract and convert bibliography entries from .bbl files and .bib files\"\"\"\n",
    "    \n",
    "    # Pattern to match \\bibitem entries in .bbl files\n",
    "    BIBITEM_PATTERN = re.compile(\n",
    "        r'\\\\bibitem\\[([^\\]]*)\\]\\{([^}]+)\\}\\s*(.*?)(?=\\\\bibitem|\\Z|\\\\end\\{thebibliography\\})',\n",
    "        re.DOTALL\n",
    "    )\n",
    "    \n",
    "    # Alternative pattern without optional argument\n",
    "    BIBITEM_SIMPLE_PATTERN = re.compile(\n",
    "        r'\\\\bibitem\\{([^}]+)\\}\\s*(.*?)(?=\\\\bibitem|\\Z|\\\\end\\{thebibliography\\})',\n",
    "        re.DOTALL\n",
    "    )\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.entries: Dict[str, BibEntry] = {}\n",
    "    \n",
    "    def _parse_bib_entry(self, content: str, start_pos: int) -> Optional[Tuple[str, str, str, int]]:\n",
    "        \"\"\"\n",
    "        Parse a single BibTeX entry starting from @type{key,...}\n",
    "        Returns (entry_type, key, fields_content, end_pos) or None\n",
    "        Handles nested braces correctly.\n",
    "        \"\"\"\n",
    "        # Match @type{key,\n",
    "        entry_start = re.match(r'@(\\w+)\\s*\\{\\s*([^,\\s]+)\\s*,', content[start_pos:])\n",
    "        if not entry_start:\n",
    "            return None\n",
    "        \n",
    "        entry_type = entry_start.group(1).lower()\n",
    "        key = entry_start.group(2).strip()\n",
    "        \n",
    "        # Find matching closing brace by counting\n",
    "        brace_count = 1\n",
    "        pos = start_pos + entry_start.end()\n",
    "        \n",
    "        while pos < len(content) and brace_count > 0:\n",
    "            if content[pos] == '{':\n",
    "                brace_count += 1\n",
    "            elif content[pos] == '}':\n",
    "                brace_count -= 1\n",
    "            pos += 1\n",
    "        \n",
    "        if brace_count != 0:\n",
    "            return None\n",
    "        \n",
    "        # Extract fields content (between key, and final })\n",
    "        fields_start = start_pos + entry_start.end()\n",
    "        fields_content = content[fields_start:pos-1]\n",
    "        \n",
    "        return (entry_type, key, fields_content, pos)\n",
    "    \n",
    "    def _parse_field_value(self, content: str, start_pos: int) -> Tuple[str, int]:\n",
    "        \"\"\"\n",
    "        Parse a field value that can be:\n",
    "        - {nested {braces} allowed}\n",
    "        - \"quoted string\"\n",
    "        - bare_number\n",
    "        Returns (value, end_pos)\n",
    "        \"\"\"\n",
    "        pos = start_pos\n",
    "        \n",
    "        # Skip whitespace\n",
    "        while pos < len(content) and content[pos] in ' \\t\\n\\r':\n",
    "            pos += 1\n",
    "        \n",
    "        if pos >= len(content):\n",
    "            return (\"\", pos)\n",
    "        \n",
    "        # Case 1: Braced value {....}\n",
    "        if content[pos] == '{':\n",
    "            brace_count = 1\n",
    "            value_start = pos + 1\n",
    "            pos += 1\n",
    "            while pos < len(content) and brace_count > 0:\n",
    "                if content[pos] == '{':\n",
    "                    brace_count += 1\n",
    "                elif content[pos] == '}':\n",
    "                    brace_count -= 1\n",
    "                pos += 1\n",
    "            value = content[value_start:pos-1]\n",
    "            return (value, pos)\n",
    "        \n",
    "        # Case 2: Quoted value \"...\"\n",
    "        elif content[pos] == '\"':\n",
    "            value_start = pos + 1\n",
    "            pos += 1\n",
    "            while pos < len(content) and content[pos] != '\"':\n",
    "                if content[pos] == '\\\\' and pos + 1 < len(content):\n",
    "                    pos += 2  # Skip escaped char\n",
    "                else:\n",
    "                    pos += 1\n",
    "            value = content[value_start:pos]\n",
    "            if pos < len(content):\n",
    "                pos += 1  # Skip closing quote\n",
    "            return (value, pos)\n",
    "        \n",
    "        # Case 3: Bare value (number or string constant)\n",
    "        else:\n",
    "            value_start = pos\n",
    "            while pos < len(content) and content[pos] not in ',}\\n':\n",
    "                pos += 1\n",
    "            value = content[value_start:pos].strip()\n",
    "            return (value, pos)\n",
    "    \n",
    "    def _parse_fields(self, fields_content: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Parse all fields from the content between @type{key, ... }\n",
    "        Handles multi-line values and nested braces.\n",
    "        \"\"\"\n",
    "        fields = {}\n",
    "        pos = 0\n",
    "        \n",
    "        while pos < len(fields_content):\n",
    "            # Skip whitespace and commas\n",
    "            while pos < len(fields_content) and fields_content[pos] in ' \\t\\n\\r,':\n",
    "                pos += 1\n",
    "            \n",
    "            if pos >= len(fields_content):\n",
    "                break\n",
    "            \n",
    "            # Match field name followed by =\n",
    "            field_match = re.match(r'(\\w+)\\s*=\\s*', fields_content[pos:])\n",
    "            if not field_match:\n",
    "                pos += 1\n",
    "                continue\n",
    "            \n",
    "            field_name = field_match.group(1).lower()\n",
    "            pos += field_match.end()\n",
    "            \n",
    "            # Parse field value\n",
    "            value, pos = self._parse_field_value(fields_content, pos)\n",
    "            \n",
    "            # Handle string concatenation with #\n",
    "            while pos < len(fields_content):\n",
    "                # Skip whitespace\n",
    "                temp_pos = pos\n",
    "                while temp_pos < len(fields_content) and fields_content[temp_pos] in ' \\t\\n\\r':\n",
    "                    temp_pos += 1\n",
    "                \n",
    "                if temp_pos < len(fields_content) and fields_content[temp_pos] == '#':\n",
    "                    temp_pos += 1\n",
    "                    additional_value, temp_pos = self._parse_field_value(fields_content, temp_pos)\n",
    "                    value += additional_value\n",
    "                    pos = temp_pos\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # Clean up the value\n",
    "            value = re.sub(r'\\s+', ' ', value).strip()\n",
    "            fields[field_name] = value\n",
    "        \n",
    "        return fields\n",
    "        \n",
    "    def parse_bib_file(self, content: str) -> Dict[str, BibEntry]:\n",
    "        \"\"\"Parse a .bib file and extract entries with robust multi-line support\"\"\"\n",
    "        entries = {}\n",
    "        \n",
    "        # Find all @type{ patterns\n",
    "        entry_pattern = re.compile(r'@\\w+\\s*\\{', re.IGNORECASE)\n",
    "        \n",
    "        for match in entry_pattern.finditer(content):\n",
    "            result = self._parse_bib_entry(content, match.start())\n",
    "            if result:\n",
    "                entry_type, key, fields_content, end_pos = result\n",
    "                \n",
    "                # Skip comments\n",
    "                if entry_type == 'comment':\n",
    "                    continue\n",
    "                \n",
    "                # Parse fields\n",
    "                fields = self._parse_fields(fields_content)\n",
    "                \n",
    "                if key and (fields or entry_type in ['string', 'preamble']):\n",
    "                    entries[key] = BibEntry(key=key, entry_type=entry_type, fields=fields)\n",
    "        \n",
    "        return entries\n",
    "    \n",
    "    def parse_bbl_file(self, content: str) -> Dict[str, BibEntry]:\n",
    "        \"\"\"Parse a .bbl file and convert bibitem entries to BibTeX format\"\"\"\n",
    "        entries = {}\n",
    "        \n",
    "        # Try pattern with optional citation label\n",
    "        for match in self.BIBITEM_PATTERN.finditer(content):\n",
    "            cite_label = match.group(1)\n",
    "            key = match.group(2).strip()\n",
    "            entry_content = match.group(3).strip()\n",
    "            \n",
    "            entry = self._parse_bibitem_content(key, entry_content)\n",
    "            if entry:\n",
    "                entries[key] = entry\n",
    "        \n",
    "        # If no entries found, try simple pattern\n",
    "        if not entries:\n",
    "            for match in self.BIBITEM_SIMPLE_PATTERN.finditer(content):\n",
    "                key = match.group(1).strip()\n",
    "                entry_content = match.group(2).strip()\n",
    "                \n",
    "                entry = self._parse_bibitem_content(key, entry_content)\n",
    "                if entry:\n",
    "                    entries[key] = entry\n",
    "        \n",
    "        return entries\n",
    "    \n",
    "    def _parse_bibitem_content(self, key: str, content: str) -> Optional[BibEntry]:\n",
    "        \"\"\"\n",
    "        Parse the content of a bibitem and extract bibliographic fields.\n",
    "        \n",
    "        Uses \\newblock as anchor points to separate:\n",
    "        - Author (before first \\newblock)\n",
    "        - Title (second \\newblock)\n",
    "        - Publication info (remaining \\newblocks)\n",
    "        \"\"\"\n",
    "        fields = {}\n",
    "        original_content = content\n",
    "        \n",
    "        content = re.sub(r'\\s+', ' ', content).strip()\n",
    "        \n",
    "        parts = content.split('\\\\newblock')\n",
    "        parts = [p.strip() for p in parts if p.strip()]\n",
    "        \n",
    "        # If no \\newblock found or content is too short, treat as unstructured\n",
    "        if len(parts) < 2 or len(content) < 50:\n",
    "            # Save cleaned content as note\n",
    "            cleaned = self._clean_latex(content)\n",
    "            if cleaned:\n",
    "                fields['note'] = cleaned[:500]\n",
    "            # Try to extract year anyway\n",
    "            year_match = re.search(r'\\b(19|20)\\d{2}\\b', content)\n",
    "            if year_match:\n",
    "                fields['year'] = year_match.group(0)\n",
    "            return BibEntry(key=key, entry_type='misc', fields=fields)\n",
    "        \n",
    "        # Extract Author (first part)\n",
    "        author_part = parts[0]\n",
    "        # Remove trailing period if present\n",
    "        author_part = re.sub(r'\\.\\s*$', '', author_part)\n",
    "        # Clean LaTeX formatting commands but keep the text\n",
    "        author_part = re.sub(r'\\\\[a-zA-Z]+\\{([^}]*)\\}', r'\\1', author_part)\n",
    "        author_part = re.sub(r'[{}]', '', author_part)\n",
    "        if author_part:\n",
    "            fields['author'] = author_part\n",
    "        \n",
    "        # Extract Title (second part)\n",
    "        if len(parts) >= 2:\n",
    "            title_part = parts[1]\n",
    "            # Remove trailing period\n",
    "            title_part = re.sub(r'\\.\\s*$', '', title_part)\n",
    "            # Clean LaTeX but keep text\n",
    "            title_part = re.sub(r'\\\\[a-zA-Z]+\\{([^}]*)\\}', r'\\1', title_part)\n",
    "            title_part = re.sub(r'[{}]', '', title_part).strip()\n",
    "            if title_part:\n",
    "                # Wrap in braces to preserve capitalization in BibTeX\n",
    "                fields['title'] = '{' + title_part + '}'\n",
    "        \n",
    "        # Extract Publication Info (remaining parts)\n",
    "        full_content = content  # Use full content for extraction\n",
    "        \n",
    "        # Extract year (4 digits: 19xx or 20xx)\n",
    "        year_match = re.search(r'\\b(19|20)\\d{2}\\b', full_content)\n",
    "        if year_match:\n",
    "            fields['year'] = year_match.group(0)\n",
    "        \n",
    "        # Extract journal from \\emph{...} or {\\em ...}\n",
    "        journal_match = re.search(r'\\\\emph\\{([^}]+)\\}', full_content)\n",
    "        if not journal_match:\n",
    "            journal_match = re.search(r'\\{\\\\em\\s+([^}]+)\\}', full_content)\n",
    "        if journal_match:\n",
    "            venue = journal_match.group(1).strip()\n",
    "            # Determine if journal or conference proceedings\n",
    "            if any(kw in venue.lower() for kw in ['proc', 'conf', 'workshop', 'symposium', 'international']):\n",
    "                fields['booktitle'] = venue\n",
    "            else:\n",
    "                fields['journal'] = venue\n",
    "        \n",
    "        # Extract Volume:Page format (e.g., \"87:085115\" or \"87:1--50\")\n",
    "        vol_page_match = re.search(r'(\\d+):(\\d+(?:--?\\d+)?)', full_content)\n",
    "        if vol_page_match:\n",
    "            fields['volume'] = vol_page_match.group(1)\n",
    "            pages = vol_page_match.group(2).replace('–', '--').replace('-', '--')\n",
    "            # Normalize single dash to double dash\n",
    "            if '--' not in pages and re.match(r'\\d+\\d+', pages):\n",
    "                pass  # Single page number, keep as is\n",
    "            fields['pages'] = pages\n",
    "        else:\n",
    "            # Try separate volume and pages patterns\n",
    "            volume_match = re.search(r'vol(?:ume)?\\.?\\s*(\\d+)', full_content, re.IGNORECASE)\n",
    "            if volume_match:\n",
    "                fields['volume'] = volume_match.group(1)\n",
    "            \n",
    "            pages_match = re.search(r'pages?\\s*[:\\s]*(\\d+(?:\\s*[-–]\\s*\\d+)?)', full_content, re.IGNORECASE)\n",
    "            if pages_match:\n",
    "                fields['pages'] = pages_match.group(1).replace('–', '--')\n",
    "        \n",
    "        # Extract URL\n",
    "        url_match = re.search(r'\\\\url\\{([^}]+)\\}', full_content)\n",
    "        if url_match:\n",
    "            fields['url'] = url_match.group(1)\n",
    "        \n",
    "        # Extract DOI\n",
    "        doi_match = re.search(r'doi[:\\s]*([^\\s,}]+)', full_content, re.IGNORECASE)\n",
    "        if doi_match:\n",
    "            fields['doi'] = doi_match.group(1)\n",
    "        \n",
    "        # Determine entry type\n",
    "        entry_type = self._guess_entry_type(fields, full_content)\n",
    "        \n",
    "        # If still missing key fields, add note with original content\n",
    "        if 'title' not in fields and 'author' not in fields:\n",
    "            fields['note'] = self._clean_latex(original_content)[:500]\n",
    "        \n",
    "        return BibEntry(key=key, entry_type=entry_type, fields=fields)\n",
    "    \n",
    "    def _clean_latex(self, content: str) -> str:\n",
    "        \"\"\"Remove common LaTeX formatting commands\"\"\"\n",
    "        # Remove \\newblock\n",
    "        content = re.sub(r'\\\\newblock\\s*', '', content)\n",
    "        # Remove common formatting\n",
    "        content = re.sub(r'\\\\textit\\{([^}]*)\\}', r'\\1', content)\n",
    "        content = re.sub(r'\\\\textbf\\{([^}]*)\\}', r'\\1', content)\n",
    "        content = re.sub(r'\\\\texttt\\{([^}]*)\\}', r'\\1', content)\n",
    "        content = re.sub(r'\\\\emph\\{([^}]*)\\}', r'\\1', content)\n",
    "        # Remove escaped characters\n",
    "        content = content.replace('\\\\&', '&')\n",
    "        content = content.replace('\\\\~', '~')\n",
    "        content = content.replace('\\\\{', '{')\n",
    "        content = content.replace('\\\\}', '}')\n",
    "        # Normalize whitespace\n",
    "        content = re.sub(r'\\s+', ' ', content)\n",
    "        return content.strip()\n",
    "    \n",
    "    def _guess_entry_type(self, fields: Dict[str, str], content: str) -> str:\n",
    "        \"\"\"Guess the BibTeX entry type based on available fields\"\"\"\n",
    "        content_lower = content.lower()\n",
    "        \n",
    "        if 'booktitle' in fields or 'proceedings' in content_lower or 'conference' in content_lower:\n",
    "            return 'inproceedings'\n",
    "        elif 'journal' in fields:\n",
    "            return 'article'\n",
    "        elif 'url' in fields and ('howpublished' in content_lower or 'accessed' in content_lower):\n",
    "            return 'misc'\n",
    "        elif 'publisher' in content_lower or 'press' in content_lower:\n",
    "            return 'book'\n",
    "        elif 'thesis' in content_lower:\n",
    "            if 'phd' in content_lower or 'doctoral' in content_lower:\n",
    "                return 'phdthesis'\n",
    "            elif 'master' in content_lower:\n",
    "                return 'mastersthesis'\n",
    "        elif 'arxiv' in content_lower:\n",
    "            return 'article'\n",
    "        \n",
    "        return 'misc'\n",
    "    \n",
    "    def parse_tex_bibitems(self, content: str) -> Dict[str, BibEntry]:\n",
    "        \"\"\"\n",
    "        Parse \\bibitem entries directly from .tex file content.\n",
    "        This handles cases where bibliography is defined inline in the tex file.\n",
    "        \"\"\"\n",
    "        entries = {}\n",
    "        \n",
    "        # Look for thebibliography environment\n",
    "        bib_env_pattern = re.compile(\n",
    "            r'\\\\begin\\{thebibliography\\}.*?(.*?)\\\\end\\{thebibliography\\}',\n",
    "            re.DOTALL\n",
    "        )\n",
    "        \n",
    "        bib_match = bib_env_pattern.search(content)\n",
    "        if bib_match:\n",
    "            bib_content = bib_match.group(1)\n",
    "            \n",
    "            # Try pattern with optional citation label first\n",
    "            for match in self.BIBITEM_PATTERN.finditer(bib_content):\n",
    "                cite_label = match.group(1)\n",
    "                key = match.group(2).strip()\n",
    "                entry_content = match.group(3).strip()\n",
    "                \n",
    "                entry = self._parse_bibitem_content(key, entry_content)\n",
    "                if entry:\n",
    "                    entries[key] = entry\n",
    "            \n",
    "            # If no entries found, try simple pattern\n",
    "            if not entries:\n",
    "                for match in self.BIBITEM_SIMPLE_PATTERN.finditer(bib_content):\n",
    "                    key = match.group(1).strip()\n",
    "                    entry_content = match.group(2).strip()\n",
    "                    \n",
    "                    entry = self._parse_bibitem_content(key, entry_content)\n",
    "                    if entry:\n",
    "                        entries[key] = entry\n",
    "        \n",
    "        return entries\n",
    "    \n",
    "    def load_from_directory(self, base_dir: str) -> Dict[str, BibEntry]:\n",
    "        \"\"\"Load all bibliography entries from .bib, .bbl, and .tex files in directory\"\"\"\n",
    "        base_path = Path(base_dir)\n",
    "        \n",
    "        # Load .bib files (highest priority - already in BibTeX format)\n",
    "        for bib_file in base_path.rglob('*.bib'):\n",
    "            try:\n",
    "                with open(bib_file, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                    content = f.read()\n",
    "                entries = self.parse_bib_file(content)\n",
    "                self.entries.update(entries)\n",
    "                if entries:\n",
    "                    print(f\"Loaded {len(entries)} entries from {bib_file.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {bib_file}: {e}\")\n",
    "        \n",
    "        # Load .bbl files\n",
    "        for bbl_file in base_path.rglob('*.bbl'):\n",
    "            try:\n",
    "                with open(bbl_file, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                    content = f.read()\n",
    "                entries = self.parse_bbl_file(content)\n",
    "                # Only add entries not already present from .bib files\n",
    "                new_count = 0\n",
    "                for key, entry in entries.items():\n",
    "                    if key not in self.entries:\n",
    "                        self.entries[key] = entry\n",
    "                        new_count += 1\n",
    "                if new_count:\n",
    "                    print(f\"Loaded {new_count} entries from {bbl_file.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {bbl_file}: {e}\")\n",
    "        \n",
    "        # Load \\bibitem entries from .tex files\n",
    "        for tex_file in base_path.rglob('*.tex'):\n",
    "            try:\n",
    "                with open(tex_file, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                # Only parse if file contains thebibliography environment\n",
    "                if r'\\begin{thebibliography}' in content:\n",
    "                    entries = self.parse_tex_bibitems(content)\n",
    "                    # Only add entries not already present\n",
    "                    new_count = 0\n",
    "                    for key, entry in entries.items():\n",
    "                        if key not in self.entries:\n",
    "                            self.entries[key] = entry\n",
    "                            new_count += 1\n",
    "                    if new_count:\n",
    "                        print(f\"Loaded {new_count} bibitem entries from {tex_file.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading bibitems from {tex_file}: {e}\")\n",
    "        \n",
    "        return self.entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "502eb56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deduplicator:\n",
    "    \"\"\"Handles deduplication of references and content\"\"\"\n",
    "    \n",
    "    # Minimum similarity threshold for title fuzzy matching\n",
    "    TITLE_SIMILARITY_THRESHOLD = 0.6\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.content_hashes: Dict[str, str] = {}  # hash -> unique_id\n",
    "        self.reference_hashes: Dict[str, str] = {}  # content_hash -> key\n",
    "        self.key_mappings: Dict[str, str] = {}  # old_key -> canonical_key\n",
    "        \n",
    "    # ==================== Reference Deduplication ====================\n",
    "    \n",
    "    def _titles_are_similar(self, entry1: BibEntry, entry2: BibEntry) -> bool:\n",
    "        \"\"\"\n",
    "        Check if two entries have similar titles using fuzzy matching.\n",
    "        Returns True if titles are similar enough to be considered the same reference.\n",
    "        Returns True if either entry lacks a title (can't verify, allow merge based on other fields).\n",
    "        \"\"\"\n",
    "        title1 = entry1.get_normalized_title()\n",
    "        title2 = entry2.get_normalized_title()\n",
    "        \n",
    "        # If either is empty, we can't compare - allow based on other matching criteria\n",
    "        if not title1 or not title2:\n",
    "            return True\n",
    "        \n",
    "        # Quick exact match\n",
    "        if title1 == title2:\n",
    "            return True\n",
    "        \n",
    "        # Compute Jaccard similarity on words\n",
    "        words1 = set(title1.split())\n",
    "        words2 = set(title2.split())\n",
    "        \n",
    "        if not words1 or not words2:\n",
    "            return True\n",
    "        \n",
    "        intersection = len(words1 & words2)\n",
    "        union = len(words1 | words2)\n",
    "        \n",
    "        similarity = intersection / union if union > 0 else 0.0\n",
    "        \n",
    "        return similarity >= self.TITLE_SIMILARITY_THRESHOLD\n",
    "    \n",
    "    def _create_merged_entry(self, canonical: BibEntry, duplicate: BibEntry) -> BibEntry:\n",
    "        \"\"\"\n",
    "        Create a new merged entry without mutating the originals.\n",
    "        Fields from duplicate are only added if canonical lacks them.\n",
    "        \"\"\"\n",
    "        # Create a copy of canonical's fields\n",
    "        merged_fields = dict(canonical.fields)\n",
    "        \n",
    "        # Add missing fields from duplicate (don't overwrite existing)\n",
    "        for field, value in duplicate.fields.items():\n",
    "            if field not in merged_fields or not merged_fields[field].strip():\n",
    "                if value and value.strip():\n",
    "                    merged_fields[field] = value\n",
    "        \n",
    "        # Return new BibEntry (don't modify original)\n",
    "        return BibEntry(\n",
    "            key=canonical.key,\n",
    "            entry_type=canonical.entry_type,\n",
    "            fields=merged_fields\n",
    "        )\n",
    "    \n",
    "    def deduplicate_references(self, entries: Dict[str, BibEntry]) -> Dict[str, BibEntry]:\n",
    "        \"\"\"\n",
    "        Deduplicate bibliography entries safely.\n",
    "        \n",
    "        - Creates new objects instead of mutating originals\n",
    "        - Verifies title similarity before merging (fuzzy match)\n",
    "        - Only merges fields that are missing in canonical entry\n",
    "        \"\"\"\n",
    "        deduplicated: Dict[str, BibEntry] = {}\n",
    "        hash_to_key: Dict[str, str] = {}\n",
    "        self.key_mappings.clear()\n",
    "        \n",
    "        for key, entry in entries.items():\n",
    "            content_hash = entry.content_hash()\n",
    "            \n",
    "            if content_hash in hash_to_key:\n",
    "                canonical_key = hash_to_key[content_hash]\n",
    "                canonical_entry = deduplicated[canonical_key]\n",
    "                \n",
    "                # SAFETY CHECK: Verify titles are actually similar\n",
    "                if not self._titles_are_similar(canonical_entry, entry):\n",
    "                    # Hash collision but different content - treat as unique\n",
    "                    # Use a modified hash to differentiate\n",
    "                    unique_hash = f\"{content_hash}_{key}\"\n",
    "                    hash_to_key[unique_hash] = key\n",
    "                    # Create a copy to avoid mutation\n",
    "                    deduplicated[key] = BibEntry(\n",
    "                        key=entry.key,\n",
    "                        entry_type=entry.entry_type,\n",
    "                        fields=dict(entry.fields)\n",
    "                    )\n",
    "                    self.key_mappings[key] = key\n",
    "                    continue\n",
    "                \n",
    "                # Titles match - safe to merge\n",
    "                # Create new merged entry (don't mutate canonical)\n",
    "                merged_entry = self._create_merged_entry(canonical_entry, entry)\n",
    "                deduplicated[canonical_key] = merged_entry\n",
    "                \n",
    "                self.key_mappings[key] = canonical_key\n",
    "                print(f\"Merged duplicate: {key} -> {canonical_key}\")\n",
    "            else:\n",
    "                # New unique entry - create a copy\n",
    "                hash_to_key[content_hash] = key\n",
    "                deduplicated[key] = BibEntry(\n",
    "                    key=entry.key,\n",
    "                    entry_type=entry.entry_type,\n",
    "                    fields=dict(entry.fields)\n",
    "                )\n",
    "                self.key_mappings[key] = key\n",
    "        \n",
    "        self.reference_hashes = hash_to_key\n",
    "        return deduplicated\n",
    "    \n",
    "    def find_similar_references(self, entries: Dict[str, BibEntry], \n",
    "                                 threshold: float = 0.8) -> List[Tuple[str, str, float]]:\n",
    "        \"\"\"\n",
    "        Find potentially duplicate references using fuzzy matching.\n",
    "        Returns list of (key1, key2, similarity_score)\n",
    "        \"\"\"\n",
    "        similar_pairs = []\n",
    "        keys = list(entries.keys())\n",
    "        \n",
    "        for i, key1 in enumerate(keys):\n",
    "            for key2 in keys[i+1:]:\n",
    "                entry1 = entries[key1]\n",
    "                entry2 = entries[key2]\n",
    "                \n",
    "                similarity = self._compute_entry_similarity(entry1, entry2)\n",
    "                if similarity >= threshold:\n",
    "                    similar_pairs.append((key1, key2, similarity))\n",
    "        \n",
    "        return sorted(similar_pairs, key=lambda x: -x[2])\n",
    "    \n",
    "    def _compute_entry_similarity(self, entry1: BibEntry, entry2: BibEntry) -> float:\n",
    "        \"\"\"Compute similarity between two bibliography entries\"\"\"\n",
    "        scores = []\n",
    "        \n",
    "        # Compare titles\n",
    "        if 'title' in entry1.fields and 'title' in entry2.fields:\n",
    "            title_sim = self._string_similarity(\n",
    "                entry1.fields['title'].lower(),\n",
    "                entry2.fields['title'].lower()\n",
    "            )\n",
    "            scores.append(title_sim * 2)  # Weight title higher\n",
    "        \n",
    "        # Compare authors\n",
    "        if 'author' in entry1.fields and 'author' in entry2.fields:\n",
    "            author_sim = self._string_similarity(\n",
    "                entry1.fields['author'].lower(),\n",
    "                entry2.fields['author'].lower()\n",
    "            )\n",
    "            scores.append(author_sim * 1.5)\n",
    "        \n",
    "        # Compare year\n",
    "        if 'year' in entry1.fields and 'year' in entry2.fields:\n",
    "            year_sim = 1.0 if entry1.fields['year'] == entry2.fields['year'] else 0.0\n",
    "            scores.append(year_sim)\n",
    "        \n",
    "        if not scores:\n",
    "            return 0.0\n",
    "        \n",
    "        return sum(scores) / (2 + 1.5 + 1)  # Normalized by weights\n",
    "    \n",
    "    def _string_similarity(self, s1: str, s2: str) -> float:\n",
    "        \"\"\"Compute similarity between two strings using Jaccard similarity on words\"\"\"\n",
    "        words1 = set(s1.split())\n",
    "        words2 = set(s2.split())\n",
    "        \n",
    "        if not words1 or not words2:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = len(words1 & words2)\n",
    "        union = len(words1 | words2)\n",
    "        \n",
    "        return intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    # ==================== Content Deduplication ====================\n",
    "    \n",
    "    def deduplicate_content(self, root: HierarchyNode) -> HierarchyNode:\n",
    "        \"\"\"\n",
    "        Deduplicate content in the hierarchy tree.\n",
    "        Identical content elements are given the same unique_id.\n",
    "        \"\"\"\n",
    "        self.content_hashes.clear()\n",
    "        return self._deduplicate_node(root)\n",
    "    \n",
    "    def _deduplicate_node(self, node: HierarchyNode) -> HierarchyNode:\n",
    "        \"\"\"Recursively deduplicate a node and its children\"\"\"\n",
    "        # Process children first\n",
    "        deduplicated_children = []\n",
    "        for child in node.children:\n",
    "            deduplicated_child = self._deduplicate_node(child)\n",
    "            deduplicated_children.append(deduplicated_child)\n",
    "        \n",
    "        node.children = deduplicated_children\n",
    "        \n",
    "        # For leaf nodes, check for duplicates\n",
    "        if node.node_type in LEAF_TYPES and node.content:\n",
    "            content_hash = node.content_hash\n",
    "            \n",
    "            if content_hash in self.content_hashes:\n",
    "                # Use existing unique_id for duplicate content\n",
    "                node.unique_id = self.content_hashes[content_hash]\n",
    "            else:\n",
    "                # Store this as the canonical version\n",
    "                self.content_hashes[content_hash] = node.unique_id\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def get_duplicate_stats(self, root: HierarchyNode) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics about duplicates in the hierarchy\"\"\"\n",
    "        stats = {\n",
    "            'total_nodes': 0,\n",
    "            'unique_contents': 0,\n",
    "            'duplicate_contents': 0,\n",
    "            'by_type': defaultdict(lambda: {'total': 0, 'unique': 0})\n",
    "        }\n",
    "        \n",
    "        seen_hashes = set()\n",
    "        \n",
    "        def count_node(node: HierarchyNode):\n",
    "            stats['total_nodes'] += 1\n",
    "            stats['by_type'][node.node_type.value]['total'] += 1\n",
    "            \n",
    "            if node.node_type in LEAF_TYPES and node.content_hash:\n",
    "                if node.content_hash not in seen_hashes:\n",
    "                    seen_hashes.add(node.content_hash)\n",
    "                    stats['unique_contents'] += 1\n",
    "                    stats['by_type'][node.node_type.value]['unique'] += 1\n",
    "                else:\n",
    "                    stats['duplicate_contents'] += 1\n",
    "            \n",
    "            for child in node.children:\n",
    "                count_node(child)\n",
    "        \n",
    "        count_node(root)\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06918bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaTeXParser:\n",
    "    \"\"\"\n",
    "    Main LaTeX parser that integrates all components:\n",
    "    - Multi-file gathering\n",
    "    - Hierarchy construction\n",
    "    - Reference extraction\n",
    "    - Deduplication\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir: str):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.gatherer = LaTeXFileGatherer(base_dir)\n",
    "        self.cleaner = LaTeXCleaner()\n",
    "        self.hierarchy_parser = HierarchyParser(self.cleaner)\n",
    "        self.bib_extractor = BibTeXExtractor()\n",
    "        self.deduplicator = Deduplicator()\n",
    "        \n",
    "        # Results\n",
    "        self.hierarchy: Optional[HierarchyNode] = None\n",
    "        self.references: Dict[str, BibEntry] = {}\n",
    "        self.file_contents: Dict[str, str] = {}\n",
    "        \n",
    "    def parse(self, main_file: str = \"main.tex\") -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Parse the LaTeX document starting from the main file.\n",
    "        Returns a dictionary with hierarchy, references, and statistics.\n",
    "        \"\"\"\n",
    "        print(f\"Parsing LaTeX document from: {self.base_dir / main_file}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Multi-file gathering\n",
    "        print(\"\\n[1] Gathering files...\")\n",
    "        self.file_contents = self.gatherer.gather_files(main_file)\n",
    "        print(f\"    Found {len(self.file_contents)} files in compilation path\")\n",
    "        \n",
    "        # Report unused files\n",
    "        unused = self.gatherer.get_unused_files()\n",
    "        if unused:\n",
    "            print(f\"    Unused files ({len(unused)}):\")\n",
    "            for f in list(unused)[:5]:\n",
    "                print(f\"      - {Path(f).name}\")\n",
    "            if len(unused) > 5:\n",
    "                print(f\"      ... and {len(unused) - 5} more\")\n",
    "        \n",
    "        # Step 2: Build hierarchy\n",
    "        print(\"\\n[2] Building hierarchy...\")\n",
    "        merged_content = self.gatherer.get_merged_content()\n",
    "        self.hierarchy = self.hierarchy_parser.parse_with_file_markers(merged_content)\n",
    "        \n",
    "        hierarchy_stats = self._count_hierarchy_nodes(self.hierarchy)\n",
    "        print(f\"    Built hierarchy with {hierarchy_stats['total']} nodes:\")\n",
    "        for node_type, count in hierarchy_stats['by_type'].items():\n",
    "            print(f\"      - {node_type}: {count}\")\n",
    "        \n",
    "        # Step 3: Extract references\n",
    "        print(\"\\n[3] Extracting references...\")\n",
    "        self.references = self.bib_extractor.load_from_directory(str(self.base_dir))\n",
    "        print(f\"    Found {len(self.references)} bibliography entries\")\n",
    "        \n",
    "        # Step 4: Deduplicate references\n",
    "        print(\"\\n[4] Deduplicating references...\")\n",
    "        original_count = len(self.references)\n",
    "        self.references = self.deduplicator.deduplicate_references(self.references)\n",
    "        dedup_count = original_count - len(self.references)\n",
    "        print(f\"    Removed {dedup_count} duplicate references\")\n",
    "        \n",
    "        # Find similar references (potential duplicates with different keys)\n",
    "        similar = self.deduplicator.find_similar_references(self.references, threshold=0.7)\n",
    "        if similar:\n",
    "            print(f\"    Found {len(similar)} pairs of similar references:\")\n",
    "            for key1, key2, sim in similar[:3]:\n",
    "                print(f\"      - '{key1}' ~ '{key2}' (similarity: {sim:.2%})\")\n",
    "        \n",
    "        # Step 5: Deduplicate content\n",
    "        print(\"\\n[5] Deduplicating content...\")\n",
    "        self.hierarchy = self.deduplicator.deduplicate_content(self.hierarchy)\n",
    "        dup_stats = self.deduplicator.get_duplicate_stats(self.hierarchy)\n",
    "        print(f\"    Unique content items: {dup_stats['unique_contents']}\")\n",
    "        print(f\"    Duplicate content items: {dup_stats['duplicate_contents']}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Parsing complete!\")\n",
    "        \n",
    "        return {\n",
    "            'hierarchy': self.hierarchy,\n",
    "            'references': self.references,\n",
    "            'file_contents': self.file_contents,\n",
    "            'stats': {\n",
    "                'files': len(self.file_contents),\n",
    "                'unused_files': len(unused),\n",
    "                'hierarchy_nodes': hierarchy_stats,\n",
    "                'references': len(self.references),\n",
    "                'duplicate_refs_removed': dedup_count,\n",
    "                'content_dedup': dup_stats\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _count_hierarchy_nodes(self, node: HierarchyNode) -> Dict[str, Any]:\n",
    "        \"\"\"Count nodes in hierarchy by type\"\"\"\n",
    "        stats = {'total': 0, 'by_type': defaultdict(int)}\n",
    "        \n",
    "        def count(n: HierarchyNode):\n",
    "            stats['total'] += 1\n",
    "            stats['by_type'][n.node_type.value] += 1\n",
    "            for child in n.children:\n",
    "                count(child)\n",
    "        \n",
    "        count(node)\n",
    "        return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1336c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MilestoneExporter:\n",
    "    \"\"\"\n",
    "    Export parsed LaTeX document to the milestone-required format.\n",
    "    \n",
    "    Format for <yymm-id>.json:\n",
    "    {\n",
    "        \"elements\": {\n",
    "            \"id-string-1\": \"Cleaned latex content of element 1\",\n",
    "            ...\n",
    "        },\n",
    "        \"hierarchy\": {\n",
    "            \"1\": {  // Version 1\n",
    "                \"child-id\": \"parent-id\",\n",
    "                ...\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.elements: Dict[str, str] = {}  # unique_id -> cleaned content\n",
    "        self.hierarchy: Dict[str, Dict[str, str]] = {}  # version -> {child_id: parent_id}\n",
    "        \n",
    "    def export_document(self, root: HierarchyNode, version: str = \"1\") -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Export a parsed document to the milestone format.\n",
    "        \n",
    "        Args:\n",
    "            root: The root HierarchyNode of the parsed document\n",
    "            version: Version identifier (e.g., \"1\", \"2\", etc.)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary in the required format\n",
    "        \"\"\"\n",
    "        self.elements.clear()\n",
    "        self.hierarchy.clear()\n",
    "        \n",
    "        # Process the hierarchy tree\n",
    "        self._process_node(root, parent_id=None, version=version)\n",
    "        \n",
    "        return {\n",
    "            \"elements\": self.elements,\n",
    "            \"hierarchy\": self.hierarchy\n",
    "        }\n",
    "    \n",
    "    def _process_node(self, node: HierarchyNode, parent_id: Optional[str], version: str):\n",
    "        \"\"\"Recursively process nodes to extract elements and build hierarchy\"\"\"\n",
    "        \n",
    "        # Initialize version hierarchy if not exists\n",
    "        if version not in self.hierarchy:\n",
    "            self.hierarchy[version] = {}\n",
    "        \n",
    "        current_id = node.unique_id\n",
    "        \n",
    "        # For leaf nodes, store the content in elements\n",
    "        if node.node_type in LEAF_TYPES:\n",
    "            if node.content:\n",
    "                # Only store if not already present (deduplication)\n",
    "                if current_id not in self.elements:\n",
    "                    self.elements[current_id] = node.content\n",
    "                \n",
    "                # Add to hierarchy (child -> parent relationship)\n",
    "                if parent_id:\n",
    "                    self.hierarchy[version][current_id] = parent_id\n",
    "        else:\n",
    "            # For non-leaf nodes, also track in hierarchy but don't store content\n",
    "            # Store section/chapter titles as content for context\n",
    "            if node.title:\n",
    "                content = f\"[{node.node_type.value.upper()}] {node.title}\"\n",
    "                if current_id not in self.elements:\n",
    "                    self.elements[current_id] = content\n",
    "            \n",
    "            # Add to hierarchy\n",
    "            if parent_id:\n",
    "                self.hierarchy[version][current_id] = parent_id\n",
    "        \n",
    "        # Process children\n",
    "        for child in node.children:\n",
    "            self._process_node(child, current_id, version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02e40886",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainFileDetector:\n",
    "    \"\"\"Automatically detect the main LaTeX file in a directory.\"\"\"\n",
    "    \n",
    "    # Common main file names\n",
    "    MAIN_FILE_CANDIDATES = [\n",
    "        'main.tex', 'paper.tex', 'article.tex', 'manuscript.tex',\n",
    "        'thesis.tex', 'document.tex', 'root.tex'\n",
    "    ]\n",
    "    \n",
    "    # Document class pattern to identify main files\n",
    "    DOCUMENT_CLASS_PATTERN = re.compile(r'\\\\documentclass', re.IGNORECASE)\n",
    "    BEGIN_DOCUMENT_PATTERN = re.compile(r'\\\\begin\\{document\\}', re.IGNORECASE)\n",
    "    AUTHOR_DOCUMENT_PATTERN = re.compile(r'\\\\author', re.IGNORECASE)\n",
    "    \n",
    "    @classmethod\n",
    "    def find_main_file(cls, tex_dir: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Find the main LaTeX file in a directory.\n",
    "        \n",
    "        Strategy:\n",
    "        1. Look for common main file names\n",
    "        2. Look for files with \\\\documentclass that also have \\\\begin{document}\n",
    "        3. If only one .tex file at root level, use that\n",
    "        \"\"\"\n",
    "        tex_path = Path(tex_dir)\n",
    "        \n",
    "        if not tex_path.exists():\n",
    "            return None\n",
    "        \n",
    "        # Strategy 1: Check common names\n",
    "        for candidate in cls.MAIN_FILE_CANDIDATES:\n",
    "            candidate_path = tex_path / candidate\n",
    "            if candidate_path.exists():\n",
    "                return candidate\n",
    "        \n",
    "        # Strategy 2: Find files with documentclass AND begin{document}\n",
    "        root_tex_files = list(tex_path.glob('*.tex'))\n",
    "        main_candidates = []\n",
    "        \n",
    "        for tex_file in root_tex_files:\n",
    "            try:\n",
    "                with open(tex_file, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                has_docclass = cls.DOCUMENT_CLASS_PATTERN.search(content)\n",
    "                has_begin_doc = cls.BEGIN_DOCUMENT_PATTERN.search(content)\n",
    "                has_author = cls.AUTHOR_DOCUMENT_PATTERN.search(content)\n",
    "                \n",
    "                if has_docclass and has_begin_doc and has_author:\n",
    "                    main_candidates.append(tex_file.name)\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        if len(main_candidates) == 1:\n",
    "            return main_candidates[0]\n",
    "        elif len(main_candidates) > 1:\n",
    "            # Prefer shorter filenames (less likely to be appendix, etc.)\n",
    "            return min(main_candidates, key=len)\n",
    "        \n",
    "        # Strategy 3: If only one tex file at root level\n",
    "        if len(root_tex_files) == 1:\n",
    "            return root_tex_files[0].name\n",
    "        \n",
    "        # Strategy 4: Look for any tex file with documentclass\n",
    "        for tex_file in root_tex_files:\n",
    "            try:\n",
    "                with open(tex_file, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                    content = f.read()\n",
    "                if cls.DOCUMENT_CLASS_PATTERN.search(content):\n",
    "                    return tex_file.name\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "class MultiVersionProcessor:\n",
    "    \"\"\"Process multiple versions of the same paper.\"\"\"\n",
    "    \n",
    "    def __init__(self, paper_dir: str):\n",
    "        self.paper_dir = Path(paper_dir)\n",
    "        self.arxiv_id = self.paper_dir.name\n",
    "        # self.tex_dir = self.paper_dir / 'tex'\n",
    "        \n",
    "        # Check if tex/ subdirectory exists, otherwise use paper_dir directly\n",
    "        tex_dir_candidate = self.paper_dir / 'tex'\n",
    "        self.tex_dir = tex_dir_candidate if tex_dir_candidate.exists() else self.paper_dir\n",
    "        \n",
    "        self.versions: Dict[str, Path] = {}\n",
    "        self.results: Dict[str, Any] = {}\n",
    "        \n",
    "    def discover_versions(self) -> List[str]:\n",
    "        \"\"\"Find all version directories (e.g., 2305-14596v1, 2305-14596v2).\"\"\"\n",
    "        self.versions.clear()\n",
    "        \n",
    "        if not self.tex_dir.exists():\n",
    "            return []\n",
    "        \n",
    "        # Look for version folders\n",
    "        version_pattern = re.compile(rf'{re.escape(self.arxiv_id)}v(\\d+)', re.IGNORECASE)\n",
    "        \n",
    "        for item in self.tex_dir.iterdir():\n",
    "            if item.is_dir():\n",
    "                match = version_pattern.match(item.name)\n",
    "                if match:\n",
    "                    version_num = match.group(1)\n",
    "                    self.versions[version_num] = item\n",
    "        \n",
    "        # Sort versions numerically\n",
    "        return sorted(self.versions.keys(), key=int)\n",
    "    \n",
    "    def parse_version(self, version: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Parse a specific version of the paper.\"\"\"\n",
    "        if version not in self.versions:\n",
    "            return None\n",
    "        \n",
    "        version_dir = self.versions[version]\n",
    "        main_file = MainFileDetector.find_main_file(str(version_dir))\n",
    "        \n",
    "        if not main_file:\n",
    "            print(f\"  Warning: Could not find main file in {version_dir}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            parser = LaTeXParser(str(version_dir))\n",
    "            result = parser.parse(main_file)\n",
    "            return {\n",
    "                'parser': parser,\n",
    "                'result': result,\n",
    "                'main_file': main_file\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"  Error parsing version {version}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_all_versions(self) -> Dict[str, Any]:\n",
    "        \"\"\"Process all versions and return combined results.\"\"\"\n",
    "        versions = self.discover_versions()\n",
    "        \n",
    "        if not versions:\n",
    "            print(f\"  No versions found for {self.arxiv_id}\")\n",
    "            return {}\n",
    "        \n",
    "        print(f\"  Found {len(versions)} version(s): {versions}\")\n",
    "        \n",
    "        for version in versions:\n",
    "            print(f\"\\n  Processing version {version}...\")\n",
    "            result = self.parse_version(version)\n",
    "            if result:\n",
    "                self.results[version] = result\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def export_combined(self, output_dir: str) -> Optional[Path]:\n",
    "        \"\"\"\n",
    "        Export combined results for all versions.\n",
    "        Elements are deduplicated across versions.\n",
    "        Each version has its own hierarchy.\n",
    "        References are deduplicated and merged across versions.\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            return None\n",
    "        \n",
    "        exporter = MilestoneExporter()\n",
    "        combined_elements = {}\n",
    "        combined_hierarchy = {}\n",
    "        \n",
    "        # Collect all references from all versions for cross-version deduplication\n",
    "        # Create copies to avoid mutating original parser references\n",
    "        all_references: Dict[str, BibEntry] = {}\n",
    "        for version, data in self.results.items():\n",
    "            parser = data['parser']\n",
    "            for key, entry in parser.references.items():\n",
    "                if key not in all_references:\n",
    "                    # Create a copy of the entry\n",
    "                    all_references[key] = BibEntry(\n",
    "                        key=entry.key,\n",
    "                        entry_type=entry.entry_type,\n",
    "                        fields=dict(entry.fields)\n",
    "                    )\n",
    "                else:\n",
    "                    # Merge fields from this version's entry (unionize)\n",
    "                    existing = all_references[key]\n",
    "                    for field, value in entry.fields.items():\n",
    "                        if field not in existing.fields or not existing.fields[field].strip():\n",
    "                            if value and value.strip():\n",
    "                                existing.fields[field] = value\n",
    "        \n",
    "        # Deduplicate references across all versions\n",
    "        # This handles entries with DIFFERENT keys but SAME content\n",
    "        deduplicator = Deduplicator()\n",
    "        original_count = len(all_references)\n",
    "        deduplicated_refs = deduplicator.deduplicate_references(all_references)\n",
    "        \n",
    "        if original_count > len(deduplicated_refs):\n",
    "            print(f\"  Cross-version reference deduplication: {original_count} -> {len(deduplicated_refs)} entries\")\n",
    "        \n",
    "        for version, data in self.results.items():\n",
    "            parser = data['parser']\n",
    "            if parser.hierarchy:\n",
    "                # Export this version\n",
    "                version_data = exporter.export_document(parser.hierarchy, version=version)\n",
    "                \n",
    "                # Merge elements (deduplicated by ID)\n",
    "                combined_elements.update(version_data['elements'])\n",
    "                \n",
    "                # Add version hierarchy\n",
    "                combined_hierarchy.update(version_data['hierarchy'])\n",
    "        \n",
    "        # Create output structure\n",
    "        output_data = {\n",
    "            'elements': combined_elements,\n",
    "            'hierarchy': combined_hierarchy\n",
    "        }\n",
    "        \n",
    "        # Create output directory\n",
    "        out_path = Path(output_dir) / self.arxiv_id\n",
    "        out_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save main JSON file\n",
    "        main_json = out_path / f\"{self.arxiv_id}.json\"\n",
    "        with open(main_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Export deduplicated BibTeX references (already merged across versions)\n",
    "        all_bibtex = {}\n",
    "        for key, entry in deduplicated_refs.items():\n",
    "            all_bibtex[key] = {\n",
    "                'type': entry.entry_type,\n",
    "                'fields': entry.fields\n",
    "            }\n",
    "        \n",
    "        if all_bibtex:\n",
    "            bibtex_json = out_path / 'extracted_bibtex.json'\n",
    "            with open(bibtex_json, 'w', encoding='utf-8') as f:\n",
    "                json.dump(all_bibtex, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01b4f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchProcessor:\n",
    "    \"\"\"\n",
    "    Process all papers in a folder.\n",
    "    \n",
    "    Expected folder structure:\n",
    "    folder/\n",
    "        2305-14596/\n",
    "            metadata.json\n",
    "            references.json\n",
    "            tex/\n",
    "                2305-14596v1/\n",
    "                    main.tex\n",
    "                    ...\n",
    "                2305-14596v2/\n",
    "                    main.tex\n",
    "                    ...\n",
    "        2305-14597/\n",
    "            ...\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, folder: str, output_folder: str = None):\n",
    "        self.folder = Path(folder)\n",
    "        self.output_folder = Path(output_folder) if output_folder else self.folder.parent / f\"{self.folder.name}_output\"\n",
    "        \n",
    "        self.papers: List[Path] = []\n",
    "        self.results: Dict[str, Any] = {}\n",
    "        self.stats = {\n",
    "            'total_papers': 0,\n",
    "            'processed': 0,\n",
    "            'failed': 0,\n",
    "            'skipped': 0,\n",
    "            'total_elements': 0,\n",
    "            'total_versions': 0\n",
    "        }\n",
    "    \n",
    "    def discover_papers(self) -> List[str]:\n",
    "        \"\"\"Find all paper directories in the student folder.\"\"\"\n",
    "        self.papers.clear()\n",
    "        \n",
    "        # Look for directories matching arXiv ID pattern\n",
    "        arxiv_pattern = re.compile(r'\\d{4}-\\d{4,5}')\n",
    "        \n",
    "        for item in sorted(self.folder.iterdir()):\n",
    "            if item.is_dir() and arxiv_pattern.match(item.name):\n",
    "                # Accept all paper folders (even without tex files)\n",
    "                # We want to preserve metadata.json and references.json for all papers\n",
    "                self.papers.append(item)\n",
    "        \n",
    "        self.stats['total_papers'] = len(self.papers)\n",
    "        return [p.name for p in self.papers]\n",
    "    \n",
    "    def process_paper(self, paper_dir: Path) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Process a single paper with all its versions.\"\"\"\n",
    "        processor = MultiVersionProcessor(str(paper_dir))\n",
    "        results = processor.process_all_versions()\n",
    "        \n",
    "        # Create output folder for this paper regardless of whether it has tex files\n",
    "        paper_output_path = self.output_folder / paper_dir.name\n",
    "        paper_output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Always copy metadata.json and references.json if they exist\n",
    "        metadata_src = paper_dir / 'metadata.json'\n",
    "        if metadata_src.exists():\n",
    "            shutil.copy(metadata_src, paper_output_path / 'metadata.json')\n",
    "        \n",
    "        references_src = paper_dir / 'references.json'\n",
    "        if references_src.exists():\n",
    "            shutil.copy(references_src, paper_output_path / 'references.json')\n",
    "        \n",
    "        if results:\n",
    "            output_path = processor.export_combined(str(self.output_folder))\n",
    "            \n",
    "            # Calculate stats\n",
    "            total_elements = 0\n",
    "            for version_data in results.values():\n",
    "                parser = version_data.get('parser')\n",
    "                if parser and parser.hierarchy:\n",
    "                    def count_nodes(node):\n",
    "                        count = 1\n",
    "                        for child in node.children:\n",
    "                            count += count_nodes(child)\n",
    "                        return count\n",
    "                    total_elements += count_nodes(parser.hierarchy)\n",
    "            \n",
    "            return {\n",
    "                'arxiv_id': paper_dir.name,\n",
    "                'versions_processed': len(results),\n",
    "                'output_path': output_path,\n",
    "                'total_elements': total_elements,\n",
    "                'has_tex': True\n",
    "            }\n",
    "        else:\n",
    "            # Paper has no tex files, but we still copied metadata/references\n",
    "            return {\n",
    "                'arxiv_id': paper_dir.name,\n",
    "                'versions_processed': 0,\n",
    "                'output_path': paper_output_path,\n",
    "                'total_elements': 0,\n",
    "                'has_tex': False\n",
    "            }\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def process_all(self, limit: int = None, verbose: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process all papers in the folder.\n",
    "        \n",
    "        Args:\n",
    "            limit: Maximum number of papers to process (for testing)\n",
    "            verbose: Whether to print progress\n",
    "        \"\"\"\n",
    "        papers = self.discover_papers()\n",
    "        \n",
    "        if not papers:\n",
    "            print(f\"No papers found in {self.folder}\")\n",
    "            return self.stats\n",
    "        \n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Found {len(papers)} papers to process\")\n",
    "        print(f\"Output folder: {self.output_folder}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # Create output folder\n",
    "        self.output_folder.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        papers_to_process = self.papers[:limit] if limit else self.papers\n",
    "        \n",
    "        for i, paper_dir in enumerate(papers_to_process):\n",
    "            arxiv_id = paper_dir.name\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\n[{i+1}/{len(papers_to_process)}] Processing {arxiv_id}...\")\n",
    "                print(\"-\" * 40)\n",
    "            \n",
    "            try:\n",
    "                result = self.process_paper(paper_dir)\n",
    "                \n",
    "                if result:\n",
    "                    self.results[arxiv_id] = result\n",
    "                    self.stats['processed'] += 1\n",
    "                    \n",
    "                    if result.get('has_tex', True):\n",
    "                        self.stats['total_versions'] += result['versions_processed']\n",
    "                        self.stats['total_elements'] += result['total_elements']\n",
    "                        \n",
    "                        if verbose:\n",
    "                            print(f\"  Successfully processed {result['versions_processed']} version(s)\")\n",
    "                            print(f\"  Elements: {result['total_elements']}\")\n",
    "                    else:\n",
    "                        if verbose:\n",
    "                            print(f\"  Folder created with metadata/references (no .tex files)\")\n",
    "                else:\n",
    "                    self.stats['skipped'] += 1\n",
    "                    if verbose:\n",
    "                        print(f\"  Skipped (no versions found or parse failed)\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                self.stats['failed'] += 1\n",
    "                if verbose:\n",
    "                    print(f\"  Failed: {str(e)}\")\n",
    "        \n",
    "        # Save processing summary\n",
    "        summary_file = self.output_folder / 'processing_summary.json'\n",
    "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'stats': self.stats,\n",
    "                'papers': {\n",
    "                    arxiv_id: {\n",
    "                        'versions_processed': r['versions_processed'],\n",
    "                        'total_elements': r['total_elements']\n",
    "                    }\n",
    "                    for arxiv_id, r in self.results.items()\n",
    "                }\n",
    "            }, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Summary saved to: {summary_file}\")\n",
    "        \n",
    "        return self.stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5ab0b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15000 papers in 23127238\n",
      "============================================================\n",
      "Found 15000 papers to process\n",
      "Output folder: 23127238_output\n",
      "============================================================\n",
      "\n",
      "\n",
      "[1/5] Processing 2304-14607...\n",
      "----------------------------------------\n",
      "  Found 1 version(s): ['1']\n",
      "\n",
      "  Processing version 1...\n",
      "  Warning: Could not find main file in 23127238\\2304-14607\\tex\\2304-14607v1\n",
      "  Folder created with metadata/references (no .tex files)\n",
      "\n",
      "[2/5] Processing 2304-14608...\n",
      "----------------------------------------\n",
      "  Found 2 version(s): ['1', '2']\n",
      "\n",
      "  Processing version 1...\n",
      "Parsing LaTeX document from: 23127238\\2304-14608\\tex\\2304-14608v1\\light_source_2023.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 236 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 195\n",
      "      - section: 7\n",
      "      - figure: 16\n",
      "      - subsection: 10\n",
      "      - table: 1\n",
      "      - subsubsection: 4\n",
      "      - acknowledgments: 1\n",
      "\n",
      "[3] Extracting references...\n",
      "Loaded 18 entries from lightsourcebib.bib\n",
      "    Found 18 bibliography entries\n",
      "\n",
      "[4] Deduplicating references...\n",
      "    Removed 0 duplicate references\n",
      "\n",
      "[5] Deduplicating content...\n",
      "    Unique content items: 212\n",
      "    Duplicate content items: 0\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "\n",
      "  Processing version 2...\n",
      "Parsing LaTeX document from: 23127238\\2304-14608\\tex\\2304-14608v2\\light_source_2023_final.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 246 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 205\n",
      "      - section: 7\n",
      "      - figure: 16\n",
      "      - subsection: 10\n",
      "      - table: 1\n",
      "      - subsubsection: 4\n",
      "      - acknowledgments: 1\n",
      "\n",
      "[3] Extracting references...\n",
      "Loaded 18 entries from light_source_2023.bib\n",
      "    Found 18 bibliography entries\n",
      "\n",
      "[4] Deduplicating references...\n",
      "    Removed 0 duplicate references\n",
      "\n",
      "[5] Deduplicating content...\n",
      "    Unique content items: 222\n",
      "    Duplicate content items: 0\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "  Successfully processed 2 version(s)\n",
      "  Elements: 482\n",
      "\n",
      "[3/5] Processing 2304-14609...\n",
      "----------------------------------------\n",
      "  Found 3 version(s): ['1', '2', '3']\n",
      "\n",
      "  Processing version 1...\n",
      "Parsing LaTeX document from: 23127238\\2304-14609\\tex\\2304-14609v1\\arxiv1.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 313 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - section: 11\n",
      "      - block_formula: 13\n",
      "      - figure: 9\n",
      "      - sentence: 273\n",
      "      - acknowledgments: 1\n",
      "      - subsection: 4\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 0 bibliography entries\n",
      "\n",
      "[4] Deduplicating references...\n",
      "    Removed 0 duplicate references\n",
      "\n",
      "[5] Deduplicating content...\n",
      "    Unique content items: 293\n",
      "    Duplicate content items: 2\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "\n",
      "  Processing version 2...\n",
      "Parsing LaTeX document from: 23127238\\2304-14609\\tex\\2304-14609v2\\manuscript.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 351 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 305\n",
      "      - section: 11\n",
      "      - block_formula: 17\n",
      "      - figure: 11\n",
      "      - table: 1\n",
      "      - subsection: 4\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 0 bibliography entries\n",
      "\n",
      "[4] Deduplicating references...\n",
      "    Removed 0 duplicate references\n",
      "\n",
      "[5] Deduplicating content...\n",
      "    Unique content items: 334\n",
      "    Duplicate content items: 0\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "\n",
      "  Processing version 3...\n",
      "Parsing LaTeX document from: 23127238\\2304-14609\\tex\\2304-14609v3\\main.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 371 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 319\n",
      "      - section: 9\n",
      "      - block_formula: 18\n",
      "      - table: 1\n",
      "      - figure: 14\n",
      "      - subsection: 7\n",
      "      - acknowledgments: 1\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 0 bibliography entries\n",
      "\n",
      "[4] Deduplicating references...\n",
      "    Removed 0 duplicate references\n",
      "\n",
      "[5] Deduplicating content...\n",
      "    Unique content items: 311\n",
      "    Duplicate content items: 41\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "  Successfully processed 3 version(s)\n",
      "  Elements: 1035\n",
      "\n",
      "[4/5] Processing 2304-14610...\n",
      "----------------------------------------\n",
      "  Found 2 version(s): ['1', '2']\n",
      "\n",
      "  Processing version 1...\n",
      "Parsing LaTeX document from: 23127238\\2304-14610\\tex\\2304-14610v1\\ijcai23.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 268 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 209\n",
      "      - section: 12\n",
      "      - figure: 13\n",
      "      - subsubsection: 6\n",
      "      - subsection: 7\n",
      "      - block_formula: 11\n",
      "      - table: 7\n",
      "      - acknowledgments: 1\n",
      "\n",
      "[3] Extracting references...\n",
      "Loaded 79 entries from ijcai23.bib\n",
      "    Found 79 bibliography entries\n",
      "\n",
      "[4] Deduplicating references...\n",
      "Merged duplicate: fu2016weighted -> Fu_2016_CVPR\n",
      "    Removed 1 duplicate references\n",
      "\n",
      "[5] Deduplicating content...\n",
      "    Unique content items: 240\n",
      "    Duplicate content items: 0\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "\n",
      "  Processing version 2...\n",
      "Parsing LaTeX document from: 23127238\\2304-14610\\tex\\2304-14610v2\\ijcai23.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 268 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 209\n",
      "      - section: 12\n",
      "      - figure: 13\n",
      "      - subsubsection: 6\n",
      "      - subsection: 7\n",
      "      - block_formula: 11\n",
      "      - table: 7\n",
      "      - acknowledgments: 1\n",
      "\n",
      "[3] Extracting references...\n",
      "Loaded 79 entries from ijcai23.bib\n",
      "    Found 79 bibliography entries\n",
      "\n",
      "[4] Deduplicating references...\n",
      "Merged duplicate: fu2016weighted -> Fu_2016_CVPR\n",
      "    Removed 1 duplicate references\n",
      "\n",
      "[5] Deduplicating content...\n",
      "    Unique content items: 240\n",
      "    Duplicate content items: 0\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "  Successfully processed 2 version(s)\n",
      "  Elements: 536\n",
      "\n",
      "[5/5] Processing 2304-14611...\n",
      "----------------------------------------\n",
      "  Found 1 version(s): ['1']\n",
      "\n",
      "  Processing version 1...\n",
      "Parsing LaTeX document from: 23127238\\2304-14611\\tex\\2304-14611v1\\main.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 164 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 127\n",
      "      - section: 5\n",
      "      - subsection: 7\n",
      "      - block_formula: 19\n",
      "      - figure: 3\n",
      "      - table: 1\n",
      "\n",
      "[3] Extracting references...\n",
      "Loaded 33 entries from ref.bib\n",
      "    Found 33 bibliography entries\n",
      "\n",
      "[4] Deduplicating references...\n",
      "    Removed 0 duplicate references\n",
      "    Found 1 pairs of similar references:\n",
      "      - 'ye2022optimal' ~ 'wu2022communication' (similarity: 71.17%)\n",
      "\n",
      "[5] Deduplicating content...\n",
      "    Unique content items: 150\n",
      "    Duplicate content items: 0\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "  Successfully processed 1 version(s)\n",
      "  Elements: 164\n",
      "Summary saved to: 23127238_output\\processing_summary.json\n"
     ]
    }
   ],
   "source": [
    "# Test with a few papers first\n",
    "FOLDER = \"23127238\"\n",
    "OUTPUT_FOLDER = \"23127238_output\"\n",
    "\n",
    "# Initialize batch processor\n",
    "batch_processor = BatchProcessor(FOLDER, OUTPUT_FOLDER)\n",
    "\n",
    "# Discover all papers\n",
    "papers = batch_processor.discover_papers()\n",
    "print(f\"Found {len(papers)} papers in {FOLDER}\")\n",
    "\n",
    "stats = batch_processor.process_all(limit=5, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
