{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1dab0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Dict, Any, Set, Tuple\n",
    "from enum import Enum\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87fb41ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeType(Enum):\n",
    "    \"\"\"Types of nodes in the document hierarchy\"\"\"\n",
    "    DOCUMENT = \"document\"\n",
    "    CHAPTER = \"chapter\"\n",
    "    SECTION = \"section\"\n",
    "    SUBSECTION = \"subsection\"\n",
    "    SUBSUBSECTION = \"subsubsection\"\n",
    "    PARAGRAPH = \"paragraph\"\n",
    "    SUBPARAGRAPH = \"subparagraph\"\n",
    "    # Leaf nodes\n",
    "    SENTENCE = \"sentence\"\n",
    "    BLOCK_FORMULA = \"block_formula\"\n",
    "    FIGURE = \"figure\"\n",
    "    TABLE = \"table\"\n",
    "    # Special\n",
    "    ABSTRACT = \"abstract\"\n",
    "    ACKNOWLEDGMENTS = \"acknowledgments\"\n",
    "    APPENDIX = \"appendix\"\n",
    "\n",
    "\n",
    "# Hierarchy order for determining nesting levels\n",
    "HIERARCHY_ORDER = [\n",
    "    NodeType.DOCUMENT,\n",
    "    NodeType.CHAPTER,\n",
    "    NodeType.SECTION,\n",
    "    NodeType.SUBSECTION,\n",
    "    NodeType.SUBSUBSECTION,\n",
    "    NodeType.PARAGRAPH,\n",
    "    NodeType.SUBPARAGRAPH,\n",
    "]\n",
    "\n",
    "# Leaf node types (smallest elements)\n",
    "LEAF_TYPES = {NodeType.SENTENCE, NodeType.BLOCK_FORMULA, NodeType.FIGURE, NodeType.TABLE}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HierarchyNode:\n",
    "    \"\"\"A node in the document hierarchy tree\"\"\"\n",
    "    node_type: NodeType\n",
    "    title: str = \"\"\n",
    "    content: str = \"\"\n",
    "    children: List['HierarchyNode'] = field(default_factory=list)\n",
    "    label: str = \"\"\n",
    "    source_file: str = \"\"\n",
    "    content_hash: str = \"\"\n",
    "    unique_id: str = \"\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if not self.unique_id:\n",
    "            self.unique_id = hashlib.md5(\n",
    "                f\"{self.node_type.value}:{self.title}:{self.content[:100]}\".encode()\n",
    "            ).hexdigest()[:12]\n",
    "        if not self.content_hash and self.content:\n",
    "            self.content_hash = hashlib.md5(self.content.encode()).hexdigest()\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert node to dictionary for JSON serialization\"\"\"\n",
    "        return {\n",
    "            \"type\": self.node_type.value,\n",
    "            \"title\": self.title,\n",
    "            \"content\": self.content if self.node_type in LEAF_TYPES else \"\",\n",
    "            \"label\": self.label,\n",
    "            \"source_file\": self.source_file,\n",
    "            \"content_hash\": self.content_hash,\n",
    "            \"unique_id\": self.unique_id,\n",
    "            \"children\": [child.to_dict() for child in self.children]\n",
    "        }\n",
    "\n",
    "\n",
    "@dataclass \n",
    "class BibEntry:\n",
    "    \"\"\"A bibliography entry\"\"\"\n",
    "    key: str\n",
    "    entry_type: str\n",
    "    fields: Dict[str, str] = field(default_factory=dict)\n",
    "    \n",
    "    def to_bibtex(self) -> str:\n",
    "        \"\"\"Convert to BibTeX format\"\"\"\n",
    "        lines = [f\"@{self.entry_type}{{{self.key},\"]\n",
    "        for key, value in self.fields.items():\n",
    "            lines.append(f\"  {key} = {{{value}}},\")\n",
    "        lines.append(\"}\")\n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    def content_hash(self) -> str:\n",
    "        \"\"\"Generate hash based on content for deduplication\"\"\"\n",
    "        # Normalize fields for comparison\n",
    "        normalized = {k.lower(): v.lower().strip() for k, v in self.fields.items()}\n",
    "        content = json.dumps(normalized, sort_keys=True)\n",
    "        return hashlib.md5(content.encode()).hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fbdf1a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaTeXFileGatherer:\n",
    "    # Patterns to match \\input and \\include commands\n",
    "    PATTERN = re.compile(r'(?m)^(?![^%\\n]*%).*\\\\(?:input|include)\\{([^}]+)\\}')\n",
    "    \n",
    "    def __init__(self, base_dir: str):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.included_files: Set[str] = set()\n",
    "        self.file_contents: Dict[str, str] = {}\n",
    "        self.file_order: List[str] = []\n",
    "        \n",
    "    def gather_files(self, main_file: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Recursively gather all files starting from the main file.\n",
    "        Returns a dict mapping file paths to their contents.\n",
    "        \"\"\"\n",
    "        self.included_files.clear()\n",
    "        self.file_contents.clear()\n",
    "        self.file_order.clear()\n",
    "        \n",
    "        main_path = self.base_dir / main_file\n",
    "        self._process_file(main_path)\n",
    "        \n",
    "        return self.file_contents\n",
    "    \n",
    "    def _resolve_path(self, include_path: str, current_file: Path) -> Path:\n",
    "        \"\"\"Resolve the path of an included file\"\"\"\n",
    "        # Add .tex extension if not present\n",
    "        if not include_path.endswith('.tex'):\n",
    "            include_path += '.tex'\n",
    "        \n",
    "        # Try relative to current file first\n",
    "        relative_path = current_file.parent / include_path\n",
    "        if relative_path.exists():\n",
    "            return relative_path\n",
    "        \n",
    "        # Try relative to base directory\n",
    "        base_relative = self.base_dir / include_path\n",
    "        if base_relative.exists():\n",
    "            return base_relative\n",
    "        \n",
    "        return relative_path  # Return even if doesn't exist for error reporting\n",
    "    \n",
    "    def _process_file(self, file_path: Path) -> str:\n",
    "        \"\"\"Process a single file and recursively process includes\"\"\"\n",
    "        # Normalize path for tracking\n",
    "        normalized_path = str(file_path.resolve())\n",
    "        \n",
    "        if normalized_path in self.included_files:\n",
    "            return \"\"  # Already processed\n",
    "        \n",
    "        if not file_path.exists():\n",
    "            print(f\"Warning: File not found: {file_path}\")\n",
    "            return \"\"\n",
    "        \n",
    "        self.included_files.add(normalized_path)\n",
    "        self.file_order.append(normalized_path)\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                content = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            return \"\"\n",
    "        \n",
    "        # Store original content\n",
    "        self.file_contents[normalized_path] = content\n",
    "        \n",
    "        # Find and process all includes\n",
    "        expanded_content = self._expand_includes(content, file_path)\n",
    "        \n",
    "        return expanded_content\n",
    "    \n",
    "    def _expand_includes(self, content: str, current_file: Path) -> str:\n",
    "        \"\"\"Expand \\input and \\include commands in content\"\"\"\n",
    "        result = content\n",
    "        \n",
    "        # Process \\input commands\n",
    "        for match in self.PATTERN.finditer(content):\n",
    "            include_path = match.group(1)\n",
    "            resolved_path = self._resolve_path(include_path, current_file)\n",
    "            included_content = self._process_file(resolved_path)\n",
    "        \n",
    "        # Process \\include commands  \n",
    "        for match in self.PATTERN.finditer(content):\n",
    "            include_path = match.group(1)\n",
    "            resolved_path = self._resolve_path(include_path, current_file)\n",
    "            included_content = self._process_file(resolved_path)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_merged_content(self) -> str:\n",
    "        \"\"\"Get all content merged into a single string with file markers\"\"\"\n",
    "        merged = []\n",
    "        for file_path in self.file_order:\n",
    "            content = self.file_contents.get(file_path, \"\")\n",
    "            # Add file marker for tracking\n",
    "            merged.append(f\"%%% FILE: {file_path} %%%\\n\")\n",
    "            merged.append(content)\n",
    "            merged.append(\"\\n\")\n",
    "        return \"\\n\".join(merged)\n",
    "    \n",
    "    def get_unused_files(self) -> Set[str]:\n",
    "        \"\"\"Get files that exist but are not part of compilation\"\"\"\n",
    "        all_files = set()\n",
    "        for tex_file in self.base_dir.rglob(\"*.tex\"):\n",
    "            all_files.add(str(tex_file.resolve()))\n",
    "        return all_files - self.included_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39fb3ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaTeXCleaner:\n",
    "    \"\"\"Cleans and normalizes LaTeX content\"\"\"\n",
    "    \n",
    "    # Commands to remove (no semantic meaning)\n",
    "    REMOVE_COMMANDS = [\n",
    "        r'\\\\centering',\n",
    "        r'\\\\raggedright',\n",
    "        r'\\\\raggedleft',\n",
    "        r'\\\\noindent',\n",
    "        r'\\\\smallskip',\n",
    "        r'\\\\medskip',\n",
    "        r'\\\\bigskip',\n",
    "        r'\\\\newpage',\n",
    "        r'\\\\clearpage',\n",
    "        r'\\\\pagebreak',\n",
    "        r'\\\\linebreak',\n",
    "        r'\\\\hfill',\n",
    "        r'\\\\vfill',\n",
    "        r'\\\\hspace\\{[^}]*\\}',\n",
    "        r'\\\\vspace\\{[^}]*\\}',\n",
    "        r'\\\\phantom\\{[^}]*\\}',\n",
    "        r'\\\\hphantom\\{[^}]*\\}',\n",
    "        r'\\\\vphantom\\{[^}]*\\}',\n",
    "\n",
    "        r'\\\\par',\n",
    "        r'\\\\parindent',\n",
    "        r'\\\\parskip',\n",
    "        r'\\\\baselineskip',\n",
    "        r'\\\\stretch\\{[^}]*\\}',\n",
    "\n",
    "        # Font / style\n",
    "        r'\\\\textbf\\{([^}]*)\\}',\n",
    "        r'\\\\textit\\{([^}]*)\\}',\n",
    "        r'\\\\emph\\{([^}]*)\\}',\n",
    "        r'\\\\underline\\{([^}]*)\\}',\n",
    "        r'\\\\texttt\\{([^}]*)\\}',\n",
    "        r'\\\\bfseries',\n",
    "        r'\\\\itshape',\n",
    "        r'\\\\ttfamily',\n",
    "\n",
    "        r'\\\\item',\n",
    "        r'\\\\item\\[[^\\]]*\\]',\n",
    "\n",
    "    ]\n",
    "    \n",
    "    # Table formatting commands to remove\n",
    "    TABLE_FORMATTING = [\n",
    "        r'\\\\toprule',\n",
    "        r'\\\\midrule',\n",
    "        r'\\\\bottomrule',\n",
    "        r'\\\\hline',\n",
    "        r'\\\\cline\\{[^}]*\\}',\n",
    "        r'\\\\multicolumn\\{[^}]*\\}\\{[^}]*\\}',\n",
    "        r'\\\\multirow\\{[^}]*\\}\\{[^}]*\\}',\n",
    "        \n",
    "        r'\\\\addlinespace',\n",
    "        r'\\\\cmidrule\\{[^}]*\\}', \n",
    "    ]\n",
    "    \n",
    "    # Figure/table placement specifiers\n",
    "    PLACEMENT_SPECS = re.compile(r'\\[([htbp!]+)\\]')\n",
    "    \n",
    "    # Inline math patterns (to normalize to $...$)\n",
    "    INLINE_MATH_PATTERNS = [\n",
    "        (re.compile(r'\\\\[(](.*?)\\\\[)]', re.DOTALL), r'$\\1$'),  # \\(...\\)\n",
    "        (re.compile(r'\\\\begin\\{math\\}(.*?)\\\\end\\{math\\}', re.DOTALL), r'$\\1$'),\n",
    "    ]\n",
    "    \n",
    "    # Block math environments (to normalize to equation)\n",
    "    BLOCK_MATH_ENVS = [\n",
    "        'displaymath', 'eqnarray', 'eqnarray*', 'align', 'align*',\n",
    "        'gather', 'gather*', 'multline', 'multline*', 'flalign', 'flalign*'\n",
    "    ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Compile removal patterns\n",
    "        self.remove_patterns = [re.compile(p) for p in self.REMOVE_COMMANDS + self.TABLE_FORMATTING]\n",
    "    \n",
    "    def clean(self, content: str) -> str:\n",
    "        \"\"\"Apply all cleaning operations\"\"\"\n",
    "        result = content\n",
    "        \n",
    "        # Remove comments (but preserve % in math mode)\n",
    "        result = self._remove_comments(result)\n",
    "        result = self.normalize_math(result)\n",
    "        \n",
    "        # Remove formatting commands\n",
    "        for pattern in self.remove_patterns:\n",
    "            result = pattern.sub('', result)\n",
    "        \n",
    "        # Remove placement specifiers\n",
    "        result = self.PLACEMENT_SPECS.sub('', result)\n",
    "        \n",
    "        # Normalize whitespace\n",
    "        result = self._normalize_whitespace(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def normalize_math(self, content: str) -> str:\n",
    "        \"\"\"Normalize all math expressions\"\"\"\n",
    "        result = content\n",
    "        \n",
    "        # Normalize inline math to $...$\n",
    "        for pattern, replacement in self.INLINE_MATH_PATTERNS:\n",
    "            result = pattern.sub(replacement, result)\n",
    "        \n",
    "        # Normalize $$...$$ to equation environment\n",
    "        result = re.sub(\n",
    "            r'\\$\\$(.*?)\\$\\$',\n",
    "            r'\\\\begin{equation}\\1\\\\end{equation}',\n",
    "            result,\n",
    "            flags=re.DOTALL\n",
    "        )\n",
    "        \n",
    "        # Normalize other block math environments to equation\n",
    "        for env in self.BLOCK_MATH_ENVS:\n",
    "            pattern = re.compile(\n",
    "                rf'\\\\begin\\{{{env}\\}}(.*?)\\\\end\\{{{env}\\}}',\n",
    "                re.DOTALL\n",
    "            )\n",
    "            result = pattern.sub(r'\\\\begin{equation}\\1\\\\end{equation}', result)\n",
    "\n",
    "        # result = re.sub(r'~\\\\mathrm\\{([^}]*)\\}', r'\\1', result)\n",
    "        # result = re.sub(r'~', '', result)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def _remove_comments(self, content: str) -> str:\n",
    "        \"\"\"Remove LaTeX comments while preserving escaped %\"\"\"\n",
    "        lines = content.split('\\n')\n",
    "        result_lines = []\n",
    "        for line in lines:\n",
    "            # Find % that are not escaped\n",
    "            new_line = []\n",
    "            i = 0\n",
    "            while i < len(line):\n",
    "                if line[i] == '%' and (i == 0 or line[i-1] != '\\\\'):\n",
    "                    break  # Rest of line is comment\n",
    "                new_line.append(line[i])\n",
    "                i += 1\n",
    "            result_lines.append(''.join(new_line))\n",
    "        return '\\n'.join(result_lines)\n",
    "    \n",
    "    def _normalize_whitespace(self, content: str) -> str:\n",
    "        \"\"\"Normalize whitespace in content\"\"\"\n",
    "        # Replace multiple spaces with single space\n",
    "        result = re.sub(r'[ \\t]+', ' ', content)\n",
    "        # Replace multiple newlines with double newline\n",
    "        result = re.sub(r'\\n{3,}', '\\n\\n', result)\n",
    "        return result.strip()\n",
    "    \n",
    "    def extract_text_content(self, content: str) -> str:\n",
    "        \"\"\"Extract plain text from LaTeX, removing commands\"\"\"\n",
    "        result = content\n",
    "        \n",
    "        # Remove \\command{...} but keep the content in braces\n",
    "        result = re.sub(r'\\\\(?:textbf|textit|textrm|texttt|emph|underline)\\{([^}]*)\\}', r'\\1', result)\n",
    "        \n",
    "        # Remove \\command without braces\n",
    "        result = re.sub(r'\\\\(?:bf|it|rm|tt|em|sc)\\b', '', result)\n",
    "        \n",
    "        # Remove citations and references (keep for now, mark them)\n",
    "        result = re.sub(r'\\\\cite[pt]?\\{[^}]*\\}', '[CITATION]', result)\n",
    "        result = re.sub(r'\\\\ref\\{[^}]*\\}', '[REF]', result)\n",
    "        \n",
    "        # Remove labels\n",
    "        result = re.sub(r'\\\\label\\{[^}]*\\}', '', result)\n",
    "        \n",
    "        # Remove remaining simple commands\n",
    "        result = re.sub(r'\\\\[a-zA-Z]+\\*?(?:\\[[^\\]]*\\])?(?:\\{[^}]*\\})?', '', result)\n",
    "        \n",
    "        # Clean up braces\n",
    "        result = re.sub(r'[{}]', '', result)\n",
    "        \n",
    "        return result.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2a1052d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchyParser:\n",
    "    \"\"\"Parses LaTeX content into a hierarchical structure\"\"\"\n",
    "    \n",
    "    # Section command patterns with their hierarchy levels\n",
    "    SECTION_PATTERNS = [\n",
    "        (r'\\\\chapter\\*?\\{([^}]*)\\}', NodeType.CHAPTER),\n",
    "        (r'\\\\section\\*?\\{([^}]*)\\}', NodeType.SECTION),\n",
    "        (r'\\\\subsection\\*?\\{([^}]*)\\}', NodeType.SUBSECTION),\n",
    "        (r'\\\\subsubsection\\*?\\{([^}]*)\\}', NodeType.SUBSUBSECTION),\n",
    "        (r'\\\\paragraph\\*?\\{([^}]*)\\}', NodeType.PARAGRAPH),\n",
    "        (r'\\\\subparagraph\\*?\\{([^}]*)\\}', NodeType.SUBPARAGRAPH),\n",
    "    ]\n",
    "    \n",
    "    # Special sections to detect\n",
    "    SPECIAL_PATTERNS = [\n",
    "        (r'\\\\begin\\{abstract\\}(.*?)\\\\end\\{abstract\\}', NodeType.ABSTRACT),\n",
    "        (r'\\\\chapter\\*?\\{Acknowledgments?\\}', NodeType.ACKNOWLEDGMENTS),\n",
    "        (r'\\\\section\\*?\\{Acknowledgments?\\}', NodeType.ACKNOWLEDGMENTS),\n",
    "        (r'\\\\appendix', NodeType.APPENDIX),\n",
    "    ]\n",
    "    \n",
    "    # Block math environments\n",
    "    BLOCK_MATH_ENVS = [\n",
    "        'equation', 'equation*', 'align', 'align*', 'gather', 'gather*',\n",
    "        'multline', 'multline*', 'eqnarray', 'eqnarray*', 'displaymath'\n",
    "    ]\n",
    "    \n",
    "    # Figure/table environments\n",
    "    FLOAT_ENVS = ['figure', 'figure*', 'table', 'table*']\n",
    "    \n",
    "    # References section patterns (to exclude)\n",
    "    REFERENCES_PATTERNS = [\n",
    "        r'\\\\begin\\{thebibliography\\}',\n",
    "        r'\\\\bibliography\\{',\n",
    "        r'\\\\printbibliography',\n",
    "        r'\\\\section\\*?\\{References\\}',\n",
    "        r'\\\\section\\*?\\{Bibliography\\}',\n",
    "        r'\\\\chapter\\*?\\{References\\}',\n",
    "        r'\\\\chapter\\*?\\{Bibliography\\}',\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, cleaner: LaTeXCleaner = None):\n",
    "        self.cleaner = cleaner or LaTeXCleaner()\n",
    "        self._compile_patterns()\n",
    "        \n",
    "    def _compile_patterns(self):\n",
    "        \"\"\"Compile regex patterns for efficiency\"\"\"\n",
    "        self.section_patterns = [\n",
    "            (re.compile(pattern, re.DOTALL), node_type)\n",
    "            for pattern, node_type in self.SECTION_PATTERNS\n",
    "        ]\n",
    "        self.special_patterns = [\n",
    "            (re.compile(pattern, re.DOTALL | re.IGNORECASE), node_type)\n",
    "            for pattern, node_type in self.SPECIAL_PATTERNS\n",
    "        ]\n",
    "        self.references_pattern = re.compile(\n",
    "            '|'.join(self.REFERENCES_PATTERNS), re.IGNORECASE\n",
    "        )\n",
    "        \n",
    "        # Block math pattern\n",
    "        math_env_pattern = '|'.join(re.escape(env) for env in self.BLOCK_MATH_ENVS)\n",
    "        self.block_math_pattern = re.compile(\n",
    "            rf'\\\\begin\\{{({math_env_pattern})\\}}(.*?)\\\\end\\{{\\1\\}}|'\n",
    "            rf'\\$\\$(.*?)\\$\\$',\n",
    "            re.DOTALL\n",
    "        )\n",
    "        \n",
    "        # Figure/table pattern\n",
    "        float_env_pattern = '|'.join(re.escape(env) for env in self.FLOAT_ENVS)\n",
    "        self.float_pattern = re.compile(\n",
    "            rf'\\\\begin\\{{({float_env_pattern})\\}}(.*?)\\\\end\\{{\\1\\}}',\n",
    "            re.DOTALL\n",
    "        )\n",
    "        \n",
    "        # Label pattern\n",
    "        self.label_pattern = re.compile(r'\\\\label\\{([^}]*)\\}')\n",
    "        \n",
    "        # Caption pattern\n",
    "        self.caption_pattern = re.compile(r'\\\\caption\\{([^}]*)\\}')\n",
    "    \n",
    "    def _is_references_section(self, content: str) -> bool:\n",
    "        \"\"\"Check if content is a references/bibliography section\"\"\"\n",
    "        return bool(self.references_pattern.search(content))\n",
    "    \n",
    "    def _get_hierarchy_level(self, node_type: NodeType) -> int:\n",
    "        \"\"\"Get the hierarchy level of a node type\"\"\"\n",
    "        try:\n",
    "            return HIERARCHY_ORDER.index(node_type)\n",
    "        except ValueError:\n",
    "            return len(HIERARCHY_ORDER)  # Leaf nodes\n",
    "    \n",
    "    def _extract_sections(self, content: str, source_file: str = \"\") -> List[Tuple[int, NodeType, str, str]]:\n",
    "        \"\"\"\n",
    "        Extract all section markers from content.\n",
    "        Returns list of (position, node_type, title, full_match)\n",
    "        \"\"\"\n",
    "        sections = []\n",
    "        \n",
    "        # Find all section commands\n",
    "        for pattern, node_type in self.section_patterns:\n",
    "            for match in pattern.finditer(content):\n",
    "                title = match.group(1).strip()\n",
    "                sections.append((match.start(), node_type, title, match.group(0)))\n",
    "        \n",
    "        # Sort by position\n",
    "        sections.sort(key=lambda x: x[0])\n",
    "        return sections\n",
    "    \n",
    "    def _extract_block_formulas(self, content: str) -> List[Tuple[int, int, str]]:\n",
    "        \"\"\"Extract block formulas with their positions\"\"\"\n",
    "        formulas = []\n",
    "        for match in self.block_math_pattern.finditer(content):\n",
    "            formula_content = match.group(2) if match.group(2) else match.group(3)\n",
    "            formulas.append((match.start(), match.end(), formula_content.strip()))\n",
    "        return formulas\n",
    "    \n",
    "    def _extract_floats(self, content: str) -> List[Tuple[int, int, str, str, str]]:\n",
    "        \"\"\"Extract figures and tables with their positions\"\"\"\n",
    "        floats = []\n",
    "        for match in self.float_pattern.finditer(content):\n",
    "            env_type = match.group(1)\n",
    "            env_content = match.group(2)\n",
    "            \n",
    "            # Extract label\n",
    "            label_match = self.label_pattern.search(env_content)\n",
    "            label = label_match.group(1) if label_match else \"\"\n",
    "            \n",
    "            # Extract caption\n",
    "            caption_match = self.caption_pattern.search(env_content)\n",
    "            caption = caption_match.group(1) if caption_match else \"\"\n",
    "            \n",
    "            node_type = NodeType.FIGURE if 'figure' in env_type else NodeType.TABLE\n",
    "            floats.append((match.start(), match.end(), env_content, label, caption, node_type))\n",
    "        \n",
    "        return floats\n",
    "    \n",
    "    def _split_into_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into sentences\"\"\"\n",
    "        # Clean the text first\n",
    "        cleaned = self.cleaner.extract_text_content(text)\n",
    "        \n",
    "        # Split on sentence-ending punctuation\n",
    "        # Handle common abbreviations\n",
    "        abbrev_pattern = r'(?<![A-Z])[.!?](?=\\s+[A-Z]|\\s*$)'\n",
    "        \n",
    "        # Simple sentence splitting (handles most cases)\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', cleaned)\n",
    "        \n",
    "        # Filter empty sentences and strip whitespace\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        return sentences\n",
    "    \n",
    "    def _extract_content_between(self, content: str, start: int, end: int) -> str:\n",
    "        \"\"\"Extract content between two positions, excluding formulas and floats\"\"\"\n",
    "        return content[start:end]\n",
    "    \n",
    "    def _create_leaf_nodes(self, content: str, source_file: str = \"\") -> List[HierarchyNode]:\n",
    "        \"\"\"Create leaf nodes (sentences, formulas, figures) from content\"\"\"\n",
    "        nodes = []\n",
    "        \n",
    "        # Check if this is a references section\n",
    "        if self._is_references_section(content):\n",
    "            return nodes\n",
    "        \n",
    "        # Extract formulas and floats first\n",
    "        formulas = self._extract_block_formulas(content)\n",
    "        floats = self._extract_floats(content)\n",
    "        \n",
    "        # Mark positions of formulas and floats\n",
    "        excluded_ranges = []\n",
    "        \n",
    "        for start, end, formula_content in formulas:\n",
    "            excluded_ranges.append((start, end))\n",
    "            nodes.append(HierarchyNode(\n",
    "                node_type=NodeType.BLOCK_FORMULA,\n",
    "                content=formula_content,\n",
    "                source_file=source_file\n",
    "            ))\n",
    "        \n",
    "        for start, end, float_content, label, caption, node_type in floats:\n",
    "            excluded_ranges.append((start, end))\n",
    "            nodes.append(HierarchyNode(\n",
    "                node_type=node_type,\n",
    "                title=caption,\n",
    "                content=float_content,\n",
    "                label=label,\n",
    "                source_file=source_file\n",
    "            ))\n",
    "        \n",
    "        # Sort excluded ranges\n",
    "        excluded_ranges.sort()\n",
    "        \n",
    "        # Extract text between excluded ranges\n",
    "        text_segments = []\n",
    "        last_end = 0\n",
    "        for start, end in excluded_ranges:\n",
    "            if start > last_end:\n",
    "                text_segments.append(content[last_end:start])\n",
    "            last_end = max(last_end, end)\n",
    "        if last_end < len(content):\n",
    "            text_segments.append(content[last_end:])\n",
    "        \n",
    "        # Split text into sentences\n",
    "        full_text = ' '.join(text_segments)\n",
    "        sentences = self._split_into_sentences(full_text)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if sentence and len(sentence) > 10:  # Filter very short fragments\n",
    "                nodes.append(HierarchyNode(\n",
    "                    node_type=NodeType.SENTENCE,\n",
    "                    content=sentence,\n",
    "                    source_file=source_file\n",
    "                ))\n",
    "        \n",
    "        return nodes\n",
    "    \n",
    "    def _extract_abstract(self, content: str, source_file: str):\n",
    "        pattern = re.compile(\n",
    "            r'\\\\begin\\{abstract\\}(.*?)\\\\end\\{abstract\\}',\n",
    "            re.DOTALL | re.IGNORECASE\n",
    "        )\n",
    "        match = pattern.search(content)\n",
    "        if not match:\n",
    "            return content, None\n",
    "\n",
    "        abstract_raw = match.group(1)\n",
    "\n",
    "        abstract_node = HierarchyNode(\n",
    "            node_type=NodeType.ABSTRACT,\n",
    "            title=\"Abstract\",\n",
    "            source_file=source_file\n",
    "        )\n",
    "\n",
    "        # ðŸ”¥ QUAN TRá»ŒNG: táº¡o leaf nodes\n",
    "        abstract_node.children = self._create_leaf_nodes(\n",
    "            abstract_raw, source_file\n",
    "        )\n",
    "\n",
    "        # remove abstract khá»i content chÃ­nh\n",
    "        content = content[:match.start()] + content[match.end():]\n",
    "\n",
    "        return content, abstract_node\n",
    "    \n",
    "    def _extract_acknowledgments_content(self, content: str, start: int) -> str:\n",
    "        \"\"\"\n",
    "        Extract acknowledgments content until references/bibliography\n",
    "        \"\"\"\n",
    "        tail = content[start:]\n",
    "\n",
    "        ref_match = self.references_pattern.search(tail)\n",
    "        if ref_match:\n",
    "            return tail[:ref_match.start()]\n",
    "        return tail\n",
    "\n",
    "\n",
    "    \n",
    "    def parse(self, content: str, source_file: str = \"\") -> HierarchyNode:\n",
    "        \"\"\"Parse LaTeX content into a hierarchical tree\"\"\"\n",
    "        # Create root document node\n",
    "        root = HierarchyNode(\n",
    "            node_type=NodeType.DOCUMENT,\n",
    "            title=\"Document\",\n",
    "            source_file=source_file\n",
    "        )\n",
    "        \n",
    "        # Clean content\n",
    "        cleaned_content = self.cleaner.clean(content)\n",
    "\n",
    "        cleaned_content, abstract_node = self._extract_abstract(cleaned_content, source_file)\n",
    "        if abstract_node:\n",
    "            root.children.append(abstract_node)\n",
    "        \n",
    "        # Extract sections\n",
    "        sections = self._extract_sections(cleaned_content, source_file)\n",
    "        \n",
    "        if not sections:\n",
    "            # No sections found, create leaf nodes directly under root\n",
    "            root.children = self._create_leaf_nodes(cleaned_content, source_file)\n",
    "            return root\n",
    "        \n",
    "        # Build hierarchy using a stack\n",
    "        stack = [(root, -1)]  # (node, hierarchy_level)\n",
    "        \n",
    "        for i, (pos, node_type, title, full_match) in enumerate(sections):\n",
    "            # Check if this is a references section\n",
    "            is_ack = 'acknowledg' in title.lower()\n",
    "\n",
    "            if self._is_references_section(title) and not is_ack:\n",
    "                continue\n",
    "\n",
    "            \n",
    "            # Get content until next section\n",
    "            if i + 1 < len(sections):\n",
    "                next_pos = sections[i + 1][0]\n",
    "            else:\n",
    "                next_pos = len(cleaned_content)\n",
    "            \n",
    "            content_start = pos + len(full_match)\n",
    "\n",
    "            is_ack = 'acknowledg' in title.lower()\n",
    "\n",
    "            if is_ack:\n",
    "                section_content = self._extract_acknowledgments_content(\n",
    "                    cleaned_content, content_start\n",
    "                )\n",
    "            else:\n",
    "                section_content = cleaned_content[content_start:next_pos]\n",
    "            \n",
    "            # Check for special sections (acknowledgments, etc.)\n",
    "            is_acknowledgments = 'acknowledg' in title.lower()\n",
    "            if is_acknowledgments:\n",
    "                node_type = NodeType.ACKNOWLEDGMENTS\n",
    "            \n",
    "            level = self._get_hierarchy_level(node_type)\n",
    "            \n",
    "            # Find parent node\n",
    "            while stack and stack[-1][1] >= level:\n",
    "                stack.pop()\n",
    "            \n",
    "            parent = stack[-1][0] if stack else root\n",
    "            \n",
    "            # Create section node\n",
    "            section_node = HierarchyNode(\n",
    "                node_type=NodeType.ACKNOWLEDGMENTS if is_acknowledgments else node_type,\n",
    "                title=title,\n",
    "                source_file=source_file\n",
    "            )\n",
    "\n",
    "            section_node.children = self._create_leaf_nodes(\n",
    "                section_content, source_file\n",
    "            )\n",
    "            \n",
    "            # Extract label if present at start of section\n",
    "            label_match = self.label_pattern.search(section_content[:200])\n",
    "            if label_match:\n",
    "                section_node.label = label_match.group(1)\n",
    "            \n",
    "            # Add leaf nodes (sentences, formulas, figures)\n",
    "            section_node.children = self._create_leaf_nodes(section_content, source_file)\n",
    "            \n",
    "            parent.children.append(section_node)\n",
    "            stack.append((section_node, level))\n",
    "        \n",
    "        return root\n",
    "    \n",
    "    def parse_with_file_markers(self, merged_content: str) -> HierarchyNode:\n",
    "        \"\"\"Parse merged content that has file markers\"\"\"\n",
    "        # Split by file markers\n",
    "        file_pattern = re.compile(r'%%% FILE: (.+?) %%%\\n')\n",
    "        parts = file_pattern.split(merged_content)\n",
    "        \n",
    "        # Create root\n",
    "        root = HierarchyNode(\n",
    "            node_type=NodeType.DOCUMENT,\n",
    "            title=\"Document\"\n",
    "        )\n",
    "        \n",
    "        # Process each file's content\n",
    "        current_file = \"\"\n",
    "        for i, part in enumerate(parts):\n",
    "            if i % 2 == 1:  # File path\n",
    "                current_file = part\n",
    "            elif part.strip():  # Content\n",
    "                file_root = self.parse(part, current_file)\n",
    "                # Merge children into main root\n",
    "                root.children.extend(file_root.children)\n",
    "        \n",
    "        return root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ac6c187",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BibTeXExtractor:\n",
    "    \"\"\"Extract and convert bibliography entries from .bbl files and .bib files\"\"\"\n",
    "    \n",
    "    # Pattern to match \\bibitem entries in .bbl files\n",
    "    BIBITEM_PATTERN = re.compile(\n",
    "        r'\\\\bibitem\\[([^\\]]*)\\]\\{([^}]+)\\}\\s*(.*?)(?=\\\\bibitem|\\Z|\\\\end\\{thebibliography\\})',\n",
    "        re.DOTALL\n",
    "    )\n",
    "    \n",
    "    # Alternative pattern without optional argument\n",
    "    BIBITEM_SIMPLE_PATTERN = re.compile(\n",
    "        r'\\\\bibitem\\{([^}]+)\\}\\s*(.*?)(?=\\\\bibitem|\\Z|\\\\end\\{thebibliography\\})',\n",
    "        re.DOTALL\n",
    "    )\n",
    "    \n",
    "    # Pattern for BibTeX entries in .bib files  \n",
    "    BIBTEX_ENTRY_PATTERN = re.compile(\n",
    "        r'@(\\w+)\\{([^,]+),\\s*(.*?)\\n\\}',\n",
    "        re.DOTALL\n",
    "    )\n",
    "    \n",
    "    # Pattern for BibTeX fields\n",
    "    BIBTEX_FIELD_PATTERN = re.compile(\n",
    "        r'(\\w+)\\s*=\\s*\\{([^{}]*(?:\\{[^{}]*\\}[^{}]*)*)\\}',\n",
    "        re.DOTALL\n",
    "    )\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.entries: Dict[str, BibEntry] = {}\n",
    "        \n",
    "    def parse_bib_file(self, content: str) -> Dict[str, BibEntry]:\n",
    "        \"\"\"Parse a .bib file and extract entries\"\"\"\n",
    "        entries = {}\n",
    "        \n",
    "        for match in self.BIBTEX_ENTRY_PATTERN.finditer(content):\n",
    "            entry_type = match.group(1).lower()\n",
    "            key = match.group(2).strip()\n",
    "            fields_str = match.group(3)\n",
    "            \n",
    "            # Parse fields\n",
    "            fields = {}\n",
    "            for field_match in self.BIBTEX_FIELD_PATTERN.finditer(fields_str):\n",
    "                field_name = field_match.group(1).lower()\n",
    "                field_value = field_match.group(2).strip()\n",
    "                fields[field_name] = field_value\n",
    "            \n",
    "            entries[key] = BibEntry(key=key, entry_type=entry_type, fields=fields)\n",
    "        \n",
    "        return entries\n",
    "    \n",
    "    def parse_bbl_file(self, content: str) -> Dict[str, BibEntry]:\n",
    "        \"\"\"Parse a .bbl file and convert bibitem entries to BibTeX format\"\"\"\n",
    "        entries = {}\n",
    "        \n",
    "        # Try pattern with optional citation label\n",
    "        for match in self.BIBITEM_PATTERN.finditer(content):\n",
    "            cite_label = match.group(1)\n",
    "            key = match.group(2).strip()\n",
    "            entry_content = match.group(3).strip()\n",
    "            \n",
    "            entry = self._parse_bibitem_content(key, entry_content, cite_label)\n",
    "            if entry:\n",
    "                entries[key] = entry\n",
    "        \n",
    "        # If no entries found, try simple pattern\n",
    "        if not entries:\n",
    "            for match in self.BIBITEM_SIMPLE_PATTERN.finditer(content):\n",
    "                key = match.group(1).strip()\n",
    "                entry_content = match.group(2).strip()\n",
    "                \n",
    "                entry = self._parse_bibitem_content(key, entry_content)\n",
    "                if entry:\n",
    "                    entries[key] = entry\n",
    "        \n",
    "        return entries\n",
    "    \n",
    "    def _parse_bibitem_content(self, key: str, content: str, cite_label: str = \"\") -> Optional[BibEntry]:\n",
    "        \"\"\"Parse the content of a bibitem and extract bibliographic fields\"\"\"\n",
    "        fields = {}\n",
    "        \n",
    "        # Clean up LaTeX commands\n",
    "        content = self._clean_latex(content)\n",
    "        \n",
    "        # Try to extract structured information\n",
    "        # Extract authors (usually at the beginning)\n",
    "        author_match = re.match(r'^([^.]+?)\\.', content)\n",
    "        if author_match:\n",
    "            authors = author_match.group(1).strip()\n",
    "            authors = self._normalize_authors(authors)\n",
    "            fields['author'] = authors\n",
    "            content = content[author_match.end():].strip()\n",
    "        \n",
    "        # Extract title (usually in \\newblock or after authors)\n",
    "        title_match = re.search(r'\\\\newblock\\s*\\{?([^.]+)\\}?\\.', content)\n",
    "        if title_match:\n",
    "            fields['title'] = title_match.group(1).strip()\n",
    "        else:\n",
    "            # Try to find title as first sentence after author\n",
    "            title_match = re.match(r'^([^.]+)\\.', content)\n",
    "            if title_match:\n",
    "                fields['title'] = title_match.group(1).strip()\n",
    "        \n",
    "        # Extract year\n",
    "        year_match = re.search(r'\\b(19|20)\\d{2}\\b', content)\n",
    "        if year_match:\n",
    "            fields['year'] = year_match.group(0)\n",
    "        \n",
    "        # Extract journal/booktitle (often in \\emph{})\n",
    "        journal_match = re.search(r'\\\\emph\\{([^}]+)\\}', content)\n",
    "        if journal_match:\n",
    "            venue = journal_match.group(1).strip()\n",
    "            # Guess if it's a journal or conference\n",
    "            if any(kw in venue.lower() for kw in ['proc', 'conf', 'workshop', 'symposium']):\n",
    "                fields['booktitle'] = venue\n",
    "            else:\n",
    "                fields['journal'] = venue\n",
    "        \n",
    "        # Extract URL\n",
    "        url_match = re.search(r'\\\\url\\{([^}]+)\\}', content)\n",
    "        if url_match:\n",
    "            fields['url'] = url_match.group(1)\n",
    "        \n",
    "        # Extract DOI\n",
    "        doi_match = re.search(r'doi:\\s*([^\\s,}]+)', content, re.IGNORECASE)\n",
    "        if doi_match:\n",
    "            fields['doi'] = doi_match.group(1)\n",
    "        \n",
    "        # Extract pages\n",
    "        pages_match = re.search(r'pages?\\s*[:\\s]*(\\d+(?:\\s*[-â€“]\\s*\\d+)?)', content, re.IGNORECASE)\n",
    "        if pages_match:\n",
    "            fields['pages'] = pages_match.group(1).replace('â€“', '--')\n",
    "        \n",
    "        # Extract volume\n",
    "        volume_match = re.search(r'vol(?:ume)?\\.?\\s*(\\d+)', content, re.IGNORECASE)\n",
    "        if volume_match:\n",
    "            fields['volume'] = volume_match.group(1)\n",
    "        \n",
    "        # Determine entry type\n",
    "        entry_type = self._guess_entry_type(fields, content)\n",
    "        \n",
    "        # Store original content as note if parsing was incomplete\n",
    "        if len(fields) < 3:\n",
    "            fields['note'] = content[:500]  # Truncate long notes\n",
    "        \n",
    "        return BibEntry(key=key, entry_type=entry_type, fields=fields)\n",
    "    \n",
    "    def _clean_latex(self, content: str) -> str:\n",
    "        \"\"\"Remove common LaTeX formatting commands\"\"\"\n",
    "        # Remove \\newblock\n",
    "        content = re.sub(r'\\\\newblock\\s*', '', content)\n",
    "        # Remove common formatting\n",
    "        content = re.sub(r'\\\\textit\\{([^}]*)\\}', r'\\1', content)\n",
    "        content = re.sub(r'\\\\textbf\\{([^}]*)\\}', r'\\1', content)\n",
    "        content = re.sub(r'\\\\texttt\\{([^}]*)\\}', r'\\1', content)\n",
    "        content = re.sub(r'\\\\emph\\{([^}]*)\\}', r'\\1', content)\n",
    "        # Remove escaped characters\n",
    "        content = content.replace('\\\\&', '&')\n",
    "        content = content.replace('\\\\~', '~')\n",
    "        content = content.replace('\\\\{', '{')\n",
    "        content = content.replace('\\\\}', '}')\n",
    "        # Normalize whitespace\n",
    "        content = re.sub(r'\\s+', ' ', content)\n",
    "        return content.strip()\n",
    "    \n",
    "    def _normalize_authors(self, authors: str) -> str:\n",
    "        \"\"\"Normalize author names to BibTeX format\"\"\"\n",
    "        # Replace 'and' with ' and '\n",
    "        authors = re.sub(r'\\s+and\\s+', ' and ', authors)\n",
    "        # Replace commas between authors (but not in names)\n",
    "        # Handle \"et al.\"\n",
    "        authors = re.sub(r',?\\s*et\\s*al\\.?', ' and others', authors)\n",
    "        return authors.strip()\n",
    "    \n",
    "    def _guess_entry_type(self, fields: Dict[str, str], content: str) -> str:\n",
    "        \"\"\"Guess the BibTeX entry type based on available fields\"\"\"\n",
    "        content_lower = content.lower()\n",
    "        \n",
    "        if 'booktitle' in fields or 'proceedings' in content_lower or 'conference' in content_lower:\n",
    "            return 'inproceedings'\n",
    "        elif 'journal' in fields:\n",
    "            return 'article'\n",
    "        elif 'url' in fields and ('howpublished' in content_lower or 'accessed' in content_lower):\n",
    "            return 'misc'\n",
    "        elif 'publisher' in content_lower or 'press' in content_lower:\n",
    "            return 'book'\n",
    "        elif 'thesis' in content_lower:\n",
    "            if 'phd' in content_lower or 'doctoral' in content_lower:\n",
    "                return 'phdthesis'\n",
    "            elif 'master' in content_lower:\n",
    "                return 'mastersthesis'\n",
    "        elif 'arxiv' in content_lower:\n",
    "            return 'article'\n",
    "        \n",
    "        return 'misc'\n",
    "    \n",
    "    def load_from_directory(self, base_dir: str) -> Dict[str, BibEntry]:\n",
    "        \"\"\"Load all bibliography entries from .bib and .bbl files in directory\"\"\"\n",
    "        base_path = Path(base_dir)\n",
    "        \n",
    "        # Load .bib files\n",
    "        for bib_file in base_path.rglob('*.bib'):\n",
    "            try:\n",
    "                with open(bib_file, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                    content = f.read()\n",
    "                entries = self.parse_bib_file(content)\n",
    "                self.entries.update(entries)\n",
    "                print(f\"Loaded {len(entries)} entries from {bib_file.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {bib_file}: {e}\")\n",
    "        \n",
    "        # Load .bbl files\n",
    "        for bbl_file in base_path.rglob('*.bbl'):\n",
    "            try:\n",
    "                with open(bbl_file, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                    content = f.read()\n",
    "                entries = self.parse_bbl_file(content)\n",
    "                # Only add entries not already present from .bib files\n",
    "                for key, entry in entries.items():\n",
    "                    if key not in self.entries:\n",
    "                        self.entries[key] = entry\n",
    "                print(f\"Loaded {len(entries)} entries from {bbl_file.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {bbl_file}: {e}\")\n",
    "        \n",
    "        return self.entries\n",
    "    \n",
    "    def to_bibtex(self) -> str:\n",
    "        \"\"\"Export all entries to BibTeX format\"\"\"\n",
    "        return '\\n\\n'.join(entry.to_bibtex() for entry in self.entries.values())\n",
    "    \n",
    "    def get_entries(self) -> Dict[str, BibEntry]:\n",
    "        \"\"\"Get all loaded entries\"\"\"\n",
    "        return self.entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "502eb56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deduplicator:\n",
    "    \"\"\"Handles deduplication of references and content\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.content_hashes: Dict[str, str] = {}  # hash -> unique_id\n",
    "        self.reference_hashes: Dict[str, str] = {}  # content_hash -> key\n",
    "        \n",
    "    # ==================== Reference Deduplication ====================\n",
    "    \n",
    "    def deduplicate_references(self, entries: Dict[str, BibEntry]) -> Dict[str, BibEntry]:\n",
    "        \"\"\"\n",
    "        Deduplicate bibliography entries.\n",
    "        Entries with different keys but same content are merged.\n",
    "        \"\"\"\n",
    "        deduplicated = {}\n",
    "        hash_to_key: Dict[str, str] = {}\n",
    "        key_mappings: Dict[str, str] = {}  # old_key -> canonical_key\n",
    "        \n",
    "        for key, entry in entries.items():\n",
    "            content_hash = entry.content_hash()\n",
    "            \n",
    "            if content_hash in hash_to_key:\n",
    "                # Duplicate found - merge fields\n",
    "                canonical_key = hash_to_key[content_hash]\n",
    "                canonical_entry = deduplicated[canonical_key]\n",
    "                \n",
    "                # Merge fields (prefer non-empty values)\n",
    "                for field, value in entry.fields.items():\n",
    "                    if field not in canonical_entry.fields or not canonical_entry.fields[field]:\n",
    "                        canonical_entry.fields[field] = value\n",
    "                \n",
    "                key_mappings[key] = canonical_key\n",
    "                print(f\"Merged duplicate: {key} -> {canonical_key}\")\n",
    "            else:\n",
    "                # New unique entry\n",
    "                hash_to_key[content_hash] = key\n",
    "                deduplicated[key] = entry\n",
    "                key_mappings[key] = key\n",
    "        \n",
    "        self.reference_hashes = hash_to_key\n",
    "        return deduplicated\n",
    "    \n",
    "    def find_similar_references(self, entries: Dict[str, BibEntry], \n",
    "                                 threshold: float = 0.8) -> List[Tuple[str, str, float]]:\n",
    "        \"\"\"\n",
    "        Find potentially duplicate references using fuzzy matching.\n",
    "        Returns list of (key1, key2, similarity_score)\n",
    "        \"\"\"\n",
    "        similar_pairs = []\n",
    "        keys = list(entries.keys())\n",
    "        \n",
    "        for i, key1 in enumerate(keys):\n",
    "            for key2 in keys[i+1:]:\n",
    "                entry1 = entries[key1]\n",
    "                entry2 = entries[key2]\n",
    "                \n",
    "                similarity = self._compute_entry_similarity(entry1, entry2)\n",
    "                if similarity >= threshold:\n",
    "                    similar_pairs.append((key1, key2, similarity))\n",
    "        \n",
    "        return sorted(similar_pairs, key=lambda x: -x[2])\n",
    "    \n",
    "    def _compute_entry_similarity(self, entry1: BibEntry, entry2: BibEntry) -> float:\n",
    "        \"\"\"Compute similarity between two bibliography entries\"\"\"\n",
    "        scores = []\n",
    "        \n",
    "        # Compare titles\n",
    "        if 'title' in entry1.fields and 'title' in entry2.fields:\n",
    "            title_sim = self._string_similarity(\n",
    "                entry1.fields['title'].lower(),\n",
    "                entry2.fields['title'].lower()\n",
    "            )\n",
    "            scores.append(title_sim * 2)  # Weight title higher\n",
    "        \n",
    "        # Compare authors\n",
    "        if 'author' in entry1.fields and 'author' in entry2.fields:\n",
    "            author_sim = self._string_similarity(\n",
    "                entry1.fields['author'].lower(),\n",
    "                entry2.fields['author'].lower()\n",
    "            )\n",
    "            scores.append(author_sim * 1.5)\n",
    "        \n",
    "        # Compare year\n",
    "        if 'year' in entry1.fields and 'year' in entry2.fields:\n",
    "            year_sim = 1.0 if entry1.fields['year'] == entry2.fields['year'] else 0.0\n",
    "            scores.append(year_sim)\n",
    "        \n",
    "        if not scores:\n",
    "            return 0.0\n",
    "        \n",
    "        return sum(scores) / (2 + 1.5 + 1)  # Normalized by weights\n",
    "    \n",
    "    def _string_similarity(self, s1: str, s2: str) -> float:\n",
    "        \"\"\"Compute similarity between two strings using Jaccard similarity on words\"\"\"\n",
    "        words1 = set(s1.split())\n",
    "        words2 = set(s2.split())\n",
    "        \n",
    "        if not words1 or not words2:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = len(words1 & words2)\n",
    "        union = len(words1 | words2)\n",
    "        \n",
    "        return intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    # ==================== Content Deduplication ====================\n",
    "    \n",
    "    def deduplicate_content(self, root: HierarchyNode) -> HierarchyNode:\n",
    "        \"\"\"\n",
    "        Deduplicate content in the hierarchy tree.\n",
    "        Identical content elements are given the same unique_id.\n",
    "        \"\"\"\n",
    "        self.content_hashes.clear()\n",
    "        return self._deduplicate_node(root)\n",
    "    \n",
    "    def _deduplicate_node(self, node: HierarchyNode) -> HierarchyNode:\n",
    "        \"\"\"Recursively deduplicate a node and its children\"\"\"\n",
    "        # Process children first\n",
    "        deduplicated_children = []\n",
    "        for child in node.children:\n",
    "            deduplicated_child = self._deduplicate_node(child)\n",
    "            deduplicated_children.append(deduplicated_child)\n",
    "        \n",
    "        node.children = deduplicated_children\n",
    "        \n",
    "        # For leaf nodes, check for duplicates\n",
    "        if node.node_type in LEAF_TYPES and node.content:\n",
    "            content_hash = node.content_hash\n",
    "            \n",
    "            if content_hash in self.content_hashes:\n",
    "                # Use existing unique_id for duplicate content\n",
    "                node.unique_id = self.content_hashes[content_hash]\n",
    "            else:\n",
    "                # Store this as the canonical version\n",
    "                self.content_hashes[content_hash] = node.unique_id\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def get_duplicate_stats(self, root: HierarchyNode) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics about duplicates in the hierarchy\"\"\"\n",
    "        stats = {\n",
    "            'total_nodes': 0,\n",
    "            'unique_contents': 0,\n",
    "            'duplicate_contents': 0,\n",
    "            'by_type': defaultdict(lambda: {'total': 0, 'unique': 0})\n",
    "        }\n",
    "        \n",
    "        seen_hashes = set()\n",
    "        \n",
    "        def count_node(node: HierarchyNode):\n",
    "            stats['total_nodes'] += 1\n",
    "            stats['by_type'][node.node_type.value]['total'] += 1\n",
    "            \n",
    "            if node.node_type in LEAF_TYPES and node.content_hash:\n",
    "                if node.content_hash not in seen_hashes:\n",
    "                    seen_hashes.add(node.content_hash)\n",
    "                    stats['unique_contents'] += 1\n",
    "                    stats['by_type'][node.node_type.value]['unique'] += 1\n",
    "                else:\n",
    "                    stats['duplicate_contents'] += 1\n",
    "            \n",
    "            for child in node.children:\n",
    "                count_node(child)\n",
    "        \n",
    "        count_node(root)\n",
    "        return stats\n",
    "    \n",
    "    def find_duplicate_content(self, root: HierarchyNode) -> Dict[str, List[HierarchyNode]]:\n",
    "        \"\"\"Find all duplicate content items grouped by content hash\"\"\"\n",
    "        hash_to_nodes: Dict[str, List[HierarchyNode]] = defaultdict(list)\n",
    "        \n",
    "        def collect_node(node: HierarchyNode):\n",
    "            if node.node_type in LEAF_TYPES and node.content_hash:\n",
    "                hash_to_nodes[node.content_hash].append(node)\n",
    "            for child in node.children:\n",
    "                collect_node(child)\n",
    "        \n",
    "        collect_node(root)\n",
    "        \n",
    "        # Return only groups with duplicates\n",
    "        return {h: nodes for h, nodes in hash_to_nodes.items() if len(nodes) > 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "06918bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaTeXParser:\n",
    "    \"\"\"\n",
    "    Main LaTeX parser that integrates all components:\n",
    "    - Multi-file gathering\n",
    "    - Hierarchy construction\n",
    "    - Reference extraction\n",
    "    - Deduplication\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir: str):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.gatherer = LaTeXFileGatherer(base_dir)\n",
    "        self.cleaner = LaTeXCleaner()\n",
    "        self.hierarchy_parser = HierarchyParser(self.cleaner)\n",
    "        self.bib_extractor = BibTeXExtractor()\n",
    "        self.deduplicator = Deduplicator()\n",
    "        \n",
    "        # Results\n",
    "        self.hierarchy: Optional[HierarchyNode] = None\n",
    "        self.references: Dict[str, BibEntry] = {}\n",
    "        self.file_contents: Dict[str, str] = {}\n",
    "        \n",
    "    def parse(self, main_file: str = \"main.tex\") -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Parse the LaTeX document starting from the main file.\n",
    "        Returns a dictionary with hierarchy, references, and statistics.\n",
    "        \"\"\"\n",
    "        print(f\"Parsing LaTeX document from: {self.base_dir / main_file}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Multi-file gathering\n",
    "        print(\"\\n[1] Gathering files...\")\n",
    "        self.file_contents = self.gatherer.gather_files(main_file)\n",
    "        print(f\"    Found {len(self.file_contents)} files in compilation path\")\n",
    "        \n",
    "        # Report unused files\n",
    "        unused = self.gatherer.get_unused_files()\n",
    "        if unused:\n",
    "            print(f\"    Unused files ({len(unused)}):\")\n",
    "            for f in list(unused)[:5]:\n",
    "                print(f\"      - {Path(f).name}\")\n",
    "            if len(unused) > 5:\n",
    "                print(f\"      ... and {len(unused) - 5} more\")\n",
    "        \n",
    "        # Step 2: Build hierarchy\n",
    "        print(\"\\n[2] Building hierarchy...\")\n",
    "        merged_content = self.gatherer.get_merged_content()\n",
    "        self.hierarchy = self.hierarchy_parser.parse_with_file_markers(merged_content)\n",
    "        \n",
    "        hierarchy_stats = self._count_hierarchy_nodes(self.hierarchy)\n",
    "        print(f\"    Built hierarchy with {hierarchy_stats['total']} nodes:\")\n",
    "        for node_type, count in hierarchy_stats['by_type'].items():\n",
    "            print(f\"      - {node_type}: {count}\")\n",
    "        \n",
    "        # Step 3: Extract references\n",
    "        print(\"\\n[3] Extracting references...\")\n",
    "        self.references = self.bib_extractor.load_from_directory(str(self.base_dir))\n",
    "        print(f\"    Found {len(self.references)} bibliography entries\")\n",
    "        \n",
    "        # Step 4: Deduplicate references\n",
    "        print(\"\\n[4] Deduplicating references...\")\n",
    "        original_count = len(self.references)\n",
    "        self.references = self.deduplicator.deduplicate_references(self.references)\n",
    "        dedup_count = original_count - len(self.references)\n",
    "        print(f\"    Removed {dedup_count} duplicate references\")\n",
    "        \n",
    "        # Find similar references (potential duplicates with different keys)\n",
    "        similar = self.deduplicator.find_similar_references(self.references, threshold=0.7)\n",
    "        if similar:\n",
    "            print(f\"    Found {len(similar)} pairs of similar references:\")\n",
    "            for key1, key2, sim in similar[:3]:\n",
    "                print(f\"      - '{key1}' ~ '{key2}' (similarity: {sim:.2%})\")\n",
    "        \n",
    "        # Step 5: Deduplicate content\n",
    "        print(\"\\n[5] Deduplicating content...\")\n",
    "        self.hierarchy = self.deduplicator.deduplicate_content(self.hierarchy)\n",
    "        dup_stats = self.deduplicator.get_duplicate_stats(self.hierarchy)\n",
    "        print(f\"    Unique content items: {dup_stats['unique_contents']}\")\n",
    "        print(f\"    Duplicate content items: {dup_stats['duplicate_contents']}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Parsing complete!\")\n",
    "        \n",
    "        return {\n",
    "            'hierarchy': self.hierarchy,\n",
    "            'references': self.references,\n",
    "            'file_contents': self.file_contents,\n",
    "            'stats': {\n",
    "                'files': len(self.file_contents),\n",
    "                'unused_files': len(unused),\n",
    "                'hierarchy_nodes': hierarchy_stats,\n",
    "                'references': len(self.references),\n",
    "                'duplicate_refs_removed': dedup_count,\n",
    "                'content_dedup': dup_stats\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _count_hierarchy_nodes(self, node: HierarchyNode) -> Dict[str, Any]:\n",
    "        \"\"\"Count nodes in hierarchy by type\"\"\"\n",
    "        stats = {'total': 0, 'by_type': defaultdict(int)}\n",
    "        \n",
    "        def count(n: HierarchyNode):\n",
    "            stats['total'] += 1\n",
    "            stats['by_type'][n.node_type.value] += 1\n",
    "            for child in n.children:\n",
    "                count(child)\n",
    "        \n",
    "        count(node)\n",
    "        return stats\n",
    "    \n",
    "    def to_json(self, include_content: bool = True) -> str:\n",
    "        \"\"\"Export parsed document to JSON\"\"\"\n",
    "        if not self.hierarchy:\n",
    "            return \"{}\"\n",
    "        \n",
    "        data = {\n",
    "            'hierarchy': self.hierarchy.to_dict(),\n",
    "            'references': {\n",
    "                key: {\n",
    "                    'type': entry.entry_type,\n",
    "                    'key': entry.key,\n",
    "                    'fields': entry.fields\n",
    "                }\n",
    "                for key, entry in self.references.items()\n",
    "            },\n",
    "            'metadata': {\n",
    "                'base_dir': str(self.base_dir),\n",
    "                'files_count': len(self.file_contents),\n",
    "                'references_count': len(self.references)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return json.dumps(data, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    def export_bibtex(self) -> str:\n",
    "        \"\"\"Export references to BibTeX format\"\"\"\n",
    "        return self.bib_extractor.to_bibtex()\n",
    "    \n",
    "    def print_hierarchy(self, node: HierarchyNode = None, indent: int = 0):\n",
    "        \"\"\"Print hierarchy tree for debugging\"\"\"\n",
    "        if node is None:\n",
    "            node = self.hierarchy\n",
    "        \n",
    "        if node is None:\n",
    "            print(\"No hierarchy parsed yet\")\n",
    "            return\n",
    "        \n",
    "        prefix = \"  \" * indent\n",
    "        type_str = node.node_type.value.upper()\n",
    "        title_str = f\": {node.title[:50]}...\" if node.title and len(node.title) > 50 else f\": {node.title}\" if node.title else \"\"\n",
    "        content_preview = \"\"\n",
    "        \n",
    "        if node.node_type in LEAF_TYPES and node.content:\n",
    "            content_preview = f\" [{node.content[:40]}...]\"\n",
    "        \n",
    "        print(f\"{prefix}[{type_str}]{title_str}{content_preview}\")\n",
    "        \n",
    "        for child in node.children:\n",
    "            self.print_hierarchy(child, indent + 1)\n",
    "    \n",
    "    def get_all_sentences(self) -> List[str]:\n",
    "        \"\"\"Extract all sentences from the hierarchy\"\"\"\n",
    "        sentences = []\n",
    "        \n",
    "        def extract(node: HierarchyNode):\n",
    "            if node.node_type == NodeType.SENTENCE:\n",
    "                sentences.append(node.content)\n",
    "            for child in node.children:\n",
    "                extract(child)\n",
    "        \n",
    "        if self.hierarchy:\n",
    "            extract(self.hierarchy)\n",
    "        \n",
    "        return sentences\n",
    "    \n",
    "    def get_all_formulas(self) -> List[str]:\n",
    "        \"\"\"Extract all block formulas from the hierarchy\"\"\"\n",
    "        formulas = []\n",
    "        \n",
    "        def extract(node: HierarchyNode):\n",
    "            if node.node_type == NodeType.BLOCK_FORMULA:\n",
    "                formulas.append(node.content)\n",
    "            for child in node.children:\n",
    "                extract(child)\n",
    "        \n",
    "        if self.hierarchy:\n",
    "            extract(self.hierarchy)\n",
    "        \n",
    "        return formulas\n",
    "    \n",
    "    def get_all_figures(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Extract all figures from the hierarchy\"\"\"\n",
    "        figures = []\n",
    "        \n",
    "        def extract(node: HierarchyNode):\n",
    "            if node.node_type == NodeType.FIGURE:\n",
    "                figures.append({\n",
    "                    'label': node.label,\n",
    "                    'caption': node.title,\n",
    "                    'content': node.content\n",
    "                })\n",
    "            for child in node.children:\n",
    "                extract(child)\n",
    "        \n",
    "        if self.hierarchy:\n",
    "            extract(self.hierarchy)\n",
    "        \n",
    "        return figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2aad0f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Hierarchy (top levels):\n",
      "============================================================\n",
      "[DOCUMENT] Document\n",
      "  [CHAPTER] Introduction\n",
      "  [CHAPTER] \\nlgsec\n",
      "    [SENTENCE] \n",
      "  [CHAPTER] \\nlusec\n",
      "    [SENTENCE] \n",
      "  [CHAPTER] \\autolabsec\n",
      "    [SENTENCE] \n",
      "  [CHAPTER] \\aidosec\n",
      "    [SENTENCE] \n",
      "  [CHAPTER] \\sharcsec\n",
      "  [CHAPTER] \\moocsec\n",
      "  [CHAPTER] Conclusion\n",
      "  [FIGURE] Temp figure: {#1\n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [ACKNOWLEDGMENTS] Acknowledgments\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "  [CHAPTER] Abstract\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "  [SECTION] Robot's accessibility for consumers\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SUBSECTION] \\nlgsec\n",
      "    [SUBSECTION] \\nlusec\n",
      "  [SECTION] Robot's accessibility in scientific research\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SUBSECTION] \\autolabsec\n",
      "    [SUBSECTION] \\aidosec\n",
      "    [SUBSECTION] \\sharcsec\n",
      "  [SECTION] Robot's accessibility in education\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SUBSECTION] \\moocsec\n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SECTION] Related Work\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "  [SECTION] Task Definition\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "  [SECTION] Model\n",
      "    [FIGURE] Our method generates natural language instructions...\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SUBSECTION] Compound Action Specifications\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "    [SUBSECTION] Content Selection\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SUBSUBSECTION] MDP with Inverse Reinforcement Learning\n",
      "      [SUBSUBSECTION] Sentence Planning\n",
      "    [SUBSECTION] Surface Realization\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SUBSUBSECTION] Sequence-to-Sequence Model\n",
      "      [SUBSUBSECTION] Language Model\n",
      "  [SECTION] Experimental Setup\n",
      "    [SUBSECTION] Dataset\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SUBSUBSECTION] Data Augmentation\n",
      "    [SUBSECTION] Implementation Details\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "    [SUBSECTION] Automatic Evaluation\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "    [SUBSECTION] Human Evaluation\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "  [SECTION] Results\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SUBSECTION] Aligner Ablation\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "    [SUBSECTION] Language Model Ablation\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "    [SUBSECTION] Aligner Visualization\n",
      "      [FIGURE] Alignment visualization for two\n",
      " pairs of CAS and ...\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "    [SUBSECTION] Human Evaluation\n",
      "      [FIGURE] Participants' distances from the goal.\n",
      " \\label{hri...\n",
      "      [FIGURE] {\\small ``Define the amount of information provide...\n",
      "      [FIGURE] \n",
      " Paths followed by five participants \n",
      " given huma...\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "  [SECTION] Conclusion\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "  [FIGURE] Our framework learns the kinematic model that gove...\n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SECTION] Related Work\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "  [SECTION] Multimodal Learning Framework\n",
      "    [BLOCK_FORMULA] \n",
      "    [FIGURE] Our multimodal articulation learning framework fir...\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SUBSECTION] Vision-guided Model Fitting\n",
      "      [BLOCK_FORMULA] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "    [SUBSECTION] Language-guided Model Selection\n",
      "      [BLOCK_FORMULA] \n",
      "      [BLOCK_FORMULA] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "    [SUBSECTION] Combining Vision and Language Observations\n",
      "      [BLOCK_FORMULA] \n",
      "      [BLOCK_FORMULA] \n",
      "      [BLOCK_FORMULA] \n",
      "      [BLOCK_FORMULA] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "  [FIGURE] The DCG for the utterance ``a man opens and closes...\n",
      "  [SECTION] Results\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SUBSECTION] Evaluation Metrics\n",
      "      [BLOCK_FORMULA] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "    [SUBSECTION] Results and Analysis\n",
      "      [TABLE] Overall performance of our framework on video-desc...\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "  [SECTION] Conclusion\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "  [FIGURE] : We augment Duckietowns~\\cite{paull2017duckietown\n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SECTION] Integrated benchmarking and development for reprod...\n",
      "    [FIGURE] : Designing robotic benchmarks should be more tigh...\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SUBSECTION] Reproducibility\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SUBSUBSECTION] Software\n",
      "      [SUBSUBSECTION] Hardware\n",
      "      [SUBSUBSECTION] Environment\n",
      "    [SUBSECTION] Agent Interoperability\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "    [SUBSECTION] Robot-Agent Abstraction\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "    [SUBSECTION] Robot-Benchmark Abstraction\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "  [SECTION] The \\ac{DUCKIENet\n",
      "    [FIGURE] : The well-defined interfaces enable execution of ...\n",
      "    [FIGURE] : Local development can take place on the real har...\n",
      "    [SENTENCE] \n",
      "    [SUBSECTION] The Base Platform\n",
      "      [SENTENCE] \n",
      "      [SUBSUBSECTION] The Duckietown hardware platform\n",
      "      [SUBSUBSECTION] The Duckietown software architecture\n",
      "    [SUBSECTION] System Architecture Overview\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "    [SUBSECTION] The Duckietown Automated Laboratory (\\ac{DTA\n",
      "      [FIGURE]  GUI\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SUBSUBSECTION] Localization\n",
      "      [SUBSUBSECTION] Operator console\n",
      "    [SUBSECTION] Defining the Benchmarks\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "    [SUBSECTION] DTA Operation Workflow\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "    [SUBSECTION] The Artificial Intelligence Driving Olympics (AI-D...\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "  [SECTION] Validation\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SUBSECTION] Experiment Repeatability\n",
      "      [FIGURE] : Experiment repeated on the same robot. Plots sho...\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "    [SUBSECTION] Inter-Robot Reproducibility\n",
      "      [FIGURE] : Experiments on three different robots show that ...\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "    [SUBSECTION] \\ac{DTA\n",
      "      [FIGURE] : Experiments in two different \\acp{DTA\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "    [SUBSECTION] Limitations\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "  [SECTION] Conclusions\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "  [FIGURE] The AI-DO 1 competition at NeurIPS 2018, Montr\\'ea...\n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SECTION] The Purpose of AI-DO\n",
      "    [TABLE]  \\newline\n",
      " Definitions of characteristics as they ...\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SUBSECTION] Important characteristics for learning-based robot...\n",
      "      [SUBSUBSECTION] Accessibility\n",
      "      [SUBSUBSECTION] Resource constraints\n",
      "      [SUBSUBSECTION] Modularity\n",
      "      [SUBSUBSECTION] Simulation / Realism\n",
      "      [SUBSUBSECTION] ML compatibility\n",
      "      [SUBSUBSECTION] Embodiment\n",
      "      [SUBSUBSECTION] Diverse metrics\n",
      "      [SUBSUBSECTION] Reproducible experiments\n",
      "      [SUBSUBSECTION] Automatic experiments\n",
      "  [SECTION] AI-DO 1: Competition Description\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SUBSECTION] The Challenges\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "    [SUBSECTION] Metrics and Judging\n",
      "      [SENTENCE] \n",
      "      [SUBSUBSECTION] Lane following with and without obstacles (LF/LFV)\n",
      "      [SUBSUBSECTION] Autonomous mobility on demand (AMOD)\n",
      "    [SUBSECTION] The Physical Competition Infrastructure\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SUBSUBSECTION] The robot - Duckiebot\n",
      "      [SUBSUBSECTION] The environment - Duckietown\n",
      "    [SUBSECTION] The Software Infrastructure\n",
      "      [FIGURE] Illustration showing the interconnected components...\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SUBSUBSECTION] The Development Pipeline\n",
      "      [SUBSUBSECTION] Simulation\n",
      "      [SUBSUBSECTION] Containerization\n",
      "      [SUBSUBSECTION] Baselines\n",
      "  [SECTION] AI-DO 1: Results\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SUBSECTION] Qualifications\n",
      "      [FIGURE] A bird's-eye-view of the trajectories that six dif...\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "    [SUBSECTION] Finals\n",
      "      [FIGURE] Impressions from the finals of AI-DO 1 at NeurIPS ...\n",
      "      [TABLE] Results of the simulation and live competition in ...\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SUBSUBSECTION] Outcomes\n",
      "      [SUBSUBSECTION] Awards\n",
      "  [SECTION] AI-DO 1: Lessons Learned\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SUBSECTION] Technical\n",
      "      [SENTENCE] \n",
      "      [SUBSUBSECTION] Successes\n",
      "      [SUBSUBSECTION] To Be Improved\n",
      "    [SUBSECTION] Non-Technical\n",
      "      [SENTENCE] \n",
      "      [SUBSUBSECTION] Successes\n",
      "      [SUBSUBSECTION] To Be Improved\n",
      "  [SECTION] Subsequent editions of the AI-DO\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "  [SECTION] Conclusion\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "  [SECTION] XRF sampling of seafloor sediments\n",
      "    [FIGURE] Geographically distributed team of scientists duri...\n",
      "    [FIGURE] Robot placing the XRF tool on the seafloor\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "  [SECTION] XRF sampling using SHARC\n",
      "    [FIGURE] SHARC multimodal interface\n",
      "    [FIGURE] On-shore scientist using a VR system to choose a s...\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "  [SECTION] Conclusion\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "  [SECTION] Learning Goals\n",
      "    [SENTENCE] \n",
      "  [SECTION] First Edition - 2021\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SUBSECTION] The DB21M robot\n",
      "      [FIGURE] An exploded view of the DB21M robot\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "  [SECTION] Second Edition - 2022\n",
      "    [SENTENCE] \n",
      "    [SENTENCE] \n",
      "    [SUBSECTION] The DB21J robot\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "    [SUBSECTION] Towards a full browser-based robot programming exp...\n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SENTENCE] \n",
      "      [SUBSUBSECTION] Linux is rare\n",
      "      [SUBSUBSECTION] Web Browsers are abundant\n",
      "      [SUBSUBSECTION] The missing piece\n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n",
      "  [SENTENCE] \n"
     ]
    }
   ],
   "source": [
    "# Print the hierarchy structure (limited depth for readability)\n",
    "print(\"Document Hierarchy (top levels):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def print_limited_hierarchy(node, max_depth=3, current_depth=0):\n",
    "    \"\"\"Print hierarchy with depth limit\"\"\"\n",
    "    if current_depth > max_depth:\n",
    "        return\n",
    "    \n",
    "    prefix = \"  \" * current_depth\n",
    "    type_str = node.node_type.value.upper()\n",
    "    title = node.title[:50] + \"...\" if node.title and len(node.title) > 50 else node.title\n",
    "    \n",
    "    leaf_count = sum(1 for c in node.children if c.node_type in LEAF_TYPES)\n",
    "    child_count = len(node.children) - leaf_count\n",
    "    \n",
    "    suffix = \"\"\n",
    "    if leaf_count > 0:\n",
    "        suffix = f\" ({leaf_count} leaf elements)\"\n",
    "    \n",
    "    print(f\"{prefix}[{type_str}] {title}{suffix}\")\n",
    "    \n",
    "    for child in node.children:\n",
    "        if child.node_type not in LEAF_TYPES:\n",
    "            print_limited_hierarchy(child, max_depth, current_depth + 1)\n",
    "\n",
    "print_limited_hierarchy(parser.hierarchy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c1336c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MilestoneExporter:\n",
    "    \"\"\"\n",
    "    Export parsed LaTeX document to the milestone-required format.\n",
    "    \n",
    "    Format for <yymm-id>.json:\n",
    "    {\n",
    "        \"elements\": {\n",
    "            \"id-string-1\": \"Cleaned latex content of element 1\",\n",
    "            ...\n",
    "        },\n",
    "        \"hierarchy\": {\n",
    "            \"1\": {  // Version 1\n",
    "                \"child-id\": \"parent-id\",\n",
    "                ...\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.elements: Dict[str, str] = {}  # unique_id -> cleaned content\n",
    "        self.hierarchy: Dict[str, Dict[str, str]] = {}  # version -> {child_id: parent_id}\n",
    "        \n",
    "    def export_document(self, root: HierarchyNode, version: str = \"1\") -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Export a parsed document to the milestone format.\n",
    "        \n",
    "        Args:\n",
    "            root: The root HierarchyNode of the parsed document\n",
    "            version: Version identifier (e.g., \"1\", \"2\", etc.)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary in the required format\n",
    "        \"\"\"\n",
    "        self.elements.clear()\n",
    "        self.hierarchy.clear()\n",
    "        \n",
    "        # Process the hierarchy tree\n",
    "        self._process_node(root, parent_id=None, version=version)\n",
    "        \n",
    "        return {\n",
    "            \"elements\": self.elements,\n",
    "            \"hierarchy\": self.hierarchy\n",
    "        }\n",
    "    \n",
    "    def _process_node(self, node: HierarchyNode, parent_id: Optional[str], version: str):\n",
    "        \"\"\"Recursively process nodes to extract elements and build hierarchy\"\"\"\n",
    "        \n",
    "        # Initialize version hierarchy if not exists\n",
    "        if version not in self.hierarchy:\n",
    "            self.hierarchy[version] = {}\n",
    "        \n",
    "        current_id = node.unique_id\n",
    "        \n",
    "        # For leaf nodes, store the content in elements\n",
    "        if node.node_type in LEAF_TYPES:\n",
    "            if node.content:\n",
    "                # Only store if not already present (deduplication)\n",
    "                if current_id not in self.elements:\n",
    "                    self.elements[current_id] = node.content\n",
    "                \n",
    "                # Add to hierarchy (child -> parent relationship)\n",
    "                if parent_id:\n",
    "                    self.hierarchy[version][current_id] = parent_id\n",
    "        else:\n",
    "            # For non-leaf nodes, also track in hierarchy but don't store content\n",
    "            # Store section/chapter titles as content for context\n",
    "            if node.title:\n",
    "                content = f\"[{node.node_type.value.upper()}] {node.title}\"\n",
    "                if current_id not in self.elements:\n",
    "                    self.elements[current_id] = content\n",
    "            \n",
    "            # Add to hierarchy\n",
    "            if parent_id:\n",
    "                self.hierarchy[version][current_id] = parent_id\n",
    "        \n",
    "        # Process children\n",
    "        for child in node.children:\n",
    "            self._process_node(child, current_id, version)\n",
    "    \n",
    "    def merge_versions(self, versions_data: List[Tuple[HierarchyNode, str]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Merge multiple versions into a single export.\n",
    "        Elements are deduplicated across versions.\n",
    "        \n",
    "        Args:\n",
    "            versions_data: List of (root_node, version_id) tuples\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with merged elements and per-version hierarchies\n",
    "        \"\"\"\n",
    "        self.elements.clear()\n",
    "        self.hierarchy.clear()\n",
    "        \n",
    "        for root, version in versions_data:\n",
    "            self._process_node(root, parent_id=None, version=version)\n",
    "        \n",
    "        return {\n",
    "            \"elements\": self.elements,\n",
    "            \"hierarchy\": self.hierarchy\n",
    "        }\n",
    "    \n",
    "    def save_to_file(self, data: Dict[str, Any], output_path: str):\n",
    "        \"\"\"Save exported data to JSON file\"\"\"\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Saved to {output_path}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_arxiv_id_from_path(base_dir: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract arXiv ID from directory name.\n",
    "        Expected format: yymm.nnnnn or yymm-nnnnn\n",
    "        \"\"\"\n",
    "        dir_name = Path(base_dir).name\n",
    "        # Try to match arXiv ID pattern\n",
    "        match = re.match(r'(\\d{4})[.\\-](\\d{4,5})', dir_name)\n",
    "        if match:\n",
    "            return f\"{match.group(1)}.{match.group(2)}\"\n",
    "        return dir_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e40886",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainFileDetector:\n",
    "    \"\"\"Automatically detect the main LaTeX file in a directory.\"\"\"\n",
    "    \n",
    "    # Common main file names\n",
    "    MAIN_FILE_CANDIDATES = [\n",
    "        'main.tex', 'paper.tex', 'article.tex', 'manuscript.tex',\n",
    "        'thesis.tex', 'document.tex', 'root.tex'\n",
    "    ]\n",
    "    \n",
    "    # Document class pattern to identify main files\n",
    "    DOCUMENT_CLASS_PATTERN = re.compile(r'\\\\documentclass', re.IGNORECASE)\n",
    "    BEGIN_DOCUMENT_PATTERN = re.compile(r'\\\\begin\\{document\\}', re.IGNORECASE)\n",
    "    AUTHOR_DOCUMENT_PATTERN = re.compile(r'\\\\author', re.IGNORECASE)\n",
    "    \n",
    "    @classmethod\n",
    "    def find_main_file(cls, tex_dir: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Find the main LaTeX file in a directory.\n",
    "        \n",
    "        Strategy:\n",
    "        1. Look for common main file names\n",
    "        2. Look for files with \\\\documentclass that also have \\\\begin{document}\n",
    "        3. If only one .tex file at root level, use that\n",
    "        \"\"\"\n",
    "        tex_path = Path(tex_dir)\n",
    "        \n",
    "        if not tex_path.exists():\n",
    "            return None\n",
    "        \n",
    "        # Strategy 1: Check common names\n",
    "        for candidate in cls.MAIN_FILE_CANDIDATES:\n",
    "            candidate_path = tex_path / candidate\n",
    "            if candidate_path.exists():\n",
    "                return candidate\n",
    "        \n",
    "        # Strategy 2: Find files with documentclass AND begin{document}\n",
    "        root_tex_files = list(tex_path.glob('*.tex'))\n",
    "        main_candidates = []\n",
    "        \n",
    "        for tex_file in root_tex_files:\n",
    "            try:\n",
    "                with open(tex_file, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                has_docclass = cls.DOCUMENT_CLASS_PATTERN.search(content)\n",
    "                has_begin_doc = cls.BEGIN_DOCUMENT_PATTERN.search(content)\n",
    "                has_author = cls.AUTHOR_DOCUMENT_PATTERN.search(content)\n",
    "                \n",
    "                if has_docclass and has_begin_doc and has_author:\n",
    "                    main_candidates.append(tex_file.name)\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        if len(main_candidates) == 1:\n",
    "            return main_candidates[0]\n",
    "        elif len(main_candidates) > 1:\n",
    "            # Prefer shorter filenames (less likely to be appendix, etc.)\n",
    "            return min(main_candidates, key=len)\n",
    "        \n",
    "        # Strategy 3: If only one tex file at root level\n",
    "        if len(root_tex_files) == 1:\n",
    "            return root_tex_files[0].name\n",
    "        \n",
    "        # Strategy 4: Look for any tex file with documentclass\n",
    "        for tex_file in root_tex_files:\n",
    "            try:\n",
    "                with open(tex_file, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                    content = f.read()\n",
    "                if cls.DOCUMENT_CLASS_PATTERN.search(content):\n",
    "                    return tex_file.name\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "class MultiVersionProcessor:\n",
    "    \"\"\"Process multiple versions of the same paper.\"\"\"\n",
    "    \n",
    "    def __init__(self, paper_dir: str):\n",
    "        self.paper_dir = Path(paper_dir)\n",
    "        self.arxiv_id = self.paper_dir.name\n",
    "        # self.tex_dir = self.paper_dir / 'tex'\n",
    "        \n",
    "        # Check if tex/ subdirectory exists, otherwise use paper_dir directly\n",
    "        tex_dir_candidate = self.paper_dir / 'tex'\n",
    "        self.tex_dir = tex_dir_candidate if tex_dir_candidate.exists() else self.paper_dir\n",
    "        \n",
    "        self.versions: Dict[str, Path] = {}\n",
    "        self.results: Dict[str, Any] = {}\n",
    "        \n",
    "    def discover_versions(self) -> List[str]:\n",
    "        \"\"\"Find all version directories (e.g., 2305-14596v1, 2305-14596v2).\"\"\"\n",
    "        self.versions.clear()\n",
    "        \n",
    "        if not self.tex_dir.exists():\n",
    "            return []\n",
    "        \n",
    "        # Look for version folders\n",
    "        version_pattern = re.compile(rf'{re.escape(self.arxiv_id)}v(\\d+)', re.IGNORECASE)\n",
    "        \n",
    "        for item in self.tex_dir.iterdir():\n",
    "            if item.is_dir():\n",
    "                match = version_pattern.match(item.name)\n",
    "                if match:\n",
    "                    version_num = match.group(1)\n",
    "                    self.versions[version_num] = item\n",
    "        \n",
    "        # Sort versions numerically\n",
    "        return sorted(self.versions.keys(), key=int)\n",
    "    \n",
    "    def parse_version(self, version: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Parse a specific version of the paper.\"\"\"\n",
    "        if version not in self.versions:\n",
    "            return None\n",
    "        \n",
    "        version_dir = self.versions[version]\n",
    "        main_file = MainFileDetector.find_main_file(str(version_dir))\n",
    "        \n",
    "        if not main_file:\n",
    "            print(f\"  Warning: Could not find main file in {version_dir}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            parser = LaTeXParser(str(version_dir))\n",
    "            result = parser.parse(main_file)\n",
    "            return {\n",
    "                'parser': parser,\n",
    "                'result': result,\n",
    "                'main_file': main_file\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"  Error parsing version {version}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_all_versions(self) -> Dict[str, Any]:\n",
    "        \"\"\"Process all versions and return combined results.\"\"\"\n",
    "        versions = self.discover_versions()\n",
    "        \n",
    "        if not versions:\n",
    "            print(f\"  No versions found for {self.arxiv_id}\")\n",
    "            return {}\n",
    "        \n",
    "        print(f\"  Found {len(versions)} version(s): {versions}\")\n",
    "        \n",
    "        for version in versions:\n",
    "            print(f\"\\n  Processing version {version}...\")\n",
    "            result = self.parse_version(version)\n",
    "            if result:\n",
    "                self.results[version] = result\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def export_combined(self, output_dir: str) -> Optional[Path]:\n",
    "        \"\"\"\n",
    "        Export combined results for all versions.\n",
    "        Elements are deduplicated across versions.\n",
    "        Each version has its own hierarchy.\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            return None\n",
    "        \n",
    "        exporter = MilestoneExporter()\n",
    "        combined_elements = {}\n",
    "        combined_hierarchy = {}\n",
    "        \n",
    "        for version, data in self.results.items():\n",
    "            parser = data['parser']\n",
    "            if parser.hierarchy:\n",
    "                # Export this version\n",
    "                version_data = exporter.export_document(parser.hierarchy, version=version)\n",
    "                \n",
    "                # Merge elements (deduplicated by ID)\n",
    "                combined_elements.update(version_data['elements'])\n",
    "                \n",
    "                # Add version hierarchy\n",
    "                combined_hierarchy.update(version_data['hierarchy'])\n",
    "        \n",
    "        # Create output structure\n",
    "        output_data = {\n",
    "            'elements': combined_elements,\n",
    "            'hierarchy': combined_hierarchy\n",
    "        }\n",
    "        \n",
    "        # Create output directory\n",
    "        out_path = Path(output_dir) / self.arxiv_id\n",
    "        out_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save main JSON file\n",
    "        main_json = out_path / f\"{self.arxiv_id}.json\"\n",
    "        with open(main_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Save metadata from source if exists\n",
    "        metadata_src = self.paper_dir / 'metadata.json'\n",
    "        if metadata_src.exists():\n",
    "            import shutil\n",
    "            shutil.copy(metadata_src, out_path / 'metadata.json')\n",
    "        \n",
    "        # Save references from source if exists\n",
    "        references_src = self.paper_dir / 'references.json'\n",
    "        if references_src.exists():\n",
    "            import shutil\n",
    "            shutil.copy(references_src, out_path / 'references.json')\n",
    "        \n",
    "        # Export BibTeX from all versions\n",
    "        all_bibtex = {}\n",
    "        for version, data in self.results.items():\n",
    "            parser = data['parser']\n",
    "            for key, entry in parser.references.items():\n",
    "                if key not in all_bibtex:\n",
    "                    all_bibtex[key] = {\n",
    "                        'type': entry.entry_type,\n",
    "                        'fields': entry.fields\n",
    "                    }\n",
    "        \n",
    "        if all_bibtex:\n",
    "            bibtex_json = out_path / 'extracted_bibtex.json'\n",
    "            with open(bibtex_json, 'w', encoding='utf-8') as f:\n",
    "                json.dump(all_bibtex, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b4f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchProcessor:\n",
    "    \"\"\"\n",
    "    Process all papers in a student folder.\n",
    "    \n",
    "    Expected folder structure:\n",
    "    student_folder/\n",
    "        2305-14596/\n",
    "            metadata.json\n",
    "            references.json\n",
    "            tex/\n",
    "                2305-14596v1/\n",
    "                    main.tex\n",
    "                    ...\n",
    "                2305-14596v2/\n",
    "                    main.tex\n",
    "                    ...\n",
    "        2305-14597/\n",
    "            ...\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, student_folder: str, output_folder: str = None):\n",
    "        self.student_folder = Path(student_folder)\n",
    "        self.student_id = self.student_folder.name\n",
    "        self.output_folder = Path(output_folder) if output_folder else self.student_folder.parent / f\"{self.student_id}_output\"\n",
    "        \n",
    "        self.papers: List[Path] = []\n",
    "        self.results: Dict[str, Any] = {}\n",
    "        self.stats = {\n",
    "            'total_papers': 0,\n",
    "            'processed': 0,\n",
    "            'failed': 0,\n",
    "            'skipped': 0,\n",
    "            'total_elements': 0,\n",
    "            'total_versions': 0\n",
    "        }\n",
    "    \n",
    "    def discover_papers(self) -> List[str]:\n",
    "        \"\"\"Find all paper directories in the student folder.\"\"\"\n",
    "        self.papers.clear()\n",
    "        \n",
    "        # Look for directories matching arXiv ID pattern\n",
    "        arxiv_pattern = re.compile(r'\\d{4}-\\d{4,5}')\n",
    "        \n",
    "        for item in sorted(self.student_folder.iterdir()):\n",
    "            if item.is_dir() and arxiv_pattern.match(item.name):\n",
    "                # Accept folders with tex/ subdirectory OR version folders directly\n",
    "                tex_dir = item / 'tex'\n",
    "                # If tex_dir.exists(): # If it has a tex/ subdirectory\n",
    "                has_tex = tex_dir.exists()\n",
    "                has_versions = any(\n",
    "                    (item / d.name).is_dir() and re.match(rf'{re.escape(item.name)}v\\d+', d.name, re.IGNORECASE)\n",
    "                    for d in item.iterdir() if d.is_dir()\n",
    "                )\n",
    "                if has_tex or has_versions:\n",
    "                    self.papers.append(item)\n",
    "        \n",
    "        self.stats['total_papers'] = len(self.papers)\n",
    "        return [p.name for p in self.papers]\n",
    "    \n",
    "    def process_paper(self, paper_dir: Path) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Process a single paper with all its versions.\"\"\"\n",
    "        processor = MultiVersionProcessor(str(paper_dir))\n",
    "        results = processor.process_all_versions()\n",
    "        \n",
    "        if results:\n",
    "            output_path = processor.export_combined(str(self.output_folder))\n",
    "            \n",
    "            # Calculate stats\n",
    "            total_elements = 0\n",
    "            for version_data in results.values():\n",
    "                parser = version_data.get('parser')\n",
    "                if parser and parser.hierarchy:\n",
    "                    def count_nodes(node):\n",
    "                        count = 1\n",
    "                        for child in node.children:\n",
    "                            count += count_nodes(child)\n",
    "                        return count\n",
    "                    total_elements += count_nodes(parser.hierarchy)\n",
    "            \n",
    "            return {\n",
    "                'arxiv_id': paper_dir.name,\n",
    "                'versions_processed': len(results),\n",
    "                'output_path': output_path,\n",
    "                'total_elements': total_elements\n",
    "            }\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def process_all(self, limit: int = None, verbose: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process all papers in the folder.\n",
    "        \n",
    "        Args:\n",
    "            limit: Maximum number of papers to process (for testing)\n",
    "            verbose: Whether to print progress\n",
    "        \"\"\"\n",
    "        papers = self.discover_papers()\n",
    "        \n",
    "        if not papers:\n",
    "            print(f\"No papers found in {self.student_folder}\")\n",
    "            return self.stats\n",
    "        \n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"BATCH PROCESSING: {self.student_id}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Found {len(papers)} papers to process\")\n",
    "        print(f\"Output folder: {self.output_folder}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # Create output folder\n",
    "        self.output_folder.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        papers_to_process = self.papers[:limit] if limit else self.papers\n",
    "        \n",
    "        for i, paper_dir in enumerate(papers_to_process):\n",
    "            arxiv_id = paper_dir.name\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\n[{i+1}/{len(papers_to_process)}] Processing {arxiv_id}...\")\n",
    "                print(\"-\" * 40)\n",
    "            \n",
    "            try:\n",
    "                result = self.process_paper(paper_dir)\n",
    "                \n",
    "                if result:\n",
    "                    self.results[arxiv_id] = result\n",
    "                    self.stats['processed'] += 1\n",
    "                    self.stats['total_versions'] += result['versions_processed']\n",
    "                    self.stats['total_elements'] += result['total_elements']\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f\"  âœ“ Successfully processed {result['versions_processed']} version(s)\")\n",
    "                        print(f\"    Elements: {result['total_elements']}\")\n",
    "                else:\n",
    "                    self.stats['skipped'] += 1\n",
    "                    if verbose:\n",
    "                        print(f\"  âš  Skipped (no versions found or parse failed)\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                self.stats['failed'] += 1\n",
    "                if verbose:\n",
    "                    print(f\"  âœ— Failed: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"BATCH PROCESSING COMPLETE\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"ðŸ“Š Statistics:\")\n",
    "        print(f\"   - Total papers found: {self.stats['total_papers']}\")\n",
    "        print(f\"   - Successfully processed: {self.stats['processed']}\")\n",
    "        print(f\"   - Skipped: {self.stats['skipped']}\")\n",
    "        print(f\"   - Failed: {self.stats['failed']}\")\n",
    "        print(f\"   - Total versions processed: {self.stats['total_versions']}\")\n",
    "        print(f\"   - Total elements extracted: {self.stats['total_elements']}\")\n",
    "        print(f\"\\nðŸ“ Output folder: {self.output_folder}\")\n",
    "        \n",
    "        # Save processing summary\n",
    "        summary_file = self.output_folder / 'processing_summary.json'\n",
    "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'student_id': self.student_id,\n",
    "                'stats': self.stats,\n",
    "                'papers': {\n",
    "                    arxiv_id: {\n",
    "                        'versions_processed': r['versions_processed'],\n",
    "                        'total_elements': r['total_elements']\n",
    "                    }\n",
    "                    for arxiv_id, r in self.results.items()\n",
    "                }\n",
    "            }, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"ðŸ“ Summary saved to: {summary_file}\")\n",
    "        \n",
    "        return self.stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e5ab0b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14326 papers in 23127238\n",
      "============================================================\n",
      "BATCH PROCESSING: 23127238\n",
      "============================================================\n",
      "Found 14326 papers to process\n",
      "Output folder: 23127238_output\n",
      "============================================================\n",
      "\n",
      "\n",
      "[1/5] Processing 2304-14607...\n",
      "----------------------------------------\n",
      "  Found 1 version(s): ['1']\n",
      "\n",
      "  Processing version 1...\n",
      "  Warning: Could not find main file in 23127238\\2304-14607\\tex\\2304-14607v1\n",
      "  âš  Skipped (no versions found or parse failed)\n",
      "\n",
      "[2/5] Processing 2304-14608...\n",
      "----------------------------------------\n",
      "  Found 2 version(s): ['1', '2']\n",
      "\n",
      "  Processing version 1...\n",
      "Parsing LaTeX document from: 23127238\\2304-14608\\tex\\2304-14608v1\\light_source_2023.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 236 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 195\n",
      "      - section: 7\n",
      "      - figure: 16\n",
      "      - subsection: 10\n",
      "      - table: 1\n",
      "      - subsubsection: 4\n",
      "      - acknowledgments: 1\n",
      "\n",
      "[3] Extracting references...\n",
      "Loaded 18 entries from lightsourcebib.bib\n",
      "    Found 18 bibliography entries\n",
      "\n",
      "[4] Deduplicating references...\n",
      "Merged duplicate: IceCube:2018 -> IceCube:2013\n",
      "Merged duplicate: ANTARES:2005 -> IceCube:2013\n",
      "Merged duplicate: STRAW:2019 -> IceCube:2013\n",
      "Merged duplicate: Bruijn:2015huz -> IceCube:2013\n",
      "Merged duplicate: trident:2022 -> IceCube:2013\n",
      "Merged duplicate: KM3NeT:2011ioj -> IceCube:2013\n",
      "Merged duplicate: TianWei:2022 -> IceCube:2013\n",
      "Merged duplicate: Yudi:2022 -> IceCube:2013\n",
      "Merged duplicate: Hufan:2022 -> IceCube:2013\n",
      "Merged duplicate: Jiannan:2022 -> IceCube:2013\n",
      "Merged duplicate: Wang:2023rvb -> IceCube:2013\n",
      "Merged duplicate: IceCube:2022der -> IceCube:2013\n",
      "Merged duplicate: Henningsen:2020zsj -> IceCube:2013\n",
      "Merged duplicate: baikal_neutrino2022 -> IceCube:2013\n",
      "Merged duplicate: Bailly:2021dxn -> IceCube:2013\n",
      "    Removed 15 duplicate references\n",
      "\n",
      "[5] Deduplicating content...\n",
      "    Unique content items: 212\n",
      "    Duplicate content items: 0\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "\n",
      "  Processing version 2...\n",
      "Parsing LaTeX document from: 23127238\\2304-14608\\tex\\2304-14608v2\\light_source_2023_final.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 246 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 205\n",
      "      - section: 7\n",
      "      - figure: 16\n",
      "      - subsection: 10\n",
      "      - table: 1\n",
      "      - subsubsection: 4\n",
      "      - acknowledgments: 1\n",
      "\n",
      "[3] Extracting references...\n",
      "Loaded 18 entries from light_source_2023.bib\n",
      "    Found 18 bibliography entries\n",
      "\n",
      "[4] Deduplicating references...\n",
      "Merged duplicate: IceCube:2018 -> IceCube:2013\n",
      "Merged duplicate: ANTARES:2005 -> IceCube:2013\n",
      "Merged duplicate: STRAW:2019 -> IceCube:2013\n",
      "Merged duplicate: Bruijn:2015huz -> IceCube:2013\n",
      "Merged duplicate: trident:2022 -> IceCube:2013\n",
      "Merged duplicate: KM3NeT:2011ioj -> IceCube:2013\n",
      "Merged duplicate: TianWei:2022 -> IceCube:2013\n",
      "Merged duplicate: Yudi:2022 -> IceCube:2013\n",
      "Merged duplicate: Hufan:2022 -> IceCube:2013\n",
      "Merged duplicate: Jiannan:2022 -> IceCube:2013\n",
      "Merged duplicate: Wang:2023rvb -> IceCube:2013\n",
      "Merged duplicate: IceCube:2022der -> IceCube:2013\n",
      "Merged duplicate: Henningsen:2020zsj -> IceCube:2013\n",
      "Merged duplicate: baikal_neutrino2022 -> IceCube:2013\n",
      "Merged duplicate: Bailly:2021dxn -> IceCube:2013\n",
      "    Removed 15 duplicate references\n",
      "\n",
      "[5] Deduplicating content...\n",
      "    Unique content items: 222\n",
      "    Duplicate content items: 0\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "  âœ“ Successfully processed 2 version(s)\n",
      "    Elements: 482\n",
      "\n",
      "[3/5] Processing 2304-14609...\n",
      "----------------------------------------\n",
      "  Found 3 version(s): ['1', '2', '3']\n",
      "\n",
      "  Processing version 1...\n",
      "Parsing LaTeX document from: 23127238\\2304-14609\\tex\\2304-14609v1\\arxiv1.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 313 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - section: 11\n",
      "      - block_formula: 13\n",
      "      - figure: 9\n",
      "      - sentence: 273\n",
      "      - acknowledgments: 1\n",
      "      - subsection: 4\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 0 bibliography entries\n",
      "\n",
      "[4] Deduplicating references...\n",
      "    Removed 0 duplicate references\n",
      "\n",
      "[5] Deduplicating content...\n",
      "    Unique content items: 293\n",
      "    Duplicate content items: 2\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "\n",
      "  Processing version 2...\n",
      "Parsing LaTeX document from: 23127238\\2304-14609\\tex\\2304-14609v2\\manuscript.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 351 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 305\n",
      "      - section: 11\n",
      "      - block_formula: 17\n",
      "      - figure: 11\n",
      "      - table: 1\n",
      "      - subsection: 4\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 0 bibliography entries\n",
      "\n",
      "[4] Deduplicating references...\n",
      "    Removed 0 duplicate references\n",
      "\n",
      "[5] Deduplicating content...\n",
      "    Unique content items: 334\n",
      "    Duplicate content items: 0\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "\n",
      "  Processing version 3...\n",
      "Parsing LaTeX document from: 23127238\\2304-14609\\tex\\2304-14609v3\\main.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 371 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 319\n",
      "      - section: 9\n",
      "      - block_formula: 18\n",
      "      - table: 1\n",
      "      - figure: 14\n",
      "      - subsection: 7\n",
      "      - acknowledgments: 1\n",
      "\n",
      "[3] Extracting references...\n",
      "    Found 0 bibliography entries\n",
      "\n",
      "[4] Deduplicating references...\n",
      "    Removed 0 duplicate references\n",
      "\n",
      "[5] Deduplicating content...\n",
      "    Unique content items: 311\n",
      "    Duplicate content items: 41\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "  âœ“ Successfully processed 3 version(s)\n",
      "    Elements: 1035\n",
      "\n",
      "[4/5] Processing 2304-14610...\n",
      "----------------------------------------\n",
      "  Found 2 version(s): ['1', '2']\n",
      "\n",
      "  Processing version 1...\n",
      "Parsing LaTeX document from: 23127238\\2304-14610\\tex\\2304-14610v1\\ijcai23.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 268 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 209\n",
      "      - section: 12\n",
      "      - figure: 13\n",
      "      - subsubsection: 6\n",
      "      - subsection: 7\n",
      "      - block_formula: 11\n",
      "      - table: 7\n",
      "      - acknowledgments: 1\n",
      "\n",
      "[3] Extracting references...\n",
      "Loaded 71 entries from ijcai23.bib\n",
      "    Found 71 bibliography entries\n",
      "\n",
      "[4] Deduplicating references...\n",
      "Merged duplicate: fu2016weighted -> Fu_2016_CVPR\n",
      "    Removed 1 duplicate references\n",
      "\n",
      "[5] Deduplicating content...\n",
      "    Unique content items: 240\n",
      "    Duplicate content items: 0\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "\n",
      "  Processing version 2...\n",
      "Parsing LaTeX document from: 23127238\\2304-14610\\tex\\2304-14610v2\\ijcai23.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 268 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 209\n",
      "      - section: 12\n",
      "      - figure: 13\n",
      "      - subsubsection: 6\n",
      "      - subsection: 7\n",
      "      - block_formula: 11\n",
      "      - table: 7\n",
      "      - acknowledgments: 1\n",
      "\n",
      "[3] Extracting references...\n",
      "Loaded 71 entries from ijcai23.bib\n",
      "    Found 71 bibliography entries\n",
      "\n",
      "[4] Deduplicating references...\n",
      "Merged duplicate: fu2016weighted -> Fu_2016_CVPR\n",
      "    Removed 1 duplicate references\n",
      "\n",
      "[5] Deduplicating content...\n",
      "    Unique content items: 240\n",
      "    Duplicate content items: 0\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "  âœ“ Successfully processed 2 version(s)\n",
      "    Elements: 536\n",
      "\n",
      "[5/5] Processing 2304-14611...\n",
      "----------------------------------------\n",
      "  Found 1 version(s): ['1']\n",
      "\n",
      "  Processing version 1...\n",
      "Parsing LaTeX document from: 23127238\\2304-14611\\tex\\2304-14611v1\\main.tex\n",
      "============================================================\n",
      "\n",
      "[1] Gathering files...\n",
      "    Found 1 files in compilation path\n",
      "\n",
      "[2] Building hierarchy...\n",
      "    Built hierarchy with 163 nodes:\n",
      "      - document: 1\n",
      "      - abstract: 1\n",
      "      - sentence: 127\n",
      "      - section: 5\n",
      "      - subsection: 7\n",
      "      - block_formula: 18\n",
      "      - figure: 3\n",
      "      - table: 1\n",
      "\n",
      "[3] Extracting references...\n",
      "Loaded 31 entries from ref.bib\n",
      "    Found 31 bibliography entries\n",
      "\n",
      "[4] Deduplicating references...\n",
      "    Removed 0 duplicate references\n",
      "    Found 1 pairs of similar references:\n",
      "      - 'ye2022optimal' ~ 'wu2022communication' (similarity: 71.17%)\n",
      "\n",
      "[5] Deduplicating content...\n",
      "    Unique content items: 149\n",
      "    Duplicate content items: 0\n",
      "\n",
      "============================================================\n",
      "Parsing complete!\n",
      "  âœ“ Successfully processed 1 version(s)\n",
      "    Elements: 163\n",
      "\n",
      "============================================================\n",
      "BATCH PROCESSING COMPLETE\n",
      "============================================================\n",
      "ðŸ“Š Statistics:\n",
      "   - Total papers found: 14326\n",
      "   - Successfully processed: 4\n",
      "   - Skipped: 1\n",
      "   - Failed: 0\n",
      "   - Total versions processed: 8\n",
      "   - Total elements extracted: 2216\n",
      "\n",
      "ðŸ“ Output folder: 23127238_output\n",
      "ðŸ“ Summary saved to: 23127238_output\\processing_summary.json\n"
     ]
    }
   ],
   "source": [
    "# Test with a few papers first\n",
    "STUDENT_FOLDER = \"23127238\"\n",
    "OUTPUT_FOLDER = \"23127238_output\"\n",
    "\n",
    "# Initialize batch processor\n",
    "batch_processor = BatchProcessor(STUDENT_FOLDER, OUTPUT_FOLDER)\n",
    "\n",
    "# Discover all papers\n",
    "papers = batch_processor.discover_papers()\n",
    "print(f\"Found {len(papers)} papers in {STUDENT_FOLDER}\")\n",
    "\n",
    "stats = batch_processor.process_all(limit=5, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
